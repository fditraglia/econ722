
\documentclass[12pt]{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{todonotes}
\usepackage{amssymb,amsmath,amsthm,graphicx}
\usepackage{rotating}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{color}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}

\newtheorem{assump}{Assumption}[section]
\newtheorem{pro}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{ineq}{Inequality}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{ex}{Example}[section]
\theoremstyle{definition}
\newtheorem{alg}{Algorithm}[section]


\linespread{1.3}

\begin{document}

\title{Lecture 6: Moment Selection for GMM}

\author{Francis J.\ DiTraglia}

\maketitle 

\section{Review of Generalized Method of Moments}
The best all-around reference for for GMM is Hall (2005). These notes draw on chapters 3--7 of his book and use essentially the same notation.

\subsection{Key Assumptions}
Let $f$ be a $q$-vector of functions of an observable random $r$-vector $v_t$ and a $p$-vector of parameters $\theta \in \Theta \subseteq \mathbb{R}^p$ where $\Theta$ is compact. The GMM estimator is defined as follows:
	\begin{eqnarray*}
		\bar{g}_T(\theta) &=& \frac{1}{T}\sum_{t=1}^T f(v_t, \theta)\\
		\widehat{\theta}_T &=& \underset{\theta \in \Theta}{\arg \min} \; \bar{g}_T(\theta)'W_T \bar{g}_T(\theta) 
	\end{eqnarray*}
The basic assumptions required for GMM estimation are as follows.

\paragraph{Strict Stationarity} The sequence $\{v_t\colon -\infty <t <\infty\}$ of random $r$-vectors is a strictly stationary process with sample space $\mathcal{V}\subseteq \mathbb{R}^r$. Importantly, this implies that the expectations of \emph{any} functions of $v_t$ are constant over time.

\paragraph{Population Moment Condition} $E[f(v_t, \theta_0)]=0$ for some $\theta_0 \in \mbox{interior}(\Theta)$.

\paragraph{Global Identification} For any $\widetilde{\theta}\in \Theta$ such that $\widetilde{\theta}\neq \theta_0$, $E[f(v_t,\widetilde{\theta})]\neq 0 $.


\paragraph{Weighting Matrix} The $(q\times q)$ weighting matrix $W_T$ is positive semi-definite and converges in probability to a postitive definite constant matrix $W$.

\subsection{Regularity Conditions}


\paragraph{Regularity Conditions for Moment Functions}
The $q$ moment functions $f\colon \mathcal{V}\times \Theta \rightarrow \mathbb{R}^q$ satisfy the following conditions:
	\begin{enumerate}[(i)]
		\item $f$ is $v_t$-almost surely continuous on $\Theta$
		\item $E[f(v_t, \theta)]<\infty$ exists and is continuous on $\Theta$
	\end{enumerate}

\paragraph{Regularity Conditions for Derivative Matrix}
	\begin{enumerate}[(i)]
		\item The $q\times p$ matrix $\nabla_{\theta'} f(v_t, \theta)$ exists and is $v_t$-almost continuous on $\Theta$ 
		\item $E[\nabla_{\theta}f(v_t, \theta_0)]<\infty$ exists and is continuous in a  neighborhood $N_\epsilon$ of $\theta_0$
		\item $\underset{\theta \in N_\epsilon}{\sup}\left| \left| T^{-1} \sum_{t=1}^T \nabla_\theta f(v_t, \theta) - E[\nabla_\theta f(v_t, \theta)]\right|\right| \overset{p}{\rightarrow} 0$
	\end{enumerate}

\paragraph{Regularity Conditions for Variance of Sample Moment Conditions}
\begin{enumerate}[(i)]
	\item $E[f(v_t, \theta_0)f(v_t, \theta_0)']$ exists and is finite.
	\item $\underset{T\rightarrow \infty}{\lim} Var\left[\sqrt{T}\bar{g}_T(\theta_0)\right]=S$ exists and is a finite, positive definite matrix.
\end{enumerate}

\subsection{Asymptotics Under Correct Specification}
Under the set of assumptions given above, we obtain the following:

\paragraph{Consistency of GMM Estimator} $\widehat{\theta}_T \overset{p}{\rightarrow} \theta_0$

\paragraph{Asymptotic Normality of GMM Estimator}
$\sqrt{T} (\widehat{\theta}_T - \theta_0)\overset{d}{\rightarrow} \mathcal{N}(0,MSM')$
	\begin{eqnarray*}
		M &=& (G_0 W G_0)^{-1} G_0'W\\
		G_0 &=& E[\nabla_{\theta'} f(v_t, \theta_0)]
	\end{eqnarray*}


\subsection{The J-test Statistic}
The $J$-test statistic is given by
	$$J_T = T\; \bar{g}_T(\widehat{\theta}_T')\; \widehat{S}^{-1} \;\bar{g}_T(\widehat{\theta}_T)$$
where $\widehat{S}$ is a consistent estimator of $S$, the long-run variance matrix of the GMM sample moment conditions. We will need to consider the asymptotic behavior of this quantity in two settings: when the population moment condition is satisfied, and when it is violated.

\paragraph{Correct Specification}
Earlier in this document we reviewed the basic asymptotic results for GMM estimation under standard regularity conditions \emph{assuming the population moment condition is correct}. Our main findings were that, regardless of weighting matrix, GMM is consistent and asymptotically normal. The particular choice of $W_T$ \emph{only} affects the asymptotic variance of the estimator. To study the behavior of the $J$-test in this setting, we need to examine an asymptotic expansion for the estimated sample moment. Using Taylor Expansion arguments, we can show that
	$$W_T^{1/2} \sqrt{T} \bar{g}_T(\widehat{\theta}_T) = \left[I_q - P(\theta_0) \right]W^{1/2} \sqrt{T} \bar{g}_T(\theta_0) + o_p(1)$$
where 
	\begin{eqnarray*}
	P(\theta_0) &=& F(\theta_0) \left[F(\theta_0)' F(\theta_0) \right]^{-1} F(\theta_0)'\\
		F(\theta_0) &=& W^{1/2} E[\nabla_\theta f(v_t, \theta_0)]
	\end{eqnarray*}
The matrix $P(\theta_0)$ is called the \emph{identifying restrictions} and corresponds to the particular projection of $W^{1/2}E[f(v_t,\theta)]$ actually used in GMM estimation. Its orthogonal complement, $N = I_q - P(\theta_0)$, is called the \emph{overidentifying restrictions}. The expansion just stated shows that the asymptotic behavior of the estimated sample moment is \emph{entirely governed by the overidentifying restrictions}. Via a CLT for $\sqrt{T} \bar{g}_T(\theta_0)$, it follows that
	$$W_T^{1/2} \sqrt{T} \bar{g}_T(\widehat{\theta}_T) \overset{d}{\rightarrow} \mathcal{N}(0, NW^{1/2}S W^{1/2}N')$$
Note that the $N = I_q - P(\theta_0)$ has rank $q-p$ since it is the orthogonal complement of the rank $p$ projection matrix $P(\theta_0)$. Hence, in the limit we obtain a \emph{singular normal distribution}, that is a $q$-dimensional random vector that concentrates on a $(q-p)$-dimensional subspace of $\mathbb{R}^q$. Substituting the efficient weighting matrix $\widehat{S}^{-1}$ we find that $J_T \overset{d}{\rightarrow} \chi^2_{q-p}$ by the Continuous Mapping Theorem, \emph{assuming that the population moment condition is correct}.

\paragraph{Incorrect Specification}
When the GMM population moment condition $E[f(v_t,\theta)]=0$ is \emph{false} for all $\theta\in \Theta$, the situation is completely different. In this case the probability limit of $\widehat{\theta}_T$ in general \emph{will} depend on the choice of weighting matrix and the rate of convergence depends on the rate at which $W_T$ converges to $W$. Unsurprisingly, this leads to very different behavior for the $J$-test statistic. So exactly in what sense is $E[f(v_t,\theta)]=0$ false? For now we'll consider \textbf{fixed mis-specification}. Specifically we'll suppose that
	$$E[f(v_t, \theta)] = \mu(\theta), \quad ||\mu(\theta)||>0 \quad \forall \; \theta\in\Theta$$
Note that this situation can only occur if $q>p$ since we can always solve the population moment conditions \emph{exactly} for $\theta$ in the just-identified case. Let $\widehat{S}$ be an estimator of the variance matrix of the moment conditions and let $W$ be the  probability limit of $\widehat{S}^{-1}$. Then, if $\mu_* = \mu(\theta^*)$ is the probability limit of $\bar{g}_T(\widehat{\theta})$, where $\theta^*$ is the solution to the projected moment conditions given by the identifying restrictions, we have
	$$\frac{1}{T} J_T = \bar{g}_T(\widehat{\theta}_T)' \widehat{S}^{-1}\bar{g}_T(\widehat{\theta}_T) = \mu_*' W \mu_* + o_p(1)$$
As long as $W$ is positive definite, $\mu_*' W \mu_*>0$ since $\mu(\theta)>0$ for all $\theta\in \Theta$. Thus, $J_T = T \mu_*' W \mu_* + o_p(T)$. In other words, under fixed mis-specification the $J$-test statistic \emph{diverges at rate} $T$. 

\section{Andrews' GMM Moment Selection Criteria}
The consistency and asymptotic normality results for GMM estimation rely on the assumption that the moment conditions used in estimation are correct. That is, they assume that $E[f(v_t,\theta_0)]=0$. But what if we are unsure of this assumption? In many real-world applications we have a fairly large collection of moment functions, the $q$ elements of $f$, some of which may have been derived under different economic or statistical assumptions that others. It could easily be the case that only \emph{some} of the moment functions in $f$ satisfy the moment conditions, while others do not. To take a simple example, we may have a collection of instrumental variables that arise from different sources or different assumptions on the DGP. Perhaps only some of these instruments are truly exogenous but we are unsure which. Andrews (1999) proposes a family of \emph{moment selection criteria} (MSC) for this situation, in which the aim is to consistently select \emph{any and all} elements of $f$ that satisfy the moment condition, and eliminate those that do not. 

Roughly speaking, the intuition is as follows. When we studied AIC, BIC and friends, we discussed how the maximized log-likelihood measures model fit but unfairly advantages models with more parameters. The various model selection criteria we examined amounted to adding some kind of ``penalty'' term to correct for this by \emph{penalizing} more complicated models. In a similar vein, so long as we have more moment conditions than parameters, the $J$-test statistic provides a measure of how well the data ``fit'' the moment conditions: the bigger the statistic, the greater the evidence that the moment conditions are violated. The problem is that $J$-test statistic tends to increase as we add additional moment conditions \emph{even if they are correct}. Thus, if we simply compared $J$-statistics, we would be led to select \emph{too few} moment conditions. To correct for this, Andrews (1999) considers a variety of ``bonus terms'' that \emph{reward} estimators based on a lager number of moment conditions. Using this idea, he derives GMM analogues of AIC, BIC and the Hannan-Quinn information criterion, and studies the conditions under which a bonus term will yield consistent moment selection.

\subsection{Notation}
Let $f_{max}$ be a $(q\times 1)$ vector containing all of the moment functions under consideration. Let $c$ be a \emph{selection vector}, a $(q\times 1)$ vector of ones and zeros indicating which elements of $f_{max}$ we use in estimation for a \emph{particular candidate specification}. Let $\mathcal{C}$ denote the set of all candidates and $|c|$ denote the number of moment conditions used to estimate candidate $c$. Naturally, we require that there are at least as many moment conditions as parameters to estimate. Let $\widehat{\theta}_T(c)$ be the efficient two-step GMM estimator based on the moment conditions $E[f(v_t,\theta, c)]=0$ and define
\begin{eqnarray*}
 	V_\theta(c) &=& \left[G_0(c) S(c)^{-1} G_0(c) \right]^{-1}\\
 	G_0(c) &=& E[\nabla_\theta' f(v_t, \theta_0; c)]\\
 	S(c) &=& \lim_{T\rightarrow\infty} Var\left[\frac{1}{\sqrt{T}}\sum_{t=1}^T f(v_t, \theta_0;c)\right]\\
 	J_T(c) &=& T \bar{g}_T\left(\widehat{\theta}_T(c);c\right)'\widehat{S}_T(c)^{-1}\bar{g}_T\left(\widehat{\theta}_T(c);c\right)
 \end{eqnarray*}
where $\widehat{S}(c)$ is a consistent estimator of $S(c)$.

\subsection{Moment Selection Heuristics}
So how should we choose $c$? Two principles come to mind. First, we know that using only correctly specified moment conditions in estimation ensures that $\widehat{\theta}_T \overset{p}{\rightarrow} \theta_0$. Thus, to ensure consistent estimation, we should seek to eliminate any moment conditions whose expectation is non-zero. Second, it can be shown (see Hall, 2005; Theorem 6.1) that adding additional correctly specified moment conditions \emph{cannot increase} the asymptotic variance of our estimator. Putting these two pieces together, Andrews (1999) suggests that we attempt to identify the \emph{maximal set of correctly specified moment conditions}.

But what exactly does this mean? Identification is a bit tricky when we start to consider the possibility that some of our moment conditions do not have expectation zero. The potential problem is that different subsets of $f_{\max}$ could satisfy the population moment condition at \emph{different values} of $\theta$. We will need to rule this possbility out somehow. Let $\mathcal{Z}^0$ denote the set of all candidates $c$ such that $E[f(v_t, \theta; c)] = 0$ \emph{for some} $\theta\in \Theta$. Then, of all the candidates $c\in \mathcal{Z}^0$, let $\mathcal{MZ}^0$ denote those that contain the \emph{maximum number} of elements of $f_{max}$. For Andrews' suggestion to be meaningful, we need to assume that $\mathcal{MZ}^0$ contains \emph{exactly one element}, which we'll call $c_0$.

Andrews proposes adding a ``bonus term'' to the $J$-test statistic, leading to moment selection critera (MSC) of the form
	$$MSC(c) = J_T(c) - B(T, |c|)$$
where $B$ is a ``bonus term'' that ``rewards'' specifications that use more moment conditions in estimation and may depend on sample size. In calculating the $J$-test statistic, Andrews recommends using a \emph{centered} covariance matrix estimator
	$$\widehat{S}(c) = \frac{1}{T} \sum_{t=1}^T \left[f(v_t, \widehat{\theta}_T(c);c) - \bar{g}_T(\widehat{\theta}_T(c); c) \right]\left[f(v_t, \widehat{\theta}_T(c);c) - \bar{g}_T(\widehat{\theta}_T(c); c) \right]'$$
based on the weighing matrix that \emph{would be} efficient if the moment conditions were correctly specified. This estimator is consistent for $S(c)$ \emph{regardless} of whether the population moment conditions hold. To carry out moment selection, we choose $c$ to \emph{minimize} the criterion, defining $\widehat{c}_T = \underset{c\in\mathcal{C}}{\arg\min} \;\mbox{MSC}(c)$.



\subsection{Consistent Selection}
The main point of Andrews (1999) is to establish sufficient conditions on the bonus term that guarantee consistent selection of any and all correctly specified moment conditions with probability approaching one in the limit. First we'll take a look at the conditions, and then the proof.

\paragraph{Regularity Conditions for the $J$-test Statistic}
	\begin{enumerate}[(i)]
		\item If $E[f(v_t, \theta;c)] =0$ for a unique $\theta \in \Theta$, then $J_T(c) \overset{d}{\rightarrow} \chi^2_{|c| -p}$
		\item If $E[f(v_t, \theta;c)] \neq 0$ for a \emph{all} $\theta \in \Theta$ then $T^{-1} J_T(c) \overset{p}{\rightarrow} a(c)$, a finite, positive constant that may depend on $c$.
	\end{enumerate}


\paragraph{Regularity Conditions for Bonus Term} The bonus term can be written as $B(|c|,T) = \kappa_T h(|c|)$, where
	\begin{enumerate}[(i)]
		\item $h(\cdot)$ is strictly increasing
		\item $\kappa_T \rightarrow \infty$ as $T\rightarrow \infty$ and $\kappa_T =o(T)$
	\end{enumerate}

\paragraph{Identification Conditions}
	\begin{enumerate}[(i)]
		\item $\mathcal{MZ}^0 = \{c_0\}$
		\item $E[f(v_t, \theta_0; c_0)] = 0$ and $E[f(v_t, \theta; c_0)]\neq 0$ for any $\theta \neq \theta_0$
	\end{enumerate}


\begin{thm}
	Under the preceding assumptions, $\widehat{c}_T \overset{p}{\rightarrow} c_0$.
\end{thm}
\begin{proof}
We're trying to show that the moment conditions $\widehat{c}_T$ selected by our criterion are consistent for the maximal set $c_0$ of correct moment conditions. By definition $\widehat{c}_T = \underset{c\in \mathcal{C}}{\arg \min} \; MSC_T(c)$, so we need to show that
	$$\lim_{T\rightarrow \infty} P\left[\left\{MSC_T(c) - MSC_T(c_0)>0, \; \forall c \neq c_0\right\} \right] = 1$$
To simplify the notation, define
	\begin{eqnarray*}
		\Delta_T(c, c_0) &=& MSC_T(c) - MSC_T(c_0)\\
						&=& \left[J_T(c) -h(|c|)\kappa_T \right] - \left[J_T(c_0) -h(|c_0|)\kappa_T \right]\\
						&=& \left[J_T(c) - J_T(c_0)\right] + \kappa_T \left[h(|c_0|) - h(|c|) \right]
	\end{eqnarray*}
Now, we are interested in $\Delta_T(c, c_0)$ \emph{only} for situations in which $c\neq c_0$. Subject to this restriction, there are two cases, which we consider in turn. 
\paragraph{Case 1} Consider $c_1\neq c_0$ such that $E[f(v_t, \theta_1;c_1)]=0$ \emph{for a unique} $\theta_1$. In this case the first Regularity Condition for the $J$-test Statistic applies to \emph{both} $c_1$ \emph{and} $c_0$ and we have 
	$$J_T(c_1) - J_T(c_0) \overset{d}{\rightarrow} \chi^2_{|c_1| - p} - \chi^2_{|c_0|-p} = O_p(1)$$
By the first Identification Condition, $c_0$ is the \emph{unique} maximal set of correct moment conditions. Hence $|c_0| > |c_1|$. Now, by the first Regularity Condition for the Bonus Term, $h$ is strictly increasing. It follows that $h(|c_0|) - h(|c_1|)>0$. By the second Regularity Condition for the Bonus Term, $\kappa_T \rightarrow \infty$. Thus, 
	$$\kappa_T \left[h(|c_0|) - h(|c|) \right] \rightarrow \infty$$ 
It follows that $\Delta_T(c_1, c_0) \rightarrow \infty$ and we obtain our desired result.


\paragraph{Case 2} Consider $c_2 \neq c_0$ such that $E[f(v_t, \theta; c_2)]\neq 0$ for any $\theta \in \Theta$. In this case, the \emph{first} Regularity Condition for the $J$-test Statistic applies to $c_0$, while the \emph{second} applies to $c_2$ so we have
	$$T^{-1}\left[J_T(c_2) - J_T(c_0) \right] = a(c_2) + o_p(1) - T^{-1} O_p(1)$$
Now, whatever the value $\left[h(|c_0|) - h(|c|) \right]$ happens to be, it is definitely finite since $h$ is strictly increasing by the first Regularity Condition for the Bonus Term, and both $|c|$ and $|c_0|$ are finite. By the second Regularity Condition for the Bonus Term, $\kappa_T = o(T)$. Hence,
	$$T^{-1} \kappa_T\left[h(|c_0|) - h(|c|) \right] = o(1)$$
Putting the pieces together, we have
\begin{eqnarray*}
	T^{-1} \Delta_T(c_2, c_0) &=& a(c_2) + o_p(1) - T^{-1}O_p(1) + o(1)\\
	&=& a(c_2) + o_p(1)
\end{eqnarray*}
By the second Regularity Condition for the $J$-test Statistic, $a(c_2) >0$. Thus, $T^{-1}\Delta_T(c_2,c_0) >0$ with probability approaching one as $T\rightarrow \infty$. It follows that $\Delta_T(c_2,c_0)  \rightarrow \infty$ with probability approaching one as $T\rightarrow \infty$, as required.
\end{proof}

\subsection{Which Criteria Are Consistent?}
Among some other possibility, Andrews (1999) considers the following criteria which are constructed by making the bonus term resemble some of our old friends from maximum likelihood model selection:
\begin{eqnarray*}
	\mbox{GMM-BIC}(c) &=& J_T(c) - \left(|c| - p\right)\log(T)\\
	\mbox{GMM-HQ}(c) &=& J_T(c) - 2.01\left(|c| - p\right)\log(\log(T))\\
	\mbox{GMM-AIC}(c) &=& J_T(c) - 2\left(|c| - p\right)\log(T)
\end{eqnarray*}
We see immediately that GMM-AIC does \emph{not} satisfy the necessary conditions for consistency, since $\kappa_T = 2$ does not diverge as $T\rightarrow \infty$. In constrast, both the GMM-BIC and GMM-HQ diverge as $T\rightarrow \infty$, so we simply need to check the requirement that $\kappa_T = o(T)$. For GMM-BIC we have 
	$$\lim_{T \rightarrow \infty} \frac{\log T}{T} = \lim_{T \rightarrow \infty} \frac{1}{T} = 0$$
by l'H\^{o}pital's rule, and similarly for GMM-HQ
	$$\lim_{T \rightarrow \infty} \frac{\log \log T}{T} = \lim_{T \rightarrow \infty} \frac{1}{\log T} = 0$$
Thus both GMM-BIC and GMM-HQ provide consistent moment selection.
 
\subsection{Asymptotics for GMM-AIC}
We saw in the previous subsection that GMM-AIC does not satisfy the sufficient conditions for consistent moment selection. The question remains: how does this criterion behave in the limit? To answer this question, we revisit the proof of consistent selection from above. It turns out that GMM-AIC behaves \emph{differently} in the two cases considered in the proof. Combining them, we will see that GMM-AIC is \emph{not} a consistent moment selection criterion.

\paragraph{Case 2} In this case, we examined $c_2 \neq c_0$ such that $E[f(v_t, \theta; c_2)]\neq 0$ for any $\theta \in \Theta$. In other words, the moment conditions indexed by $c_2$ are \emph{not} satisfied for \emph{any} parameter value $\theta$. Asymptotically, GMM-AIC will \emph{never} select such a set of moment conditions. To see why, recall that $\kappa_T = 2$ for GMM-AIC. Although it does not diverge, this choice of $\kappa_T$ is \emph{still} $o(T)$. Thus, the argument from Case 2 \emph{still applies} to the GMM-AIC. We did not in fact use the assumption that $\kappa_T$ diverges in the proof of this case! 

\paragraph{Case 1} In this case, we examined $c_1\neq c_0$ such that $E[f(v_t, \theta_1;c_1)]=0$ \emph{for a unique} $\theta_1$. In other words, we considered a situation in which there \emph{is} a parameter vector $\theta_1$ at which the moment conditions indexed by $c_1$ are satisfied. Now, the difference of $J$-test statistics continues to be $O_p(1)$ regardless of the choice of $\kappa_T$, provided the regularity conditions are satisfied. Thus, substituting $\kappa_T = 2$, we have
	$$\Delta_T(c_1,c_0) = O_p(1) + 2\left[h(|c_0|) - h(|c|) \right]$$
But since the second term is a \emph{constant}, this is simply $\Delta_T(c_1,c_0) = O_p(1)$. In other words, the GMM-AIC is a \emph{random variable}, even in the limit as $T\rightarrow \infty$.

So where does this leave us? In Case 2 GMM-AIC cosistently selects $c_0$, but in Case 1 GMM-AIC is \emph{random even in the limit}. Putting these two results together, we see that, although it will never select a set of false moment conditions, GMM-AIC chooses \emph{randomly} among the set of correct moment conditions. In other words, it will not necessarily select $c_0$ as $T\rightarrow \infty$. 

\subsection{Extensions of Andrews (1999)}
Two followup papers extended the criteria described above. Andrews and Lu (2001) consider simultaneous model \emph{and} moment selection for GMM. The basic idea is the same, except that the parameter vector $\theta$ is restricted under some specifications. For example, we may consider setting a coefficient to zero. Accordingly, the ``bonus term'' depends both on the number of moment conditions used in estimation and the number of parameters that are estimated. Hong, Preston \& Shum (2003) extend Andrews and Lu (2001) to Generalized Empirical Likelihood estimators. For details on this class of estimators and their properties, see Newey \& Smith (2004). 

\subsection{Drawbacks to Andrews' Approach}
Andrews (1999) has a very specific goal: to state conditions under which it is possible to carry out consistent moment selection for GMM. This is an important and very useful contribution. Nevertheless, there are several reasons why the framework used in Andrews (1999) may not be appropriate in practical applications of GMM moment selection. 

First, the identification condition $\mathcal{MZ}^0 = \{c_0\}$ is stronger than it may appear. Section 7.3.1 of Hall (2005) gives an example: a linear IV model with one endogenous regressor, jointly normal errors, and eight instruments. Six of the instruments are exogenous, but two are in fact endogenous. The goal of moment selection is to find the exogenous instruments. Even in this very simple setting, the identification condition fails: there are two \emph{different} candidates, each containing six moment conditions, for which it is possible to find a parameter value at which the population moment conditions are satisfied. One of these parameters is the true $\theta$ and the other is not. The problem with the identification assumption isn't so much that it's strong. Without strong assumptions, it's hard to learn anything. The problem is that it's not especially \emph{transparent}: when considering a particular problem it can be hard to get a handle on whether this assumption makes sense.  


A second problem concerns irrelevant moment conditions. The idea of using any and all correctly specified moment conditions in estimation is based on the fact that the asymptotic variance of the GMM estimator \emph{cannot increase} as we use additional moment conditions in estimation. The finite-sample situation, however, can be very different. Moment conditions that add very little information, so-called ``irrelevant moment conditions,'' can lead to very poor finite sample performance. The GMM-MSC of Andrews (1999) does not address this problem. Two papers that do are Hall \& Peixe (2003) and Hall, Inoue, Jana \& Shin (2003). More recently, Chen \& Liao (2013) suggest using LASSO, which we'll study in an upcoming lecture, to choose the valid and revelant moment conditions.


A third issue concerns the nature of the analogy between the AIC and BIC and their GMM-MSC counterpoints. Like BIC, GMM-BIC is consistent and like AIC, GMM-AIC is not. The relationship ends here, however. As we saw in our first lectures of the semester there is a \emph{very specific idea} behind both the AIC and the BIC: the former attempts to correct the bias in the maximized log-likelihood as an estimator of the KL divergence, and the latter provides a large-sample approximation to the Bayesian posterior model probabilities under a uniform prior. Neither of these ideas has anything to do with the arguments behind the GMM-MSC criteria. Beyond conditions on the asymptotic behavior of the bonus term, any relationship to the AIC and BIC is merely cosmetic. This raises an interesting question: can we re-work any of the principles we used to derive model selection criteria for maximum likelihood so that they can be applied to GMM moment selection? The answer turns out to be \emph{yes}, as we will see below.



\section{The Focused Moment Selection Criterion}
This section is based on my working paper ``Using Invalid Instruments on Purpose: Focused Moment Selection for GMM.'' For the current version, see my website: \url{http://www.ditraglia.com}.\footnote{If you're really interested in this, check back again after the semester is over. I'll be posting an updated version with R and C++ source code soon.}

\subsection{Introduction}
In practical applications of GMM, we are rarely interested in determining which moment conditions are correct. More commonly, our goal is to answer a \emph{research question}, typically involving a parameter of interest $\mu$ that depends on the underlying GMM parameter vector $\theta$. Accordingly, it might make sense to try to get ``good'' estimates of $\mu$, \emph{regardless} of whether this involves using correct or incorrect moment conditions. The basic idea is that we might want to use a moment condition that is \emph{slightly mis-specified} provided that it is sufficiently informative about $\mu$: the decrease in variance could easily outweigh the increase in bias in a MSE-sense. This is very similar to the idea that underlies Mallow's $C_p$, Akaike's Final Prediction Error, and, you guessed it, the Focused Information Criterion of Hjort \& Claeskens (2003). My approach is most similar to the FIC, so I've named it the Focused Moment Selection Criterion, or FMSC for short. 

\subsection{Overview of FMSC Derivation}

\paragraph{Local Mis-specification}
	$$E\left[\begin{array}
		{c}
		g(Z_{ni},\theta_0)\\
		h(Z_{ni}, \theta_0)
	\end{array} \right] = \left[ \begin{array}
		{c} 0 \\ \tau/\sqrt{n}
	\end{array}\right]$$

\paragraph{Identification Condition} $E[g(Z_{ni},\theta_0)]=0$ identifies $\theta_0$.

\paragraph{Moment Selection Matrix} The matrix of zeros and ones $\Xi_S$ selects the moment conditions used to estimate candidate specification $S$.

\paragraph{Candidate GMM Estimator}
	$$\widehat{\theta}_S = \underset{\theta \in \Theta}{\arg \min} \left[\Xi_S f_n(\theta)\right]' \left[ \Xi_S \widetilde{W} \Xi_S'\right] \; \left[ \Xi_S f_n(\theta)\right]$$
where
$$f_n(\theta) = \frac{1}{n}\sum_{i=1}^n f(Z_{ni},\theta) = \left[\begin{array}{c} g_n(\theta)\\ h_n(\theta) \end{array} \right]=\left[\begin{array}{c}n^{-1}\sum_{i=1}^n g(Z_{ni},\theta) \\ n^{-1}\sum_{i=1}^n h(Z_{ni},\theta) \end{array}\right]$$
and $\widetilde{W}$ is positive semi-definite weighting matrix that converges in probability to a positive definite matrix $W$.

\paragraph{Some Notation}
	Let $Z$ denote the limiting random variable, for which \emph{all moment conditions are correctly specified} and define
	$$F = \left[\begin{array}
	{c} G \\ H
\end{array} \right] = \left[ \begin{array}
	{c} \nabla_{\theta} \; g(Z,\theta_0) \\ \nabla_{\theta} \; h(Z,\theta_0)
\end{array}\right]$$
and
	$$\Omega = Var\left[ f(Z,\theta_0) \right] = \left[\begin{array}
		{cc}
		\Omega_{gg} & \Omega_{gh}\\
		\Omega_{hg} & \Omega_{hh}
	\end{array} \right]$$


\paragraph{High-Level Condition I -- Expansion for GMM}
$$\sqrt{n}\left(\widehat{\theta}_S - \theta_0\right) = -K_S\sqrt{n}\left[\Xi_S f_n(\theta_0)\right] + o_p(1)$$
Where
	\begin{eqnarray*}
	 	F_S &=& \Xi_S F\\
	 	W_S &=& \Xi_S W\Xi_S'\\
	 	M_S &=& \Xi_S M\\
	 	\Omega_S &=& \Xi_S \Omega\Xi_S'\\
	 	K_S  &=& [F_S'W_SF_S]^{-1} F_S'W_S
	 \end{eqnarray*} 



\paragraph{High-Level Condition II -- CLT} $\sqrt{n}f_n(\theta_0) \overset{d}{\rightarrow}M$ where
	$$M = \left[\begin{array}{c} M_g \\ M_h\end{array} \right] \sim N\left( \left[\begin{array}{c}0\\ \tau \end{array} \right], \Omega \right)$$

\paragraph{Asymptotic Distribution of GMM Estimator} $\sqrt{n}(\widehat{\theta}_S - \theta_0 ) \rightarrow_d-K_S  M_S$

\paragraph{Target Parameter} $\mu_0 = \mu(\theta_0)$, $\widehat{\mu}_S = \mu(\widehat{\theta}_S)$ where $\mu(\cdot)$ is differentiable.

\paragraph{Asymptotic Distribution of $\widehat{\mu}_S$} 
$$\sqrt{n}\left(\widehat{\mu}_S - \mu_0\right)\rightarrow_d-\nabla_\theta\mu(\theta_0)'K_S M_S$$
$$\mbox{AMSE}\left(\widehat{\mu}_S\right) = \nabla_\theta\mu(\theta_0)'K_S \Xi_S \left\{\left[\begin{array}{cc}0&0\\0&\tau\tau'\end{array}\right] + \Omega\right\}\Xi_S'K_S'\nabla_\theta\mu(\theta_0)$$

\paragraph{Asymptotically Unbiased Estimator of $\tau$} Here is where we use the identification condition. Let $\widehat{\theta}_v$ denote the \emph{valid estimator}, that is the estimator that uses only moment conditions in $g$. Our identification assumption ensures that this estimator identifies $\theta_0$. We have
\begin{eqnarray*}
	\sqrt{n}\left(\widehat{\theta}_v - \theta_0 \right) &=&  -K_v \sqrt{n}g_n(\theta_0) + o_p(1)\\
	&\overset{d}{\rightarrow}& -K_v M_h
\end{eqnarray*}
So this estimator has \emph{no asymptotic bias}. Under local mis-specification, no consistent estimator of $\tau$ exists, but we can construct an asymptotically unbiased estimator by plugging $\widehat{\theta}_v$ into the sample analogue of the $h$ moment conditions as follows:


valid estimator uses only moment conditions in $g$: $\sqrt{n}\left(\widehat{\theta}_v - \theta_0 \right) =  -K_v \sart{n}g_n(\theta_0) + o_p(1) \overset{d}{\rightarrow} -K_v M_h$

Suppose that $p\geq r$. Then, $\widehat{\tau} = \sqrt{n} h_n(\widehat{\theta}_v) \rightarrow_d\Psi M$ where $\Psi = \left[\begin{array}{cc} -HK_v & \mathbf{I}_q \end{array}\right]$. Therefore, we have $\Psi M \sim \mathcal{N}_q(\tau, \Psi \Omega \Psi')$.

By a mean-value expansion:
	\begin{eqnarray*}
	\widehat{\tau} &=& \sqrt{n} h_n\left(\widehat{\theta}_{valid}\right) = \sqrt{n}h_n(\theta_0) + H \sqrt{n}\left(\widehat{\theta}_{valid} - \theta_0\right) + o_p(1)\\
		&=&-HK_{v} \sqrt{n}f_n(\theta_0) + \mathbf{I}_q\sqrt{n}h_n(\theta_0) +o_p(1)\\
		&=& \Psi \sqrt{n}f_n(\theta_0) + o_p(1) \rightarrow_d\Psi M
\end{eqnarray*}

\paragraph{Asymptotically Unbiased Estimator of $\tau\tau'$}
Let $\widehat{\Omega}$ and $\widehat{\Psi}$ be consistent estimators of $\Omega$ and $\Psi$. Then, $ \widehat{\tau}\widehat{\tau}' - \widehat{\Psi}\widehat{\Omega}\widehat{\Psi}' \rightarrow_d\Psi \left(MM' - \Omega\right)\Psi' $. That is, $\widehat{\tau}\widehat{\tau}' - \ \widehat{\Psi}\widehat{\Omega}\widehat{\Psi}' $ provides an asymptotically unbiased estimator of $\tau\tau'$.

\paragraph{The Focused Moment Selection Criterion} The following expression provides an asymptotically unbiased estimator of $\mbox{AMSE}(\widehat{\mu}_S)$
$$\mbox{FMSC}_n(S) = \nabla_\theta\mu(\widehat{\theta})'\widehat{K}_S\Xi_S \left\{\left[\begin{array}{cc}0&0\\0&\widehat{\tau}\widehat{\tau}' - \widehat{\Psi}\widehat{\Omega}\widehat{\Psi}'\end{array}\right] + \widehat{\Omega}\right\}\Xi_S'\widehat{K}_S' \nabla_\theta\mu(\widehat{\theta})$$

\subsection{A Very Simple Example}
This is the simplest possible example of the FMSC: choosing between OLS and 2SLS. Suppose that we have a vector of valid instruments $\mathbf{z}$ and we want to estimate the coefficient $\beta$ on a single endogenous regressor $x$ in the following linear system:
    	\begin{eqnarray*}
			y_{i} &=& \beta x_{i}  + \epsilon_{i}\\
	x_{i} &=& \mathbf{z}_{i}' \boldsymbol{\pi} + v_{i}
		\end{eqnarray*}
It's no problem accomodating additional exogenous control regressors: just project them out of the system before proceeding. 

Now, if we want to estimate $\beta$, the one option is to use the 2SLS estimator
	$$\widetilde{\beta} = \left[ \textbf{x}'P_Z\textbf{x}\right]^{-1}\textbf{x}'P_Z \textbf{y}$$
Another option is to use the OLS estimator
	$$\widehat{\beta} = \left(\textbf{x}'\textbf{x} \right)^{-1}\textbf{x}'\textbf{y}$$
But why on earth would we ever want to use OLS? If $x$ is endogenous and we have some valid instruments, shouldn't we use 2SLS? The answer, as you may have guessed is: ``it depends: there's a bias-variance tradeoff.'' By using 2SLS, we guarantee that our estimator will be asymptotically unbiased, but this comes at the cost of a much higher asymptotic variance. If $x$ is not \emph{too endogenous} it could make sense to use OLS rather than IV. This is exactly the idea that the FMSC tries to capture.

To put this problem into the FMSC framework, we write
	 $$E_n \left[\begin{array}{c} \mathbf{z}_i \epsilon_i \\ x_i \epsilon_i \end{array}\right] = \left[\begin{array}{c} \mathbf{0} \\ \tau/\sqrt{n} \end{array}\right]$$
Because everything is linear, it's straightforward to derive the limiting distributions of the OLS and 2SLS estimators. After some algebra, we find that the AMSE expressions take a very simple form:
  \begin{eqnarray*}
  \mbox{AMSE(OLS)} &=& \frac{\tau^2}{\sigma_x^4} + \frac{\sigma_\epsilon^2}{\sigma_x^2}\\
  \mbox{AMSE(2SLS)} &=& \frac{\sigma_\epsilon^2}{\gamma^2}
  \end{eqnarray*}
where $\sigma_x^2 = \gamma^2 + \sigma_v^2$, $\gamma^2 = \boldsymbol{\pi}'Q_z \boldsymbol{\pi}$, and $Q_z =\mbox{plim} \; Z'Z/n$. Thus, the AMSE of the OLS estimator is lower than that of the IV estimator whenever
$$\frac{\tau^2}{\sigma_v^2 \sigma_\epsilon^2}  < \frac{\boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2}{\boldsymbol{\pi}'Q_z \boldsymbol{\pi}} = \frac{\sigma_x^2}{\gamma^2} $$
The usual estimators of $\sigma_x^2$, $\gamma^2$, and $\sigma_v^2$ remain consistent under local mis-specification:
  \begin{eqnarray*}
     \widehat{\sigma}_x^2 &=& n^{-1}\mathbf{x}' \mathbf{x} \overset{p}{\rightarrow} \boldsymbol{\pi}'Q_z \boldsymbol{\pi} + \sigma_v^2\\
      \widehat{\gamma}^2 &=& n^{-1}\mathbf{x}' Z (Z'Z)^{-1}Z' \mathbf{x} \overset{p}{\rightarrow} \boldsymbol{\pi}'Q_z \boldsymbol{\pi} \\
      \widehat{\sigma}_v^2 &=& \widehat{\sigma}_x^2 - \widehat{\gamma}^2
  \end{eqnarray*}
To get a consistent estimator of $\sigma_\epsilon^2$ under local mis-specification, we can use either the residuals from OLS or 2SLS, but 2SLS may be more robust. To implement the FMSC for this problem, we simply need an asymptotically unbiased estimator of $\tau^2$. 

The asymptotically unbiased estimator of $\tau$ for this problem is 
  $$\widehat{\tau} = \sqrt{n} \left[\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})/n\right] = n^{-1/2}\mathbf{x}'(\mathbf{y} - \mathbf{x}\widetilde{\beta})$$ 
since $\widehat{\tau} \overset{d}{\rightarrow} N(\tau, V)$ where
    $$V = \sigma^2_\epsilon \sigma^2_x \left( \frac{\sigma^2_v}{\gamma^2}\right)$$
Hence
	$$\widehat{\tau}^2 - \widehat{\sigma}_\epsilon^2\widehat{\sigma}_x^2 \left(\frac{\widehat{\sigma}_v^2}{\widehat{\gamma}^2}\right)$$
is an asymptotically unbiased estimator of $\tau^2$. Substituting this quantity and rearranging, the FMSC tells us to use the OLS estimator whenever
	$$\widehat{T}_{FMSC} = \frac{\widehat{\tau}^2 \widehat{\gamma}^2}{\widehat{\sigma}_v^2 \widehat{\sigma}_\epsilon^2 \widehat{\sigma}_x^2} < 2$$
After some algebra, it turns out that $\widehat{T}_{FMSC}$ is \emph{numerically equivalent} to the Hausman Test statistic and that $\widehat{T}_{FMSC} \underset{d}{\rightarrow} \chi^2(1)$ when $\tau = 0$. Thus, in the simple example of choosing between OLS and 2SLS, the FMSC is identical to carrying out a Hausman Test with a critical value of 2, which corresponds to a 16\% significance level. Notice that this is exactly the same significance level that appeared when we interpreted the AIC as a hypothesis test in our simple example of estimating a normal mean! 
\end{document}