\section{Model selection via a Hold-out Sample}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Model Selection using a Hold-out Sample}

  \begin{itemize}
    \item The real problem is \alert{double} use of the data: first for estimation, then for model comparison. \pause
      \begin{itemize}
        \item Maximized sample log-likelihood is an overly optimistic estimate of expected log-likelihood and hence KL-divergence \pause
        \item In-sample squared prediction error is an overly optimistic estimator of out-of-sample squared prediction error \pause
      \end{itemize}
  \item AIC/TIC, AIC$_c$, BIC, $C_p$ \alert{penalize} sample log-likelihood or RSS to compensate. \pause
  \item Another idea: \alert{don't re-use the same data!}
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Some tikz macros
\tikzstyle{myline} = [draw, ->,  thick, shorten <=4pt, shorten >=4pt]
\tikzstyle{myyellow} = [rectangle,rounded corners,draw=black, top color=white, bottom color=yellow!50,very thick, inner sep=1em, minimum size=3em, text centered, text width = 6em]
\tikzstyle{myyellowsmall} = [rectangle,rounded corners,draw=black, top color=white, bottom color=yellow!50,very thick, inner sep=1em, minimum size=2em, text centered, text width = 3em]
\tikzstyle{myblue} = [rectangle,rounded corners, draw=black, top color=white, bottom color=blue!18,very thick, inner sep=0.5em, minimum size=3em, text centered, text width = 7em, minimum height = 4.5em]
\tikzstyle{mygreen} = [rectangle,rounded corners,draw=black, top color=white, bottom color=green!30,very thick, inner sep=1em, minimum size=3em, text centered, text width = 8em]
\tikzstyle{myred} = [rectangle,rounded corners,draw=black, top color=white, bottom color=red!30,very thick, inner sep=1em, minimum size=3em, text centered, text width = 6em]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Hold-out Sample: Partition the Full Dataset}

\begin{figure}
\centering
\begin{tikzpicture}[node distance=1cm]
\node[myred] (full) {Full Dataset};
\node[myyellow, below left =of full] (train) {Training Set};
\node[myyellow, below right =of full] (test) {Test Set};
\node[myblue, below =of train] (est) {Estimate models: $\widehat{\theta}_1, \dots, \widehat{\theta}_M$};
\node[myblue, below =of test] (eval) {Evaluate fit of $\widehat{\theta}_1, \dots, \widehat{\theta}_M$ against Test Set};

\path [myline] (full) -- (train);
\path [myline] (full) -- (test);
\path [myline] (train) -- (est);
\path [myline] (est) -- (eval);
\path [myline] (test) -- (eval);

\end{tikzpicture}
\end{figure}
\alert{Unfortunately this is extremely wasteful of data\dots}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$K$-fold Cross-Validation: ``Pseudo-out-of-sample''}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.85, transform shape, node distance=1cm]
\node[myred] (full) {Full Dataset};
\node[myyellowsmall, below =of full] (fold2) {Fold 2};
\node[myyellowsmall, left =of fold2] (fold1) {Fold 1};
\node[myyellowsmall, right =of fold2] (foldK) {Fold $K$};

\path [myline] (full) -- (fold1);
\path [myline] (full) -- (fold2);
\path [myline] (full) -- (foldK);
\path (fold2) -- (foldK) node[pos=.5] (dots) {$\dots$};
\end{tikzpicture}
\end{figure}

\vspace{-2em}

  \begin{block}{Step 1}
    Randomly partition full dataset into $K$ \alert{folds} of approx.\ equal size. 
  \end{block}

  \pause

  \begin{block}{Step 2}
    Treat $k$\textsuperscript{th} fold as a hold-out sample and estimate model using all observations \alert{except} those in fold $k$: yielding estimator $\widehat{\theta}(-k)$.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{K-fold Cross-validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$K$-fold Cross-Validation: ``Pseudo-out-of-sample''}

  \begin{block}{Step 2}
    Treat $k$\textsuperscript{th} fold as a hold-out sample and estimate model using all observations \alert{except} those in fold $k$: yielding estimator $\widehat{\theta}(-k)$.
  \end{block}

  \pause
  \begin{block}{Step 3}
    Repeat Step 2 for each $k = 1, \dots, K$. 
  \end{block}

  \pause

  \begin{block}{Step 4}
    For each $t$ calculate the prediction $\widehat{y}_t^{-k(t)}$ of $y_t$ based on $\widehat{\theta}(-k(t))$, the estimator that excluded observation $t$. 
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$K$-fold Cross-Validation: ``Pseudo-out-of-sample''}

  \begin{block}{Step 4}
    For each $t$ calculate the prediction $\widehat{y}_t^{-k(t)}$ of $y_t$ based on $\widehat{\theta}(-k(t))$, the estimator that excluded observation $t$. 
  \end{block}

  \pause

  \begin{block}{Step 5}
    Define $\text{CV}_K = \frac{1}{T}\sum_{t=1}^T L\left(y_t, \widehat{y}_t^{-k(t)}\right)$ where $L$ is a loss function. 
  \end{block}

  \pause

  \begin{block}{Step 5}
    Repeat for each model \& choose $m$ to minimize $\text{CV}_K(m)$.
  \end{block}

  \alert{CV uses each observation for parameter estimation and model evaluation but never at the same time!}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Cross-Validation (CV): Some Details}


  \begin{block}{Which Loss Function?}
  \small
    \vspace{-1em}
    \begin{itemize}
      \item For regression squared error loss makes sense \pause
      \item For classification (discrete prediction) could use zero-one loss. \pause
      \item Can also use log-likelihood/KL-divergence as a loss function\dots 
    \end{itemize}
  \end{block}

  \pause

    \vspace{-1em}

  \begin{block}{How Many Folds?}
    \small
    \vspace{-1em}
    \begin{itemize}
      \item One extreme: $K=2$. Closest to Training/Test idea. \pause
      \item Other extreme: $K=T$ \alert{Leave-one-out} CV (LOO-CV). \pause
      \item Computationally expensive model $\Rightarrow$ may prefer fewer folds. \pause
      \item If your model is a linear smoother there's a computational trick that makes LOO-CV extremely fast. (Problem Set) \pause
      \item Asymptotic properties are related to $K$\dots
    \end{itemize}
    
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotic Equivalence Between LOO-CV and TIC}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Relationship between LOO-CV and TIC}
  \begin{block}{Theorem}
  LOO-CV using KL-divergence as the loss function is asymptotically equivalent to TIC but doesn't require us to estimate the Hessian and variance of the score. 
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Large-sample Equivalence of LOO-CV and TIC}
  
  \begin{block}{Notation and Assumptions}
    For simplicity let $Y_1, \dots, Y_T \sim \text{iid}$.
    Let $\widehat{\theta}_{(t)}$ be the maximum likelihood estimator based on all observations \alert{except $t$} and $\widehat{\theta}$ be the full-sample estimator.
  \end{block}

  \pause

  \begin{block}{Log-likelihood as ``Loss''}
    $\text{CV}_1 = \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta}_{(t)})$ but since min.\ KL = max.\ log-like.\ we choose the model with \alert{highest} $\text{CV}_1(m)$.
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Overview of the Proof}
    
    First-Order Taylor Expansion of $\widehat{\theta}_{(t)}$ around $\widehat{\theta}$:

    \vspace{-2em}

	\begin{eqnarray*}
		CV_1 &=& \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta}_{(t)})\\ \pause
			&=&\frac{1}{T} \sum_{t=1}^T \left[\log f(y_t|\widehat{\theta}) + \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) \right] + o_p(1)\\ \pause
			&=& \frac{\ell(\widehat{\theta})}{T} + \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) + o_p(1)
	\end{eqnarray*}

  \alert{Crucial point: the first-order term is not zero in this case. (Why?)} 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
  \frametitle{Overview of Proof}
  
From expansion on previous slide, we simply need to show that:
\vspace{-1em}

\[\frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) = -\frac{1}{T}\mbox{tr}\left(\widehat{J}^{-1} \widehat{K} \right) + o_p(1)\]

\small
\begin{align*}
  \widehat{K}&= \frac{1}{T}\sum_{t=1}^T \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right)' \\
  \widehat{J} &= -\frac{1}{T}\sum_{t=1}^T \frac{\partial\log f(y_t|\widehat{\theta})}{\partial\theta \partial \theta'}
\end{align*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Overview of Proof}

By the definition of $\widehat{K}$ and the properties of the trace operator:
\footnotesize
\begin{eqnarray*}
		-\frac{1}{T}\mbox{tr}\left\{\widehat{J}^{-1} \widehat{K} \right\}
		&=& -\frac{1}{T}\mbox{tr}\left\{\widehat{J}^{-1}\left[\frac{1}{T}\sum_{t=1}^T \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) '\right]\right\}\\ \pause
    &=& \left[\frac{1}{T}\sum_{t=1}^T \mbox{tr}\left\{\frac{-\widehat{J}^{-1}}{T}\left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) '\right\}\right]\\ \pause
    &=& \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'} \left(-\frac{1}{T}\widehat{J}^{-1}\right)\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta} 
\end{eqnarray*}

\pause
\normalsize
So it suffices to show that
\vspace{-1em}

\[
  \alert{\left( \widehat{\theta}_{(t)} - \widehat{\theta}  \right) = -\frac{1}{T} \widehat{J}^{-1} \left[\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right] + o_p(1)}
\]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Influence Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Digression: Functionals and Influence Functions}

  \begin{block}{(Statistical) Functional}
    $\mathbb{T}=\mathbb{T}(G)$ maps a CDF $G$ to $\mathbb{R}^p$.
  \end{block}

  \pause

  \begin{block}{Example: ML Estimation}
    \vspace{-2em}
    \[\theta_0 = \mathbb{T}(G) = \underset{\theta \in \Theta}{\arg \min} \;E_G\left[\log\left\{\frac{g(Y)}{f(Y|\theta)} \right\} \right]\]
  \end{block}

  \pause

  \begin{block}{Influence Function}
    Let $\delta_y$ be a \alert{point mass} at $y$: $\delta_y(y) = 1$, $\delta_y(y')=0$ for $y'\neq y$.
    Influence function = functional derivative: how does a small change in $G$ affect $\mathbb{T}$?

    \[\text{infl}(G,y) = \lim_{\epsilon \rightarrow 0} \frac{\mathbb{T}\left[\left(1-\epsilon\right)G + \epsilon \delta_y\right] - \mathbb{T}(G)}{\epsilon}\]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Back to the Proof\dots}
 

  \begin{block}{Step 1}
    The influence function for ML estimation turns out to be $\text{infl}(G,y) = J^{-1} \frac{\partial}{\partial \theta}\log f(y|\theta_0)$.
  \end{block}

  \pause

  \begin{block}{Step 2}
    Let $\widehat{G}$ denote the empirical CDF based on $y_1, \dots, y_T$. Then:
    \vspace{-1em}

   \[
     \left( \widehat{\theta}_{(t)} - \widehat{\theta} \right) = -\frac{1}{T} \text{infl}(\widehat{G}, y_t) + o_p(1)
   \]
  \end{block}

  \pause

  \vspace{-2em}

  \begin{block}{Step 3}
    Evaluating Step 1 at $\widehat{G}$ and substituting into Step 2

    \vspace{-1em}

\[
  \alert{\left( \widehat{\theta}_{(t)} - \widehat{\theta}  \right) = -\frac{1}{T} \widehat{J}^{-1} \left[\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right] + o_p(1)}
\]
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
