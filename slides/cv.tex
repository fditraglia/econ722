%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{K-fold Cross-validation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Model Selection using a Hold-out Sample}

  \begin{itemize}
    \item The real problem is \alert{double} use of the data: first for estimation, then for model comparison. 
      \begin{itemize}
        \item Maximized sample log-likelihood is an overly optimistic estimate of expected log-likelihood and hence KL-divergence 
        \item In-sample squared prediction error is an overly optimistic estimator of out-of-sample squared prediction error 
      \end{itemize}
  \item AIC/TIC, AIC$_c$, BIC, $C_p$ \alert{penalize} sample log-likelihood or RSS to compensate. 
  \item Another idea: \alert{don't re-use the same data!}
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Some tikz macros
\tikzstyle{myline} = [draw, ->,  thick, shorten <=4pt, shorten >=4pt]
\tikzstyle{myyellow} = [rectangle,rounded corners,draw=black, top color=white, bottom color=yellow!50,very thick, inner sep=1em, minimum size=3em, text centered, text width = 6em]
\tikzstyle{myyellowsmall} = [rectangle,rounded corners,draw=black, top color=white, bottom color=yellow!50,very thick, inner sep=1em, minimum size=2em, text centered, text width = 3em]
\tikzstyle{myblue} = [rectangle,rounded corners, draw=black, top color=white, bottom color=blue!18,very thick, inner sep=0.5em, minimum size=3em, text centered, text width = 7em, minimum height = 4.5em]
\tikzstyle{mygreen} = [rectangle,rounded corners,draw=black, top color=white, bottom color=green!30,very thick, inner sep=1em, minimum size=3em, text centered, text width = 8em]
\tikzstyle{myred} = [rectangle,rounded corners,draw=black, top color=white, bottom color=red!30,very thick, inner sep=1em, minimum size=3em, text centered, text width = 6em]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Hold-out Sample: Partition the Full Dataset}

\begin{figure}
\centering
\begin{tikzpicture}[node distance=1cm]
\node[myred] (full) {Full Dataset};
\node[myyellow, below left =of full] (train) {Training Set};
\node[myyellow, below right =of full] (test) {Test Set};
\node[myblue, below =of train] (est) {Estimate models: $\widehat{\theta}_1, \dots, \widehat{\theta}_M$};
\node[myblue, below =of test] (eval) {Evaluate fit of $\widehat{\theta}_1, \dots, \widehat{\theta}_M$ against Test Set};

\path [myline] (full) -- (train);
\path [myline] (full) -- (test);
\path [myline] (train) -- (est);
\path [myline] (est) -- (eval);
\path [myline] (test) -- (eval);

\end{tikzpicture}
\end{figure}
\alert{Unfortunately this is extremely wasteful of data\dots}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$K$-fold Cross-Validation: ``Pseudo-out-of-sample''}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.85, transform shape, node distance=1cm]
\node[myred] (full) {Full Dataset};
\node[myyellowsmall, below =of full] (fold2) {Fold 2};
\node[myyellowsmall, left =of fold2] (fold1) {Fold 1};
\node[myyellowsmall, right =of fold2] (foldK) {Fold $K$};

\path [myline] (full) -- (fold1);
\path [myline] (full) -- (fold2);
\path [myline] (full) -- (foldK);
\path (fold2) -- (foldK) node[pos=.5] (dots) {$\dots$};
\end{tikzpicture}
\end{figure}

\vspace{-2em}

  \begin{block}{Step 1}
    Randomly partition full dataset into $K$ \alert{folds} of approx.\ equal size. 
  \end{block}

  

  \begin{block}{Step 2}
    Treat $k$\textsuperscript{th} fold as a hold-out sample and estimate model using all observations \alert{except} those in fold $k$: yielding estimator $\widehat{\theta}(-k)$.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$K$-fold Cross-Validation: ``Pseudo-out-of-sample''}

  \begin{block}{Step 2}
    Treat $k$\textsuperscript{th} fold as a hold-out sample and estimate model using all observations \alert{except} those in fold $k$: yielding estimator $\widehat{\theta}(-k)$.
  \end{block}

  
  \begin{block}{Step 3}
    Repeat Step 2 for each $k = 1, \dots, K$. 
  \end{block}

  

  \begin{block}{Step 4}
    For each $t$ calculate the prediction $\widehat{y}_t^{-k(t)}$ of $y_t$ based on $\widehat{\theta}(-k(t))$, the estimator that excluded observation $t$. 
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$K$-fold Cross-Validation: ``Pseudo-out-of-sample''}

  \begin{block}{Step 4}
    For each $t$ calculate the prediction $\widehat{y}_t^{-k(t)}$ of $y_t$ based on $\widehat{\theta}(-k(t))$, the estimator that excluded observation $t$. 
  \end{block}

  

  \begin{block}{Step 5}
    Define $\text{CV}_K = \frac{1}{T}\sum_{t=1}^T L\left(y_t, \widehat{y}_t^{-k(t)}\right)$ where $L$ is a loss function. 
  \end{block}

  

  \begin{block}{Step 5}
    Repeat for each model \& choose $m$ to minimize $\text{CV}_K(m)$.
  \end{block}

  \alert{CV uses each observation for parameter estimation and model evaluation but never at the same time!}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Cross-Validation (CV): Some Details}


  \begin{block}{Which Loss Function?}
  \small
    \vspace{-1em}
    \begin{itemize}
      \item For regression squared error loss makes sense 
      \item For classification (discrete prediction) could use zero-one loss. 
      \item Can also use log-likelihood/KL-divergence as a loss function\dots 
    \end{itemize}
  \end{block}

  

    \vspace{-1em}

  \begin{block}{How Many Folds?}
    \small
    \vspace{-1em}
    \begin{itemize}
      \item One extreme: $K=2$. Closest to Training/Test idea. 
      \item Other extreme: $K=T$ \alert{Leave-one-out} CV (LOO-CV). 
      \item Computationally expensive model $\Rightarrow$ may prefer fewer folds. 
      \item If your model is a linear smoother there's a computational trick that makes LOO-CV extremely fast. (Problem Set) 
      \item Asymptotic properties are related to $K$\dots
    \end{itemize}
    
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotic Equivalence Between LOO-CV and TIC}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Relationship between LOO-CV and TIC}
  \begin{block}{Theorem}
  LOO-CV using KL-divergence as the loss function is asymptotically equivalent to TIC but doesn't require us to estimate the Hessian and variance of the score. 
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Large-sample Equivalence of LOO-CV and TIC}
  
  \begin{block}{Notation and Assumptions}
    For simplicity let $Y_1, \dots, Y_T \sim \text{iid}$.
    Let $\widehat{\theta}_{(t)}$ be the maximum likelihood estimator based on all observations \alert{except $t$} and $\widehat{\theta}$ be the full-sample estimator.
  \end{block}

  

  \begin{block}{Log-likelihood as ``Loss''}
    $\text{CV}_1 = \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta}_{(t)})$ but since min.\ KL = max.\ log-like.\ we choose the model with \alert{highest} $\text{CV}_1(m)$.
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Overview of the Proof}
    
  First-Order Taylor Expansion of $\log f\big(y_t|\widehat{\theta}_{(t)}\big)$ around $\widehat{\theta}$:

    \vspace{-2em}

	\begin{eqnarray*}
		CV_1 &=& \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta}_{(t)})\\ 
			&=&\frac{1}{T} \sum_{t=1}^T \left[\log f(y_t|\widehat{\theta}) + \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) \right] + o_p(1)\\ 
			&=& \frac{\ell(\widehat{\theta})}{T} + \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) + o_p(1)
	\end{eqnarray*}

  \alert{Why isn't the first-order term zero in this case?}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Important Side Point}

  \begin{block}{Definition of ML Estimator}
    \[
      \frac{\partial \ell(\widehat{\theta})}{\partial \theta'} = \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta)}}{\partial \theta} = 0
    \]
  \end{block}

  \begin{alertblock}{In Contrast}
    \footnotesize
    \begin{align*}
      \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) &= 
      \left[\frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'} \widehat{\theta}_{(t)}\right] - \widehat{\theta}\left[\frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\right]\\
      &= \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'} \widehat{\theta}_{(t)} \neq 0
    \end{align*}
  \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} 
  \frametitle{Overview of Proof}
  
From expansion two slides back, we simply need to show that:
\vspace{-1em}

\[\frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) = -\frac{1}{T}\mbox{tr}\left(\widehat{J}^{-1} \widehat{K} \right) + o_p(1)\]

\small
\begin{align*}
  \widehat{K}&= \frac{1}{T}\sum_{t=1}^T \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right)' \\
  \widehat{J} &= -\frac{1}{T}\sum_{t=1}^T \frac{\partial\log f(y_t|\widehat{\theta})}{\partial\theta \partial \theta'}
\end{align*}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Overview of Proof}

By the definition of $\widehat{K}$ and the properties of the trace operator:
\footnotesize
\begin{eqnarray*}
		-\frac{1}{T}\mbox{tr}\left\{\widehat{J}^{-1} \widehat{K} \right\}
		&=& -\frac{1}{T}\mbox{tr}\left\{\widehat{J}^{-1}\left[\frac{1}{T}\sum_{t=1}^T \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) '\right]\right\}\\ 
    &=& \left[\frac{1}{T}\sum_{t=1}^T \mbox{tr}\left\{\frac{-\widehat{J}^{-1}}{T}\left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) '\right\}\right]\\ 
    &=& \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'} \left(-\frac{1}{T}\widehat{J}^{-1}\right)\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta} 
\end{eqnarray*}


\normalsize
So it suffices to show that
\vspace{-1em}

\[
  \alert{\left( \widehat{\theta}_{(t)} - \widehat{\theta}  \right) = -\frac{1}{T} \widehat{J}^{-1} \left[\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right] + o_p(1)}
\]

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What is an Influence Function?}

  \begin{block}{Statistical Functional}
    $\mathbb{T}=\mathbb{T}(G)$ maps a CDF $G$ to $\mathbb{R}^p$.
  \end{block}

  

  \begin{block}{Example: ML Estimation}
    \vspace{-2em}
    \[\theta_0 = \mathbb{T}(G) = \underset{\theta \in \Theta}{\arg \min} \;E_G\left[\log\left\{\frac{g(Y)}{f(Y|\theta)} \right\} \right]\]
  \end{block}

  

  \begin{block}{Influence Function}
    Let $\delta_y$ be the CDF of a \alert{point mass} at $y$:  $\delta_y(a) = \mathbbm{1}\left\{ y \leq a \right\}$.
    Influence function = functional derivative: how does a small change in $G$ affect $\mathbb{T}$?

    \[\text{infl}(G,y) = \lim_{\epsilon \rightarrow 0} \frac{\mathbb{T}\left[\left(1-\epsilon\right)G + \epsilon \delta_y\right] - \mathbb{T}(G)}{\epsilon}\]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Relating Influence Functions to $\widehat{\theta}_{(t)}$}

  \begin{block}{Empirical CDF $\widehat{G}$}
    \vspace{-1em}
    \[\widehat{G}(a) = \displaystyle\frac{1}{T}\sum_{t=1}^T \mathbbm{1}\left\{ y_t \leq a \right\} = \displaystyle\frac{1}{T} \sum_{t=1}^T \delta_{y_t}(a)\]
  \end{block}

  \vspace{-1em}

  \begin{block}{Relation to ``LOO'' Empirical CDF $\widehat{G}_{(t)}$}
    \vspace{-1em}
  \[\displaystyle \widehat{G} = \left( 1 - \frac{1}{T} \right) \widehat{G}_{(t)} + \frac{\delta_{y_t}}{T}\]
  \end{block}

  \vspace{-1em}

  \begin{block}{Applying $\mathbb{T}$ to both sides\dots}
    \vspace{-1em}
   \[ 
      \mathbb{T}(\widehat{G}) = \mathbb{T}\left( (1 - 1/T) \widehat{G}_{(t)} + \delta_{y_t}/T\right)
    \]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Relating Influence Functions to $\widehat{\theta}_{(t)}$}
  Some algebra, followed by taking $\varepsilon = 1/T$ to zero gives:
  \begin{align*}
      \mathbb{T}(\widehat{G}) &= \mathbb{T}\left( (1 - 1/T) \widehat{G}_{(t)} + \delta_{y_t}/T\right)\\
      \mathbb{T}(\widehat{G}) - \mathbb{T}(\widehat{G}_{(t)}) &= \mathbb{T}\left( (1 - 1/T) \widehat{G}_{(t)} + \delta_{y_t}/T\right) - \mathbb{T}(\widehat{G}_{(t)})\\
      \mathbb{T}(\widehat{G}) - \mathbb{T}(\widehat{G}_{(t)}) &= \frac{1}{T}\left[ \frac{\mathbb{T}\left( (1 - 1/T) \widehat{G}_{(t)} + \delta_{y_t}/T\right) - \mathbb{T}(\widehat{G}_{(t)})}{1/T}\right]\\
      \mathbb{T}(\widehat{G}) - \mathbb{T}(\widehat{G}_{(t)}) &= \frac{1}{T} \mbox{infl}\left(\widehat{G}_{(t)}, y_t\right) + o_p(1)\\
      \widehat{\theta} - \widehat{\theta}_{(t)} &= \frac{1}{T} \mbox{infl}\left( \widehat{G},y_t \right) + o_p(1)
    \end{align*}

    \alert{Last step: difference between having $\widehat{G}$ vs.\ $\widehat{G}_{(t)}$ in $\mbox{infl}$ is negligible}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Steps for Last part of TIC/LOO-CV Equivalence Proof} 
 

  \begin{block}{Step 1}
    Let $\widehat{G}$ denote the empirical CDF based on $y_1, \dots, y_T$. Then:
    \vspace{-1em}

   \[
     \left( \widehat{\theta}_{(t)} - \widehat{\theta} \right) = -\frac{1}{T} \text{infl}(\widehat{G}, y_t) + o_p(1)
   \]
  \end{block}

  \vspace{-1em}

  \begin{block}{Step 2}
    Lecture Notes: For ML, $\text{infl}(G,y) = J^{-1} \frac{\partial}{\partial \theta}\log f(y|\theta_0)$. 
  \end{block}



  \begin{block}{Step 3}
    Evaluating Step 2 at $\widehat{G}$ and substituting into Step 2

    \vspace{-1em}

\[
  \alert{\left( \widehat{\theta}_{(t)} - \widehat{\theta}  \right) = -\frac{1}{T} \widehat{J}^{-1} \left[\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right] + o_p(1)}
\]
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
