%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kullback-Leibler Divergence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Kullback-Leibler (KL) Divergence}

  \begin{block}{Motivation}
    How well does a given density $f(y)$ approximate an unknown true density $g(y)$? Use this to select between parametric models.
  \end{block}

  \pause

  \begin{alertblock}{Definition}
    \vspace{-3em}
  \[
    \text{KL}(g;f) = \underbrace{\mathbb{E}_G \left[ \log\left\{ \frac{g(Y)}{f(Y)} \right\}\right]}_{\text{True density on top}} 
    = \underbrace{\mathbb{E}_G\left[ \log g(Y) \right]}_{\substack{\text{Depends only on truth} \\ \text{Fixed across models}}} - \underbrace{\mathbb{E}_G \left[ \log f(Y) \right]}_{\substack{\text{Expected} \\ \text{log-likelihood}}}
  \]
\end{alertblock}

\vspace{-2.5em}

\pause

\begin{block}{Properties}
  \begin{itemize}
    \item \emph{Not} symmetric: $\text{KL}(g;f) \neq \text{KL}(f;g)$
    \item By Jensen's Inequality: $\text{KL}(g;f) \geq 0$ (strict iff $g=f$ a.e.)
    \item Minimize KL $\iff$ Maximize Expected log-likelihood
  \end{itemize}

\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$\text{KL}(g;f) \geq0$ with equality iff $g = f$ almost surely}

  \begin{block}{Jensen's Inequality}
    If $\varphi$ is convex, then $\varphi(\mathbb{E}[X]) \leq \mathbb{E}[\varphi(X)]$, with strict equality when $\varphi$ is affine or $X$ is constant.
  \end{block}

  \begin{block}{$\log$ is concave so $(-\log)$ is convex}
  \begin{align*}
    \mathbb{E}_G\left[ \log\left\{ \frac{g(Y)}{f(Y)} \right\} \right] &= \mathbb{E}_G\left[ -\log\left\{ \frac{f(Y)}{g(Y)} \right\} \right] \geq -\log \left\{ \mathbb{E}_G\left[ \frac{f(Y)}{g(Y)} \right] \right\}\\
    &= -\log \left\{ \int_{-\infty}^{\infty} \frac{f(y)}{g(y)} \cdot g(y)\, dy \right\}\\
    &=-\log \left\{ \int_{-\infty}^{\infty} f(y)\, dy \right\}\\
    &= -\log(1) = 0
  \end{align*}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{KL Divergence and Mis-specified MLE}

  \begin{block}{Pseudo-true Parameter Value $\theta_0$}
    \vspace{-1em}
  \[
    \widehat{\theta}_{MLE} \overset{p}{\rightarrow} \theta_0 \equiv \underset{\theta \in \Theta}{\arg \min} \; \text{KL}(g;f_\theta) = \underset{\theta \in \Theta}{\arg \max} \; \mathbb{E}_G[\log f(Y|\theta)] 
  \]
\end{block}


\pause

\begin{block}{What if $f_\theta$ is correctly specified?}
  If $g = f_\theta$ for some $\theta$ then $\text{KL}(g;f_\theta)$ is minimized at zero.
\end{block}

\pause

\begin{alertblock}{Goal: Compare Mis-specified Models}
  \vspace{-2.5em}
  \begin{align*}
    \mathbb{E}_G \left[ \log f(Y|\theta_0) \right] && \mbox{versus} && 
    \mathbb{E}_G \left[ \log h(Y|\gamma_0) \right]
  \end{align*}
  \vspace{-2em}

 where $\theta_0$ is the pseudo-true parameter value for $f_\theta$ and $\gamma_0$ is the pseudo-true parameter value for $h_\gamma$.
  
\end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias of Maximized Sample Log-Likelihood} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{How to Estimate Expected Log Likelihood?}
  \framesubtitle{For simplicity: $Y_1, \dots, Y_n \sim \mbox{ iid } g(y)$}


  \begin{block}{Unbiased but Infeasible}
    \[
      \mathbb{E}_G \left[\frac{1}{T}\ell(\theta_0)\right] = \mathbb{E}_G\left[ \frac{1}{T} \sum_{t=1}^T \log f(Y_t|\theta_0)\right] = \mathbb{E}_G\left[ \log f(Y|\theta_0) \right]
    \]
    
  \end{block}
   
  \pause

  \vspace{-1em}

  \begin{block}{Biased but Feasible}
    $T^{-1}\ell(\widehat{\theta}_{MLE})$ is a \alert{biased} estimator of $\mathbb{E}_G[\log f(Y|\theta_0)]$. 
  \end{block}

  \pause

  \begin{block}{Intuition for the Bias}
    $T^{-1}\ell(\widehat{\theta}_{MLE}) > T^{-1}\ell(\theta_0)$ unless $\widehat{\theta}_{MLE} = \theta_0$. 
    Maximized sample log-like.\ is an \alert{overly optimistic} estimator of expected log-like.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What to do about this bias?}

  \begin{enumerate}
    \item General-purpose asymptotic approximation of ``degree of over-optimism'' of maximized sample log-likelihood.
      \begin{itemize}
        \item Takeuchi's Information Criterion (TIC)
        \item Akaike's Information Criterion (AIC)
      \end{itemize}
      \pause
    \item Problem-specific finite sample approach, assuming $g \in f_\theta$.
      \begin{itemize}
        \item Corrected AIC (AIC$_c$) of Hurvich and Tsai (1989)
      \end{itemize}
  \end{enumerate}

  \pause
  \begin{alertblock}{Tradeoffs}
   TIC is most general and makes weakest assumptions, but requires very large $T$ to work well. 
   AIC is a good approximation to TIC that requires less data.
   Both AIC and TIC perform poorly when $T$ is small relative to the number of parameters, hence AIC$_c$.
  \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Review of Asymptotics for Mis-specified MLE}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Recall: Asymptotics for Mis-specified ML Estimation}
  \framesubtitle{Model $f(y|\theta)$, pseudo-true parameter $\theta_0$. For simplicity $Y_1, \dots, Y_T \sim \mbox{ iid } g(y)$.}

  \pause

  \begin{block}{Fundamental Expansion}
    \vspace{-1.5em}
  \[
    \alert{\sqrt{T} ( \widehat{\theta} - \theta_0)   = J^{-1}\left(\sqrt{T} \bar{U}_T\right) + o_p(1)}
  \]
  \footnotesize
\[
      J = -\mathbb{E}_G \left[ \frac{\partial \log f(Y|\theta_0)}{\partial \theta \partial \theta'} \right], \quad
      \bar{U}_T =\frac{1}{T} \sum_{t=1}^T \frac{\partial \log f(Y_t|\theta_0)}{\partial \theta}
\] 
\end{block}

\normalsize

\vspace{-1em}

\pause

\begin{block}{Central Limit Theorem}
  \vspace{-1em}
  \footnotesize
  \[
    \sqrt{T} \bar{U}_T \rightarrow_d U \sim N_p(0, K), \quad
    K = \text{Var}_G\left[\frac{\partial \log f(Y|\theta_0)}{\partial \theta}\right]
  \] 
  \normalsize

  \vspace{-1em}

  \[
    \alert{\sqrt{T}(\widehat{\theta} - \theta_0) \rightarrow_d J^{-1}U \sim N_p(0, J^{-1}KJ^{-1})}
  \]
\end{block}

\vspace{-1em}

\pause

\begin{block}{Information Matrix Equality}
  If $g = f_\theta$ for some $\theta \in \Theta$ then $K = J \implies \text{AVAR}(\widehat{\theta}) = J^{-1}$
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deriving AIC and TIC}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Bias Relative to Infeasible Plug-in Estimator}


  \begin{block}{Definition of Bias Term $B$}
  \[
    B = \underbrace{\frac{1}{T} \ell(\widehat{\theta})}_{\substack{ \text{feasible}\\ \text{overly-optimistic}}} - \underbrace{\int g(y) \log f(y|\widehat{\theta})\; dy}_{\substack{\text{uses data only once} \\ \text{infeas.\ not overly-optimistic}}}
  \]
\end{block}

\pause

\begin{alertblock}{Question to Answer}
  On average, over the sampling distribution of $\widehat{\theta}$, how large is $B$? AIC and TIC construct an asymptotic approximation of $\mathbb{E}[B]$.
\end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Derivation of AIC/TIC}
  \begin{block}{Step 1: Taylor Expansion}
    \vspace{-1em}
    \[B = \bar{Z}_T +  (\widehat{\theta} - \theta_0)' J  (\widehat{\theta} - \theta_0) + o_p(T^{-1})\]
    \footnotesize
    \[\bar{Z}_T = \frac{1}{T} \sum_{t=1}^T \left\{ \log f(Y_t|\theta_0) - \mathbb{E}_G[\log f(Y|\theta_0) \right\}\]
  \end{block}

  \vspace{-1em}

  \pause

  \begin{block}{Step 2: $\mathbb{E}[\bar{Z}_T] = 0$}
    \vspace{-1.5em}
    \[\mathbb{E}[B] \approx \mathbb{E}\left[ (\widehat{\theta} - \theta_0)'J(\widehat{\theta} - \theta_0) \right]\]
  \end{block}

  \pause

  \vspace{-1em}

  \begin{block}{Step 3: $\sqrt{T}(\widehat{\theta} - \theta_0) \rightarrow_d J^{-1}U$}
    \vspace{-1.5em}
    \[
      T(\widehat{\theta} - \theta_0)'J(\widehat{\theta} - \theta_0) \rightarrow_d U'J^{-1}U
    \]
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Derivation of AIC/TIC Continued\dots}
  \begin{block}{Step 3: $\sqrt{T}(\widehat{\theta} - \theta_0) \rightarrow_d J^{-1}U$}
    \vspace{-1.5em}
    \[
      T(\widehat{\theta} - \theta_0)'J(\widehat{\theta} - \theta_0) \rightarrow_d U'J^{-1}U
    \]
  \end{block}

  \pause
  
  \begin{block}{Step 4: $U \sim N_p(0, K)$}
    \vspace{-1.5em}
   \[
     \mathbb{E}[B] \approx \frac{1}{T} \mathbb{E}[U'J^{-1}U] = \frac{1}{T} \text{tr}\left\{ J^{-1}K \right\}
   \]
  \end{block}

  \pause

  \vspace{-1em}

  \begin{block}{Final Result:}
    $T^{-1}\text{tr}\left\{ J^{-1}K \right\}$ is an asymp.\ unbiased estimator of the over-optimism of $T^{-1}\ell(\widehat{\theta})$ relative to $\int g(y)\log f(y|\widehat{\theta})\; dy$.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{TIC and AIC}
  \begin{block}{Takeuchi's Information Criterion}
    Multiply by $2T$, estimate $J,K \Rightarrow \alert{\text{TIC} = 2\left[ \ell(\widehat{\theta}) - \text{tr}\left\{ \widehat{J}^{-1}\widehat{K} \right\} \right]}$
  \end{block}

  \pause

  \begin{block}{Akaike's Information Criterion}
    If $g = f_\theta$ then $J = K \Rightarrow \text{tr}\left\{ J^{-1  }K \right\} = p \Rightarrow \alert{\text{AIC} = 2\left[ \ell(\widehat{\theta}) - p \right]}$ 
  \end{block}

  \pause

  \begin{block}{Contrasting AIC and TIC} 
    Technically, AIC requires that all models under consideration are at least correctly specified while TIC doesn't. 
    But $J^{-1}K$ is hard to estimate, and if a model is badly mis-specified, $\ell(\widehat{\theta})$ dominates.
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Corrected AIC (AIC$_c$)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Corrected AIC (AIC$_c$) -- Hurvich \& Tsai (1989)}

  \begin{block}{Idea Behind AIC$_c$}
    Asymptotic approximation used for AIC/TIC works poorly if $p$ is too large relative to $T$. \alert{Try exact, finite-sample approach instead.} 
  \end{block}

  \pause

  \begin{block}{Assumption: True DGP}
    \vspace{-1.5em}
    \[
      \mathbf{y} = \mathbf{X}\boldsymbol{\beta}_0 + \boldsymbol{\varepsilon}, \quad \boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma_0^2 \mathbf{I}_T), \quad k \text{ Regressors}
    \]

    \pause

    \begin{block}{Can Show That}
      \vspace{-1em}
      \small
      \[
        KL(g,f) = \frac{T}{2}\left[ \frac{\sigma_0^2}{\sigma_1^2} - \log \left( \frac{\sigma_0^2}{\sigma_1^2} \right) - 1\right] + \left(\frac{1}{2\sigma_1^2}\right)(\boldsymbol{\beta}_0 - \boldsymbol{\beta}_1)' \mathbf{X}'\mathbf{X} (\boldsymbol{\beta}_0 - \boldsymbol{\beta}_1)
      \]
      \normalsize

      Where $f$ is a normal regression model with parameters $(\boldsymbol{\beta}_1, \sigma_1^2)$ that \alert{might not be the true parameters.} 
    \end{block}
    
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{But how can we use this?}
  \vspace{-1em}
  \small
      \[
        KL(g,f) = \frac{T}{2}\left[ \frac{\sigma_0^2}{\sigma_1^2} - \log \left( \frac{\sigma_0^2}{\sigma_1^2} \right) - 1\right] + \left(\frac{1}{2\sigma_1^2}\right)(\boldsymbol{\beta}_0 - \boldsymbol{\beta}_1)' \mathbf{X}'\mathbf{X} (\boldsymbol{\beta}_0 - \boldsymbol{\beta}_1)
      \]
      \normalsize

      \vspace{-1em}
      \begin{enumerate}
        \item Would need to know $(\beta_1, \sigma_1^2)$ for \alert{candidate model}.
          \begin{itemize}
            \item Easy: just use MLE $(\widehat{\boldsymbol{\beta}}_1, \widehat{\sigma}_1^2)$
          \end{itemize}
        \item Would need to know $(\boldsymbol{\beta}_0, \sigma_0^2)$ for \alert{true model}.
          \begin{itemize}
            \item Very hard! The whole problem is that we don't know these!
          \end{itemize}
      \end{enumerate}

      \pause

      \begin{block}{Hurvich \& Tsai (1989) Assume:}
        \vspace{-0.5em}
        \begin{itemize}
          \item Every candidate model is \alert{at least correctly specified}
          \item Implies any candidate estimator $(\widehat{\boldsymbol{\beta}}, \widehat{\sigma}^2)$ is consistent for truth.
        \end{itemize}

      \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Deriving the Corrected AIC} 
  Since $(\widehat{\boldsymbol{\beta}}, \widehat{\sigma}^2)$ are random, look at $\mathbb{E}[\widehat{KL}]$, where
  \small
      \[
        \widehat{KL} = \frac{T}{2}\left[ \frac{\sigma_0^2}{\widehat{\sigma}^2} - \log \left( \frac{\sigma_0^2}{\widehat{\sigma}^2} \right) - 1\right] + \left( \frac{1}{2 \widehat{\sigma}^2} \right)(\widehat{\boldsymbol{\beta}} -  \boldsymbol{\beta}_0)' \mathbf{X}'\mathbf{X} (\widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0)
      \]

      \pause

      \normalsize
  Finite-sample theory for correctly spec.\ normal regression model:

  \[
    \mathbb{E}\left[ \widehat{KL} \right] = \frac{T}{2}\left\{ \frac{T + k}{T-k-2} - \log(\sigma_0^2) + \mathbb{E}[\log \widehat{\sigma}^2 ] - 1\right\}
  \]
   
  \pause

  \vspace{1em}
  Eliminate constants and scaling, unbiased estimator of $\mathbb{E}[\log \widehat{\sigma}^2]$:
  \vspace{-0.5em}
 \[
   \alert{\text{AIC}_c = \log \widehat{\sigma}^2 + \frac{T+k}{T-k-2}}
 \]

  \vspace{-0.5em}
  a finite-sample unbiased estimator of KL for model comparison

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{AIC for Normal Linear Regression}
%  \begin{itemize}
%    \item Number of \alert{regressors} $ = k$ 
%      \pause
%    \item Number of \alert{parameters} $ = k+1$
%      \pause
%    \item $2 \log(\text{Likelihood}) = -T \log(2\pi) - T \log (\widehat{\sigma}^2) - T$
%      \pause
%    \item $\text{AIC} = -T \log(2\pi) - T \log(\widehat{\sigma}^2) - T - 2(k+1)$
%      \pause
%    \item Maximum Likelihood = Minimum KL so multiply by $-1$ 
%      \pause
%    \item Remove irrelevant constants, divide by $T$
%      \pause
%  \end{itemize}
%
%  \[\widetilde{\text{AIC}} = \log (\widehat{\sigma}^2) + \frac{2(k+1)}{T}\]
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Comparing AIC and AIC$_c$ for Normal Linear Regression}
%
%  \begin{align*}
%    \widetilde{\text{AIC}}(k) &= \log(\widehat{\sigma}^2_k) + \frac{2(k+1)}{T}\\ \\ 
%    \text{AIC}_c(k) &= \log(\widehat{\sigma}^2_k) + \frac{T + k}{T - k - 2}
%  \end{align*}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
