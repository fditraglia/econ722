\section{Ridge Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Ridge Regression -- OLS with an $L_2$ Penalty}


	$$\widehat{\beta}_{Ridge} =\underset{\beta}{\arg \min}\;(\mathbf{y} - X\beta)' (\mathbf{y} - X\beta) + \lambda \beta'\beta$$
  \begin{itemize}
    \item Add a penalty for large coefficients
    \item $\lambda = $ non-negative constant we choose: strength of penalty
    \item $X$ and $\mathbf{y}$ assumed to be \alert{de-meaned} (don't penalize intercept)
    \item Unlike OLS, Ridge Regression is \alert{not scale invariant}
      \begin{itemize}
        \item In OLS if we replace $\mathbf{x}_1$ with $c\mathbf{x}_1$ then $\beta_1$ becomes $\beta_1/c$.
        \item The same is not true for ridge regression!
        \item Typical to \alert{standardize} $X$ before carrying out ridge regression 
      \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Alternative Formulation of Ridge Regression Problem}
$$\widehat{\beta}_{Ridge} = \underset{\beta}{\arg \min}\;(\mathbf{y} - X\beta)' (\mathbf{y} - X\beta) \quad \mbox{subject to} \quad \beta'\beta \leq t$$


\begin{itemize}
  \item Ridge Regression is like least squares ``on a budget.'' 
  \item Make one coefficient larger $\Rightarrow$ must make another one smaller.
  \item One-to-one mapping from $t$ to $\lambda$ (data-dependend)
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Ridge as Bayesian Linear Regression}

If we ignore the intercept, which is unpenalized), Ridge Regression gives the \alert{posterior mode} from the Bayesian regression model:
	\begin{eqnarray*}
		y|X, \beta, \sigma^2 &\sim& N(X\beta,\sigma^2 I_n) \\
		\beta &\sim& N(\mathbf{0}, \tau^2 I_p)
	\end{eqnarray*}
where $\sigma^2$ is assumed known and $\lambda = \sigma^2/\tau^2$. 
(In this example, the posterior is normal so the mode equals the mean)


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Explicit Solution to the Ridge Regression Problem}

  \small
Objective Function:
\begin{eqnarray*}
	Q(\beta)&=& (\mathbf{y} - X\beta)' (\mathbf{y} - X\beta) + \lambda \beta'\beta\\
	&=&\mathbf{y}'\mathbf{y} - \beta'X \mathbf{y} - \mathbf{y}'X\beta + \beta'X'X \beta + \lambda \beta' I_p \beta\\
	&=& \mathbf{y}'\mathbf{y} - 2 \mathbf{y}'X\beta + \beta'(X'X + \lambda I_p)\beta
\end{eqnarray*}
Recall the following facts about matrix differentiation
		\[\partial (\mathbf{a}' \mathbf{x})/\partial \mathbf{x}  = \mathbf{a}, \quad
		\partial( \mathbf{x}'A \mathbf{x})/\partial \mathbf{x} = (A + A')\mathbf{x}
  \]
Thus, since $(X'X + \lambda I_p)$ is symmetric,
$$\frac{\partial}{\partial \beta} Q(\beta) = -2X'\mathbf{y} + 2(X'X + \lambda I_p)\beta$$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Explicit Solution to the Ridge Regression Problem}

  Previous Slide:
$$\frac{\partial}{\partial \beta} Q(\beta) = -2X'\mathbf{y} + 2(X'X + \lambda I_p)\beta$$

First order condition:
	$$X'\textbf{y} = (X'X + \lambda I_p)\beta$$
Hence,
	$$\widehat{\beta}_{Ridge} = (X'X + \lambda I_p)^{-1} X'\textbf{y}$$

  \vspace{1em}

  \alert{But is $(X'X + \lambda I_p)$ guaranteed to be invertible?} 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Ridge Regresion via OLS with ``Dummy Observations''}

  Ridge regression solution is identical to 
	$$\underset{\beta}{\arg \min} \left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)'\left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)$$
  where
$$\widetilde{\textbf{y}} = \left[ \begin{array}
	{c} \textbf{y} \\ \textbf{0}_p
\end{array}\right], \quad \quad \widetilde{X} = \left[ \begin{array}
	{c} X \\ \sqrt{\lambda} I_p
\end{array}\right]$$
since:
\begin{eqnarray*}
	\left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)'\left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right) &=& \left[\begin{array}
		{cc} (\mathbf{y} - X\beta)' & (-\sqrt{\lambda}\beta)'
	\end{array} \right] \left[\begin{array}
		{c} (\mathbf{y} - X\beta) \\ -\sqrt{\lambda} \beta
	\end{array} \right]\\
		&=& (\mathbf{y} - X\beta)' (\mathbf{y} - X\beta) + \lambda \beta'\beta\\
\end{eqnarray*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Ridge Regression Solution is Always Unique}

  Ridge solution is \alert{always unique}, even if there are more regressors than observations!
  This follows from the preceding slide:


$$ \widehat{\beta}_{Ridge} = \underset{\beta}{\arg \min} \left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)'\left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)$$
$$ \widetilde{\textbf{y}} = \left[ \begin{array}
	{c} \textbf{y} \\ \textbf{0}_p
\end{array}\right], \;  \widetilde{X} = \left[ \begin{array}
	{c} X \\ \sqrt{\lambda} I_p
\end{array}\right]$$

Columns of $\sqrt{\lambda}I_p$ are linearly independent, so columns of $\widetilde{X}$ are also linearly independent, \alert{regardless} of whether the same holds for the columns of $X$.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Efficient Calculations for Ridge Regression}
  

  \begin{block}{QR Decomposition}
    
  Write Ridge as OLS with ``dummy observations'' with $\widetilde{X} = QR$ so 
	$$\widehat{\beta}_{Ridge} = (\widetilde{X}' \widetilde{X})^{-1} \widetilde{X}'\,\widetilde{\textbf{y}} = R^{-1} Q' \,\widetilde{\textbf{y}}$$
which we can obtain by back-solving the system $R\widehat{\beta}_{Ridge} = Q'\, \widetilde{\mathbf{y}}$. 
  \end{block}

  \begin{block}{Singular Value Decomposition}
If $p \gg n$, it's much faster to use the SVD rather than the QR decomposition because the rank of $X$ will be $n$.
For implementation details, see Murphy (2012; Section 7.5.2).
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing Ridge and OLS}

  \begin{block}{Assumption}
   Centered data matrix $\underset{(n\times p)}{X}$ with rank $p$ so OLS estimator is unique.
  \end{block}

  \begin{block}{Economical SVD}
    \begin{itemize}
      \item 
    $\underset{(n\times p)}{X} = \underset{(n\times p)}{U}\underset{(p\times p)}{D}\underset{(p\times p)}{V'}$ with $U'U = V'V = I_p$, $D$ diagonal 
  \item Hence: $X'X = (UDV')'(UDV')=VDU'UDV' = VD^2V'$
  \item Since $V$ is square it is an orthogonal matrix: $VV' = I_p$
    \end{itemize}
  \end{block}

\end{frame}
\begin{frame}
  \frametitle{Comparing Ridge and OLS -- The ``Hat Matrix''}

    Using $X =UDV'$ and the fact that $V$ and $U$ are square orthogonal, 
    \begin{eqnarray*}
      H(\lambda) &=&  X\left( X'X + \lambda I_p \right)^{-1}X' = UDV'\left( VD^2V + \lambda VV' \right)^{-1}VDU'\\
      &=& UDV'\left( VD^2V' + \lambda VV' \right)^{-1}VDU'\\
      &=& UDV'\left[ V(D^2 + \lambda I_p)V' \right]^{-1}VDU'\\
      &=& UDV'\left( V' \right)^{-1}\left( D^2 + \lambda I_p \right)^{-1}\left( V \right)^{-1}VDU'\\
      &=& UDV'V\left( D^2 + \lambda I_p \right)^{-1}V'VDU'\\
      &=& UD\left( D^2 + \lambda I_p \right)^{-1}DU'
    \end{eqnarray*}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Model Complexity of Ridge Versus OLS}

  \begin{block}{OLS Case}
   Number of free parameters equals number of parameters $p$. 
  \end{block}

  \begin{block}{Ridge is more complicated}
    Even though there are $p$ parameters they are \alert{constrained}! 
  \end{block}

  \begin{block}{Idea: use trace of $H(\lambda)$}
    $\mbox{df}(\lambda) = \mbox{tr}\left\{ H(\lambda) \right\} = \mbox{tr}\left\{ X(X'X + \lambda I_p)^{-1}X' \right\}$
  \end{block}

  \begin{block}{Why? Works for OLS: $\lambda = 0$}
    $\mbox{df}(0) = \mbox{tr}\left\{ H(0) \right\} = \mbox{tr}\left\{ X(X'X)^{-1}X' \right\} = p$
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Effective Degrees of Freedom for Ridge Regression}
 
  Using cyclic permutation property of trace:
  \begin{eqnarray*}
    \mbox{df}(\lambda) &=&  \mbox{tr}\left\{ H(\lambda) \right\} = \mbox{tr}\left\{ X(X'X + \lambda I_p)^{-1}X' \right\}\\
 &=& \mbox{tr}\left\{UD\left( D^2 + \lambda I_p \right)^{-1}DU'\right\}\\
 &=& \mbox{tr}\left\{DU'UD\left( D^2 + \lambda I_p \right)^{-1}\right\}\\
 &=& \mbox{tr}\left\{D^2\left( D^2 + \lambda I_p \right)^{-1}\right\}\\
 &=& \sum_{j=1}^p\frac{d_j^2}{d_j^2 + \lambda}
  \end{eqnarray*}

  \begin{itemize}
    \item $\mbox{df}(\lambda) \rightarrow 0$ as $\lambda \rightarrow 0$
    \item $\mbox{df}(\lambda) = p$ when $\lambda = 0$
    \item $\mbox{df}(\lambda) < p$ when $\lambda > 0$
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing OLS and Ridge Predictions}

  \begin{eqnarray*}
  \widehat{y}(\lambda) &=&  X\widehat{\beta}(\lambda) = X\left( X'X + \lambda I_p \right)^{-1}X' \\
  &=& H(\lambda) =  \left[ UD\left( D^2 + \lambda I_p \right)^{-1}DU' \right]\mathbf{y} \\
  &=& \left[ \sum_{j=1}^p \mathbf{u}_j  \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j'\right]\mathbf{y} =  \sum_{j=1}^p   \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j\mathbf{u}_j'\mathbf{y}
  \end{eqnarray*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing OLS and Ridge Predictions}
  \small

  \begin{eqnarray*}
  \widehat{y}(\lambda) &=&
   \sum_{j=1}^p   \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j\mathbf{u}_j'\mathbf{y}
  \end{eqnarray*}

  \begin{itemize}
    \item Since $X$ is centered, $\mathbf{z}_j = d_j\mathbf{u}_j$ is the $j$th sample PC
    \item $d_j^2$ is proportional to the \alert{variance} of the $j$th sample PC
    \item Prediction from regression of $\mathbf{y}$ on $\mathbf{z}_j$ is: 
      \[ \mathbf{z}_j(\mathbf{z}_j'\mathbf{z}_j)^{-1}\mathbf{z}_j' \mathbf{y} = 
        d_j \mathbf{u}_j\left( d_j^2 \mathbf{u}_j' \mathbf{u}_j \right)^{-1} d_j \mathbf{u}_j'\mathbf{y} = \mathbf{u}_j\mathbf{u}_j'\mathbf{y}  
      \]
    \item Ridge equivalent to regressing $y$ on sample PCs of $X$ but shrinking predictions to zero: higher variance PCs are shrunk less.
    \item OLS doesn't shrink.
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Principal Components Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Principal Components Regression (PCR)}

  \small

Instead of ``smooth weights'' as in Ridge, truncate the PCs:
	\begin{enumerate}
    \item Calculate SVD $X=UDV'$ of \alert{centered} data matrix $X$
		\item Construct the sample principal components: $\mathbf{z}_i = d_j \mathbf{u}_j$.
		\item Throw away all but first $M$ principal components, where $M <p$.
		\item Regress \textbf{y} on $\mathbf{z_1}, \hdots, \mathbf{z}_k$. 
	\end{enumerate}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{PCR versus Ridge}
  \begin{itemize}
    \item PCR is a much less smooth version of Ridge
    \item Conventional wisdom is that PCR will perform worse since it shrinks low variance directions too much and doesn't shrink high variance directions at all.
  \item However, Dhillon et al.\ (2013) show that the MSE risk of PCR is always within a constant factor of that of Ridge Regression while there are situations in which Ridge can be arbitrarily worse than PCR in terms of MSE. 
    \item In practice, which is better depends on the DGP
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
