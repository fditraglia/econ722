\section{Ridge Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Ridge Regression -- OLS with an $L_2$ Penalty}


	$$\widehat{\beta}_{Ridge} =\underset{\beta}{\arg \min}\;(\mathbf{y} - X\beta)' (\mathbf{y} - X\beta) + \lambda \beta'\beta$$
  \begin{itemize}
    \item Add a penalty for large coefficients
    \item $\lambda = $ non-negative constant we choose: strength of penalty
    \item $X$ and $\mathbf{y}$ assumed to be \alert{de-meaned} (don't penalize intercept)
    \item Unlike OLS, Ridge Regression is \alert{not scale invariant}
      \begin{itemize}
        \item In OLS if we replace $\mathbf{x}_1$ with $c\mathbf{x}_1$ then $\beta_1$ becomes $\beta_1/c$.
        \item The same is not true for ridge regression!
        \item Typical to \alert{standardize} $X$ before carrying out ridge regression 
      \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Alternative Formulation of Ridge Regression Problem}
$$\widehat{\beta}_{Ridge} = \underset{\beta}{\arg \min}\;(\mathbf{y} - X\beta)' (\mathbf{y} - X\beta) \quad \mbox{subject to} \quad \beta'\beta \leq t$$


\begin{itemize}
  \item Ridge Regression is like least squares ``on a budget.'' 
  \item Make one coefficient larger $\Rightarrow$ must make another one smaller.
  \item One-to-one mapping from $t$ to $\lambda$ (data-dependent)
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Ridge as Bayesian Linear Regression}

If we ignore the intercept, which is unpenalized), Ridge Regression gives the \alert{posterior mode} from the Bayesian regression model:
	\begin{eqnarray*}
		y|X, \beta, \sigma^2 &\sim& N(X\beta,\sigma^2 I_n) \\
		\beta &\sim& N(\mathbf{0}, \tau^2 I_p)
	\end{eqnarray*}
where $\sigma^2$ is assumed known and $\lambda = \sigma^2/\tau^2$. 
(In this example, the posterior is normal so the mode equals the mean)


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Explicit Solution to the Ridge Regression Problem}

  \small
Objective Function:
\begin{eqnarray*}
	Q(\beta)&=& (\mathbf{y} - X\beta)' (\mathbf{y} - X\beta) + \lambda \beta'\beta\\
	&=&\mathbf{y}'\mathbf{y} - \beta'X \mathbf{y} - \mathbf{y}'X\beta + \beta'X'X \beta + \lambda \beta' I_p \beta\\
	&=& \mathbf{y}'\mathbf{y} - 2 \mathbf{y}'X\beta + \beta'(X'X + \lambda I_p)\beta
\end{eqnarray*}
Recall the following facts about matrix differentiation
		\[\partial (\mathbf{a}' \mathbf{x})/\partial \mathbf{x}  = \mathbf{a}, \quad
		\partial( \mathbf{x}'A \mathbf{x})/\partial \mathbf{x} = (A + A')\mathbf{x}
  \]
Thus, since $(X'X + \lambda I_p)$ is symmetric,
$$\frac{\partial}{\partial \beta} Q(\beta) = -2X'\mathbf{y} + 2(X'X + \lambda I_p)\beta$$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Explicit Solution to the Ridge Regression Problem}

  Previous Slide:
$$\frac{\partial}{\partial \beta} Q(\beta) = -2X'\mathbf{y} + 2(X'X + \lambda I_p)\beta$$

First order condition:
	$$X'\textbf{y} = (X'X + \lambda I_p)\beta$$
Hence,
	$$\widehat{\beta}_{Ridge} = (X'X + \lambda I_p)^{-1} X'\textbf{y}$$

  \vspace{1em}

  \alert{But is $(X'X + \lambda I_p)$ guaranteed to be invertible?} 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Ridge Regresion via OLS with ``Dummy Observations''}

  Ridge regression solution is identical to 
	$$\underset{\beta}{\arg \min} \left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)'\left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)$$
  where
$$\widetilde{\textbf{y}} = \left[ \begin{array}
	{c} \textbf{y} \\ \textbf{0}_p
\end{array}\right], \quad \quad \widetilde{X} = \left[ \begin{array}
	{c} X \\ \sqrt{\lambda} I_p
\end{array}\right]$$
since:
\begin{eqnarray*}
	\left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)'\left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right) &=& \left[\begin{array}
		{cc} (\mathbf{y} - X\beta)' & (-\sqrt{\lambda}\beta)'
	\end{array} \right] \left[\begin{array}
		{c} (\mathbf{y} - X\beta) \\ -\sqrt{\lambda} \beta
	\end{array} \right]\\
		&=& (\mathbf{y} - X\beta)' (\mathbf{y} - X\beta) + \lambda \beta'\beta\\
\end{eqnarray*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Ridge Regression Solution is Always Unique}

  Ridge solution is \alert{always unique}, even if there are more regressors than observations!
  This follows from the preceding slide:


$$ \widehat{\beta}_{Ridge} = \underset{\beta}{\arg \min} \left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)'\left(\widetilde{\mathbf{y}} - \widetilde{X}\beta\right)$$
$$ \widetilde{\textbf{y}} = \left[ \begin{array}
	{c} \textbf{y} \\ \textbf{0}_p
\end{array}\right], \;  \widetilde{X} = \left[ \begin{array}
	{c} X \\ \sqrt{\lambda} I_p
\end{array}\right]$$

Columns of $\sqrt{\lambda}I_p$ are linearly independent, so columns of $\widetilde{X}$ are also linearly independent, \alert{regardless} of whether the same holds for the columns of $X$.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Efficient Calculations for Ridge Regression}
  

  \begin{block}{QR Decomposition}
    
  Write Ridge as OLS with ``dummy observations'' with $\widetilde{X} = QR$ so 
	$$\widehat{\beta}_{Ridge} = (\widetilde{X}' \widetilde{X})^{-1} \widetilde{X}'\,\widetilde{\textbf{y}} = R^{-1} Q' \,\widetilde{\textbf{y}}$$
which we can obtain by back-solving the system $R\widehat{\beta}_{Ridge} = Q'\, \widetilde{\mathbf{y}}$. 
  \end{block}

  \begin{block}{Singular Value Decomposition}
If $p \gg n$, it's much faster to use the SVD rather than the QR decomposition because the rank of $X$ will be $n$.
For implementation details, see Murphy (2012; Section 7.5.2).
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing Ridge and OLS}

  \begin{block}{Assumption}
   Centered data matrix $\underset{(n\times p)}{X}$ with rank $p$ so OLS estimator is unique.
  \end{block}

  \begin{block}{Economical SVD}
    \begin{itemize}
      \item 
    $\underset{(n\times p)}{X} = \underset{(n\times p)}{U}\underset{(p\times p)}{D}\underset{(p\times p)}{V'}$ with $U'U = V'V = I_p$, $D$ diagonal 
  \item Hence: $X'X = (UDV')'(UDV')=VDU'UDV' = VD^2V'$
  \item Since $V$ is square it is an orthogonal matrix: $VV' = I_p$
    \end{itemize}
  \end{block}

\end{frame}
\begin{frame}
  \frametitle{Comparing Ridge and OLS -- The ``Hat Matrix''}

    Using $X =UDV'$ and the fact that $V$ is orthogonal, 
    \begin{eqnarray*}
      H(\lambda) &=&  X\left( X'X + \lambda I_p \right)^{-1}X' = UDV'\left( VD^2V + \lambda VV' \right)^{-1}VDU'\\
      &=& UDV'\left( VD^2V' + \lambda VV' \right)^{-1}VDU'\\
      &=& UDV'\left[ V(D^2 + \lambda I_p)V' \right]^{-1}VDU'\\
      &=& UDV'\left( V' \right)^{-1}\left( D^2 + \lambda I_p \right)^{-1}\left( V \right)^{-1}VDU'\\
      &=& UDV'V\left( D^2 + \lambda I_p \right)^{-1}V'VDU'\\
      &=& UD\left( D^2 + \lambda I_p \right)^{-1}DU'
    \end{eqnarray*}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Comparing OLS and Ridge}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Model Complexity of Ridge Versus OLS}

  \begin{block}{OLS Case}
   Number of free parameters equals number of parameters $p$. 
  \end{block}

  \begin{block}{Ridge is more complicated}
    Even though there are $p$ parameters they are \alert{constrained}! 
  \end{block}

  \begin{block}{Idea: use trace of $H(\lambda)$}
    $\mbox{df}(\lambda) = \mbox{tr}\left\{ H(\lambda) \right\} = \mbox{tr}\left\{ X(X'X + \lambda I_p)^{-1}X' \right\}$
  \end{block}

  \begin{block}{Why? Works for OLS: $\lambda = 0$}
    $\mbox{df}(0) = \mbox{tr}\left\{ H(0) \right\} = \mbox{tr}\left\{ X(X'X)^{-1}X' \right\} = p$
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Effective Degrees of Freedom for Ridge Regression}
 
  Using cyclic permutation property of trace:
  \begin{eqnarray*}
    \mbox{df}(\lambda) &=&  \mbox{tr}\left\{ H(\lambda) \right\} = \mbox{tr}\left\{ X(X'X + \lambda I_p)^{-1}X' \right\}\\
 &=& \mbox{tr}\left\{UD\left( D^2 + \lambda I_p \right)^{-1}DU'\right\}\\
 &=& \mbox{tr}\left\{DU'UD\left( D^2 + \lambda I_p \right)^{-1}\right\}\\
 &=& \mbox{tr}\left\{D^2\left( D^2 + \lambda I_p \right)^{-1}\right\}\\
 &=& \sum_{j=1}^p\frac{d_j^2}{d_j^2 + \lambda}
  \end{eqnarray*}

  \begin{itemize}
    \item $\mbox{df}(\lambda) \rightarrow 0$ as $\lambda \rightarrow \infty$
    \item $\mbox{df}(\lambda) = p$ when $\lambda = 0$
    \item $\mbox{df}(\lambda) < p$ when $\lambda > 0$
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing OLS and Ridge Predictions}

  \begin{eqnarray*}
    \widehat{y}(\lambda) &=&  X\widehat{\beta}(\lambda) = X\left( X'X + \lambda I_p \right)^{-1}X'\mathbf{y} \\
    &=& H(\lambda) \mathbf{y}=  \left[ UD\left( D^2 + \lambda I_p \right)^{-1}DU' \right]\mathbf{y} \\
  &=& \left[ \sum_{j=1}^p \mathbf{u}_j  \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j'\right]\mathbf{y} =  \sum_{j=1}^p   \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j\mathbf{u}_j'\mathbf{y}
  \end{eqnarray*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing OLS and Ridge Predictions}
  \small

  \begin{eqnarray*}
  \widehat{y}(\lambda) &=&
   \sum_{j=1}^p   \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j\mathbf{u}_j'\mathbf{y}
  \end{eqnarray*}

  \begin{itemize}
    \item Since $X$ is centered, $\mathbf{z}_j = d_j\mathbf{u}_j$ is the $j$th sample PC
    \item $d_j^2$ is proportional to the \alert{variance} of the $j$th sample PC
    \item Prediction from regression of $\mathbf{y}$ on $\mathbf{z}_j$ is: 
      \[ \mathbf{z}_j(\mathbf{z}_j'\mathbf{z}_j)^{-1}\mathbf{z}_j' \mathbf{y} = 
        d_j \mathbf{u}_j\left( d_j^2 \mathbf{u}_j' \mathbf{u}_j \right)^{-1} d_j \mathbf{u}_j'\mathbf{y} = \mathbf{u}_j\mathbf{u}_j'\mathbf{y}  
      \]
    \item Ridge equivalent to regressing $y$ on sample PCs of $X$ but shrinking predictions to zero: higher variance PCs are shrunk less.
    \item OLS doesn't shrink.
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing the MSE of OLS and Ridge}

  \begin{block}{Assumptions}
    $y = X\beta + \varepsilon$, Fixed $X$, iid data, homoskedasticity
  \end{block}

  \begin{block}{OLS Estimator: $\widehat{\beta}$} 
    $\widehat{\beta} = (X'X)^{-1}X'y \implies \text{Bias}(\widehat{\beta}) = 0 \quad \text{Var}(\widehat{\beta}) = \sigma (X'X)^{-1}$
  \end{block}

  \begin{block}{Ridge Estimator: $\widetilde{\beta}_\lambda$}
    $\widehat{\beta}_\lambda = (X'X + \lambda I)^{-1}X'y \implies \alert{\text{Bias}(\widetilde{\beta}_\lambda) = ? \quad\text{Var}(\widetilde{\beta}_\lambda) = ?}$ 
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Calculating The Bias of Ridge Regression}
  \framesubtitle{X fixed (or condition or X)}

  \begin{align*}
    \text{Bias}(\widetilde{\beta}_\lambda ) &= \mathbbm{E}\left[ (X'X + \lambda I)^{-1} X'(X\beta + \varepsilon) - \beta \right]\\
    &= (X'X + \lambda I)^{-1} X'X\beta  + (X'X + \lambda I)^{-1}\underbrace{\mathbbm{E}[X'\varepsilon]}_{0} - \beta\\
    &= (X'X + \lambda I)^{-1}\left[ (X'X + \lambda I)\beta - \lambda \beta \right] - \beta\\
    &= \beta - \lambda (X'X + \lambda I)^{-1} \beta - \beta\\
    &= - \lambda(X'X + \lambda I)^{-1} \beta
  \end{align*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Calculating The Variance of Ridge Regression}
  \framesubtitle{X fixed (or condition or X)}

  \begin{align*}
    \text{Var}(\widetilde{\beta}_\lambda ) &= \text{Var}\left[ (X'X + \lambda I)^{-1} X'(X\beta + \varepsilon) - \beta \right]\\
    &= \text{Var}\left[ (X'X + \lambda I)^{-1}X' \varepsilon \right]\\
    &= \mathbbm{E}\left[ \left\{(X'X + \lambda I)^{-1}X' \varepsilon \right\}\left\{ (X'X + \lambda I)^{-1}X' \varepsilon \right\}'\right]\\
    &= \left[(X'X + \lambda I)^{-1} X'\right] \underbrace{\mathbbm{E}[\varepsilon \varepsilon']}_{\sigma^2 I} \left[(X'X + \lambda I)^{-1} X'\right]'\\
    &= \sigma^2 (X'X + \lambda I)^{-1} X'X \left( X'X + \lambda I \right)^{-1}
  \end{align*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{For $\lambda$ Sufficiently Small, MSE(OLS)$>$MSE(Ridge)}
  \scriptsize
\begin{align*}
  \text{MSE}(\widehat{\beta}) - \text{MSE}(\widetilde{\beta}_{\lambda}) &= \left\{ \text{Bias}^2(\widehat{\beta}) + \text{Var}(\widehat{\beta}) \right\} - \left\{ \text{Bias}^2(\widetilde{\beta}_\lambda) + \text{Var}(\widetilde{\beta}_\lambda) \right\}\\
  &\vdots \\
  &= \lambda \underbrace{(X'X + \lambda I)^{-1}}_{M'}\underbrace{\left[\sigma^2 \left\{ 2I + \lambda(X'X)^{-1} \right\} - \lambda \beta \beta'  \right]}_{A}\underbrace{\left( X'X + \lambda I \right)^{-1}}_{M}
\end{align*}
\normalsize

\begin{itemize}
  \item $\lambda > 0$ and $M$ is symmetric
  \item $M$ is full rank $\implies Mv \neq 0$ unless $v = 0$
  \item $\implies v'[\lambda M'AM] v = \lambda (Mv)' A (MV)$
  \item MSE(OLS) - MSE(Ridge) is PD iff $M$ is PD
  \item To ensure $M$ is PD, make $\lambda$ small, e.g.\ $0 < \lambda < 2\sigma^2/\beta'\beta$
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
