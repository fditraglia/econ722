\section{Review of Principal Component Analysis (PCA)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Principal Component Analysis (PCA)}
  
  \begin{block}{Notation}
  Let $\mathbf{x}$ be a $p\times 1$ random vector with variance-covariance matrix $\Sigma$.
  \end{block}

  \begin{block}{Optimization Problem}
    \vspace{-1em}
    \[\boldsymbol{\alpha}_1 = \underset{\boldsymbol{\alpha}}{\arg \max} \; \;\mbox{Var}(\boldsymbol{\alpha}' \mathbf{x}) \quad \text{ subject to } \quad \boldsymbol{\alpha}'\boldsymbol{\alpha} = 1\]
  \end{block}

  \begin{block}{First Principal Component}
    The linear combination $\boldsymbol{\alpha}_1' \mathbf{x}$ is the \alert{first principal component} of $\mathbf{x}$.
    It is the direction along with $\mathbf{x}$ has \alert{maximal variation}
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Solving for $\boldsymbol{\alpha}_1$}

  \begin{block}{Lagrangian}
  $\mathcal{L}(\boldsymbol{\alpha}_1, \lambda) = \boldsymbol{\alpha}' \Sigma \boldsymbol{\alpha} - \lambda(\boldsymbol{\alpha}' \boldsymbol{\alpha} - 1)$
  \end{block}

  \begin{block}{First Order Condition}
    $2 (\Sigma\boldsymbol{\alpha}_1 - \lambda \boldsymbol{\alpha}_1) = 0 \iff (\Sigma - \lambda I_p)\boldsymbol{\alpha}_1 = 0 \iff \Sigma \boldsymbol{\alpha}_1 = \lambda \boldsymbol{\alpha}_1$
  \end{block}

  \begin{block}{Variance of 1st PC}
    $\boldsymbol{\alpha}_1$ is an e-vector of $\Sigma$ but which one? 
    Substituting, 
    \[
      \mbox{Var}(\boldsymbol{\alpha}_1' \mathbf{x}) = \boldsymbol{\alpha}_1'(\Sigma \boldsymbol{\alpha}_1) = \lambda \boldsymbol{\alpha}_1' \boldsymbol{\alpha}_1 = \lambda
    \]
  \end{block}
  \vspace{-1em}

  \begin{alertblock}{Solution}
    Var.\ of 1st PC equals $\lambda$ and this is what we want to \alert{maximize}, so $\boldsymbol{\alpha}_1$ is the e-vector corresponding to the largest e-value.
  \end{alertblock}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Subsequent Principal Components}

  \begin{block}{Additional Constraint}
    Construct 2nd PC by solving the same problem as before with the additional constraint that $\boldsymbol{\alpha}_2'\mathbf{x}$ is uncorrelated with $\boldsymbol{\alpha}_1' \mathbf{x}$.
  \end{block}

  \begin{alertblock}{$j$th Principal Component}
    The linear combination $\boldsymbol{\alpha}_j' \mathbf{x}$ where $\boldsymbol{\alpha}_j$ is the e-vector corresponding to the $j$th largest e-value of $\Sigma$.  
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Sample PCA}

  \begin{block}{Notation}
    $X = (n\times p)$ \alert{centered} data matrix -- columns are mean zero.
  \end{block}

  \begin{block}{SVD}
   $X = UDV'$, thus $X'X = VDU'UDV' = VD^2V'$
  \end{block}

  \begin{block}{Sample Variance Matrix}
    $S = n^{-1}X'X$ has same e-vectors as $X'X$ --  the columns of $V$! 
  \end{block}

  \begin{block}{Sample PCA}
    Let $\mathbf{v}_j$ be the jth column of $V$. Then,
    \begin{eqnarray*}
      \mathbf{v}_j &=& \text{PC loadings for jth PC of } S\\
      \mathbf{v}_j' \mathbf{x}_i &=& \text{PC score for individual/time period } i \\
    \end{eqnarray*}
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Sample PCA}
  \small

  \begin{block}{PC scores for $j$th PC}
    \vspace{-0.5em}
    \[
      \mathbf{z}_j = 
      \left[
        \begin{array}{c}
          z_{j1}\\ \vdots \\ z_{jn}
      \end{array}
    \right] =
      \left[
        \begin{array}{c}
          \mathbf{v}_j'\mathbf{x}_1\\
          \vdots\\
          \mathbf{v}_j'\mathbf{x}_n
      \end{array}
    \right] = 
      \left[
        \begin{array}{c}
          \mathbf{x}_1'\mathbf{v}_j\\
          \vdots\\
          \mathbf{x}_n'\mathbf{v}_j
      \end{array}
    \right] = 
      \left[
        \begin{array}{c}
          \mathbf{x}_1'\\
          \vdots\\
          \mathbf{x}_n'
      \end{array}
    \right]\mathbf{v}_j = X\mathbf{v}_j
    \]
  \end{block}

  \begin{block}{Getting PC Scores from SVD}
   Since $X = UDV'$ and $V'V = I$, $XV = UD$, i.e.\

   \[
     \left[
     \begin{array}{c}
       \mathbf{x}_1'\\
       \vdots\\
       \mathbf{x}_n'
     \end{array}
   \right]
   \left[
   \begin{array}{ccc}
     \mathbf{v}_i & \cdots & \mathbf{v}_p
   \end{array}
 \right] = 
 \left[
 \begin{array}{ccc}
   \mathbf{u}_1 & \cdots & \mathbf{u}_r
 \end{array}
 \right]
 \left[
 \begin{array}{ccc}
   d_1 & \cdots & 0\\
   & \ddots  & \\
   0 & \cdots & d_r
 \end{array}
 \right]
   \]

   \vspace{0.5em}

   \alert{Hence we see that $\mathbf{z}_j = d_j \mathbf{u}_j$}

  \end{block}

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Properties of PC Scores $\mathbf{z}_j$}

  Since $X$ has been de-meaned:
  \[
    \bar{z}_j = \frac{1}{n}\sum_{i=1}^n \mathbf{v}_j'\mathbf{x}_i = \mathbf{v}_j' \left( \frac{1}{n}\sum_{i=1}^n \mathbf{x}_i \right) = \mathbf{v}_j' \mathbf{0} = 0
  \]

  Hence, since $X'X = VD^2V'$
  \[
    \frac{1}{n}\sum_{i=1}^n (z_{ji} - \bar{z}_j)^2 = \frac{1}{n} \sum_{i=1}^n z_{ji}^2 = \frac{1}{n} \mathbf{z}_j'\mathbf{z}_j = \frac{1}{n}\left( X\mathbf{v}_j \right)'\left( X\mathbf{v}_j \right) = \mathbf{v}_j' S\mathbf{v}_j = d_j^2/n
  \]

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
