\section{Principal Component Analysis (PCA)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Principal Component Analysis (PCA)}
  
  \begin{block}{Notation}
  Let $\mathbf{x}$ be a $p\times 1$ random vector with variance-covariance matrix $\Sigma$.
  \end{block}

  \begin{block}{Optimization Problem}
    \vspace{-1em}
    \[\boldsymbol{\alpha}_1 = \underset{\boldsymbol{\alpha}}{\arg \max} \; \;\mbox{Var}(\boldsymbol{\alpha}' \mathbf{x}) \quad \text{ subject to } \quad \boldsymbol{\alpha}'\boldsymbol{\alpha} = 1\]
  \end{block}

  \begin{block}{First Principal Component}
    The linear combination $\boldsymbol{\alpha}_1' \mathbf{x}$ is the \alert{first principal component} of $\mathbf{x}$.
    The random vector $\mathbf{x}$ has \alert{maximal variation} in the direction $\mathbf{\alpha}_1$.
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Solving for $\boldsymbol{\alpha}_1$}

  \begin{block}{Lagrangian}
  $\mathcal{L}(\boldsymbol{\alpha}_1, \lambda) = \boldsymbol{\alpha}' \Sigma \boldsymbol{\alpha} - \lambda(\boldsymbol{\alpha}' \boldsymbol{\alpha} - 1)$
  \end{block}

  \begin{block}{First Order Condition}
    $2 (\Sigma\boldsymbol{\alpha}_1 - \lambda \boldsymbol{\alpha}_1) = 0 \iff (\Sigma - \lambda I_p)\boldsymbol{\alpha}_1 = 0 \iff \Sigma \boldsymbol{\alpha}_1 = \lambda \boldsymbol{\alpha}_1$
  \end{block}

  \begin{block}{Variance of 1st PC}
    $\boldsymbol{\alpha}_1$ is an e-vector of $\Sigma$ but which one? 
    Substituting, 
    \[
      \mbox{Var}(\boldsymbol{\alpha}_1' \mathbf{x}) = \boldsymbol{\alpha}_1'(\Sigma \boldsymbol{\alpha}_1) = \lambda \boldsymbol{\alpha}_1' \boldsymbol{\alpha}_1 = \lambda
    \]
  \end{block}
  \vspace{-1em}

  \begin{alertblock}{Solution}
    Var.\ of 1st PC equals $\lambda$ and this is what we want to \alert{maximize}, so $\boldsymbol{\alpha}_1$ is the e-vector corresponding to the largest e-value.
  \end{alertblock}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Subsequent Principal Components}

  \begin{block}{Additional Constraint}
    Construct 2nd PC by solving the same problem as before with the additional constraint that $\boldsymbol{\alpha}_2'\mathbf{x}$ is uncorrelated with $\boldsymbol{\alpha}_1' \mathbf{x}$.
  \end{block}

  \begin{alertblock}{$j$th Principal Component}
    The linear combination $\boldsymbol{\alpha}_j' \mathbf{x}$ where $\boldsymbol{\alpha}_j$ is the e-vector corresponding to the $j$th largest e-value of $\Sigma$.  
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Sample PCA}

  \begin{block}{Notation}
    $X = (n\times p)$ \alert{centered} data matrix -- columns are mean zero.
  \end{block}

  \begin{block}{SVD}
   $X = UDV'$, thus $X'X = VDU'UDV' = VD^2V'$
  \end{block}

  \begin{block}{Sample Variance Matrix}
    $S = n^{-1}X'X$ has same e-vectors as $X'X$ --  the columns of $V$! 
  \end{block}

  \begin{block}{Sample PCA}
    Let $\mathbf{v}_j$ be the jth column of $V$. Then,
    \begin{eqnarray*}
      \mathbf{v}_j &=& \text{PC loadings for jth PC of } S\\
      \mathbf{v}_j' \mathbf{x}_i &=& \text{PC score for individual/time period } i \\
    \end{eqnarray*}
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Sample PCA}
  \small

  \begin{block}{PC scores for $j$th PC}
    \vspace{-0.5em}
    \[
      \mathbf{z}_j = 
      \left[
        \begin{array}{c}
          z_{j1}\\ \vdots \\ z_{jn}
      \end{array}
    \right] =
      \left[
        \begin{array}{c}
          \mathbf{v}_j'\mathbf{x}_1\\
          \vdots\\
          \mathbf{v}_j'\mathbf{x}_n
      \end{array}
    \right] = 
      \left[
        \begin{array}{c}
          \mathbf{x}_1'\mathbf{v}_j\\
          \vdots\\
          \mathbf{x}_n'\mathbf{v}_j
      \end{array}
    \right] = 
      \left[
        \begin{array}{c}
          \mathbf{x}_1'\\
          \vdots\\
          \mathbf{x}_n'
      \end{array}
    \right]\mathbf{v}_j = X\mathbf{v}_j
    \]
  \end{block}

  \begin{block}{Getting PC Scores from SVD}
   Since $X = UDV'$ and $V'V = I$, $XV = UD$, i.e.\

   \[
     \left[
     \begin{array}{c}
       \mathbf{x}_1'\\
       \vdots\\
       \mathbf{x}_n'
     \end{array}
   \right]
   \left[
   \begin{array}{ccc}
     \mathbf{v}_i & \cdots & \mathbf{v}_p
   \end{array}
 \right] = 
 \left[
 \begin{array}{ccc}
   \mathbf{u}_1 & \cdots & \mathbf{u}_r
 \end{array}
 \right]
 \left[
 \begin{array}{ccc}
   d_1 & \cdots & 0\\
   & \ddots  & \\
   0 & \cdots & d_r
 \end{array}
 \right]
   \]

   \vspace{0.5em}

   \alert{Hence we see that $\mathbf{z}_j = d_j \mathbf{u}_j$}

  \end{block}

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Properties of PC Scores $\mathbf{z}_j$}

  Since $X$ has been de-meaned:
  \[
    \bar{z}_j = \frac{1}{n}\sum_{i=1}^n \mathbf{v}_j'\mathbf{x}_i = \mathbf{v}_j' \left( \frac{1}{n}\sum_{i=1}^n \mathbf{x}_i \right) = \mathbf{v}_j' \mathbf{0} = 0
  \]

  Hence, since $X'X = VD^2V'$
  \[
    \frac{1}{n}\sum_{i=1}^n (z_{ji} - \bar{z}_j)^2 = \frac{1}{n} \sum_{i=1}^n z_{ji}^2 = \frac{1}{n} \mathbf{z}_j'\mathbf{z}_j = \frac{1}{n}\left( X\mathbf{v}_j \right)'\left( X\mathbf{v}_j \right) = \mathbf{v}_j' S\mathbf{v}_j = d_j^2/n
  \]

  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Principal Components Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Principal Components Regression (PCR)}

	\begin{enumerate}
    \item Start with centered $X$ and $\mathbf{y}$.
    \item SVD of $X \implies$ PC scores: $\mathbf{z}_j = X \mathbf{v}_j =  d_j \mathbf{u}_j$.
    \item Regress \textbf{y} on $[\begin{array}{ccc} \mathbf{z}_1 & \hdots & \mathbf{z}_m\end{array}]$ where $m < p$.
      \vspace{-0.5em}
      \[
        \widehat{\mathbf{y}}_{\text{PCR}}(m) =\sum_{j = 1}^m \mathbf{z}_j\widehat{\theta}_j, \quad 
        \widehat{\theta}_j = \frac{\mathbf{z}_j' \mathbf{y}}{\mathbf{z}_j' \mathbf{z}_j} \quad \text{(PCs orthogonal)} 
      \]
	\end{enumerate}

\begin{alertblock}{Standardizing $X$}
  Because PCR is not scale invariant, it is common to standardize $X$. This amounts to PCA performed on a \alert{correlation} matrix.
\end{alertblock}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Comparing OLS, Ridge, and PCR}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing PCR, OLS and Ridge Predictions}

  \begin{block}{Assumption}
    Centered data matrix $\underset{(n\times p)}{X}$ with rank $p$ so OLS estimator is unique.
  \end{block}

  \begin{block}{SVD}
    \vspace{-1em}
    \[
      \underset{(n\times p)}{X} = \underset{(n\times p)}{U} \underset{(p\times p)}{D} \underset{(p \times p)}{V'}, \quad U'U = V'V = I_p, \quad VV' = I_p
    \]
  \end{block}

\begin{block}{Ridge Predictions}
  \vspace{-1em}
  \begin{eqnarray*}
    \widehat{\mathbf{y}}_{\text{Ridge}}(\lambda) &=&  X\widehat{\beta}_{\text{Ridge}}(\lambda) = X\left( X'X + \lambda I_p \right)^{-1}X'\mathbf{y} \\
    &=& \left[ UD\left( D^2 + \lambda I_p \right)^{-1}DU' \right]\mathbf{y} \\
  &=& 
  %\left[ \sum_{j=1}^p \mathbf{u}_j  \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j'\right]\mathbf{y} =  
  \sum_{j=1}^p   \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j\mathbf{u}_j'\mathbf{y}
  \end{eqnarray*}
\end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Comparing OLS, Ridge, and PCR Predictions}
%
%\begin{block}{OLS Predictions}
%  \vspace{-1em}
%  \[
%    \widehat{\mathbf{y}}_{\text{OLS}} = \widehat{\mathbf{y}}_{\text{Ridge}}(0) = \sum_{j=1}^p   \mathbf{u}_j\mathbf{u}_j'\mathbf{y} 
%  \]
%\end{block}
%
%\begin{block}{PCR Predictions}
%  \begin{align*}
%    \widehat{\mathbf{y}}_{\text{PCR}}(m) &= \sum_{j = 1}^m \mathbf{z}_j\widehat{\theta}_j = \sum_{j=1}^{m} \mathbf{z}_j \left[\frac{\mathbf{z}_j' \mathbf{y}}{\mathbf{z}_j' \mathbf{z}_j}\right] = \sum_{j=1}^m d_j \mathbf{u}_j \left[ \frac{(d_j \mathbf{u}_j)' \mathbf{y}}{(d_j \mathbf{u}_j)'(d_j \mathbf{u}_j)} \right]\\
%    &= \sum_{j=1}^m \mathbf{u}_j \left[ \frac{\mathbf{u}_j' \mathbf{y}}{\mathbf{u}_j'\mathbf{u}_j} \right] = \sum_{j=1}^m \mathbf{u}_j \mathbf{u}_j' \mathbf{y}
%      \end{align*}
%since $U$ has orthonormal columns.
%\end{block}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Comparing OLS, Ridge, and PCR Predictions}
%
%  Since $U$ has orthonormal columns, $\widehat{\gamma}_j \equiv \mathbf{u}_j' \mathbf{y} = \mathbf{u}_j' \mathbf{y}/\mathbf{u}_j' \mathbf{u}_j$ is the $j$th coefficient from a regression of $\mathbf{y}$ on $(\mathbf{u}_1, \dots, \mathbf{u}_p)$.
%
%  \begin{align*}
%    \widehat{\mathbf{y}}_{\text{Ridge}}(\lambda) &= \sum_{j=1}^p   \mathbf{u}_j\left( \frac{d_j^2}{d_j^2 + \lambda} \right)\widehat{\gamma}_j\\
%    \widehat{\mathbf{y}}_{\text{OLS}} &= \sum_{j=1}^p   \mathbf{u}_j \widehat{\gamma}_j\\ 
%    \widehat{\mathbf{y}}_{\text{PCR}}(m) &= \sum_{j=1}^m \mathbf{u}_j \widehat{\gamma}_j 
%  \end{align*}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Relating OLS and Ridge to PCR}

  \begin{block}{Recall: $U$ is Orthonormal}
    \vspace{-1em}
      \[
        \mathbf{u}_j\mathbf{u}_j'\mathbf{y}  =
        d_j \mathbf{u}_j\left( d_j^2 \mathbf{u}_j' \mathbf{u}_j \right)^{-1} d_j \mathbf{u}_j'\mathbf{y} =
        \mathbf{z}_j(\mathbf{z}_j'\mathbf{z}_j)^{-1}\mathbf{z}_j' \mathbf{y} = \mathbf{z}_j \widehat{\theta}_j
      \]
\end{block}

\begin{alertblock}{Substituting}
  \vspace{-1em}
  \begin{align*}
        \widehat{\mathbf{y}}_{\text{Ridge}}(\lambda) &= \sum_{j=1}^m \left( \frac{d_j^2}{d_j^2 + \lambda} \right) \mathbf{u}_j \mathbf{u}_j' \mathbf{y} = \sum_{j=1}^m \left( \frac{d_j^2}{d_j^2 + \lambda} \right) \mathbf{z}_j \widehat{\theta}_j \\
        %\widehat{\mathbf{y}}_{\text{PCR}}(m) &=\sum_{j = 1}^m \mathbf{z}_j\widehat{\theta}_j \\
        \widehat{\mathbf{y}}_{\text{OLS}} &= \widehat{\mathbf{y}}_{\text{Ridge}}(0) = \sum_{j=1}^p \mathbf{z}_j \widehat{\theta}_j 
      \end{align*}
    \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Comparing PCR, OLS, and Ridge Predictions}
  
  \small

  \[
        \widehat{\mathbf{y}}_{\text{PCR}}(m) =\sum_{j = 1}^m \mathbf{z}_j\widehat{\theta}_j, \quad
    \widehat{\mathbf{y}}_{\text{OLS}} = \sum_{j=1}^p \mathbf{z}_j \widehat{\theta}_j, \quad
        \widehat{\mathbf{y}}_{\text{Ridge}}(\lambda) = \sum_{j=1}^m \left( \frac{d_j^2}{d_j^2 + \lambda} \right) \mathbf{z}_j \widehat{\theta}_j
  \]

  \begin{itemize}
    \item $\mathbf{z}_j$ is the $j$th sample PC
    \item $d_j^2/n$ is the variance of the $j$th sample PC
    \item Ridge regresses $y$ on sample PCs but \alert{shrinks} predictions towards zero: higher variance PCs are shrunk \alert{less}.
    \item PCR \alert{truncates} the PCs with the smallest variance. 
    \item OLS neither shrinks nor truncates: is uses all the PCs. 
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Comparing OLS and Ridge Predictions}
%  \small
%
%  \begin{eqnarray*}
%  \widehat{y}(\lambda) &=&
%   \sum_{j=1}^p   \left( \frac{d_j^2}{d_j^2 + \lambda} \right)\mathbf{u}_j\mathbf{u}_j'\mathbf{y}
%  \end{eqnarray*}
%
%  \begin{itemize}
%    \item Since $X$ is centered, $\mathbf{z}_j = d_j\mathbf{u}_j$ is the $j$th sample PC
%    \item $d_j^2$ is proportional to the \alert{variance} of the $j$th sample PC
%    \item Prediction from regression of $\mathbf{y}$ on $\mathbf{z}_j$ is: 
%      \[ \mathbf{z}_j(\mathbf{z}_j'\mathbf{z}_j)^{-1}\mathbf{z}_j' \mathbf{y} = 
%        d_j \mathbf{u}_j\left( d_j^2 \mathbf{u}_j' \mathbf{u}_j \right)^{-1} d_j \mathbf{u}_j'\mathbf{y} = \mathbf{u}_j\mathbf{u}_j'\mathbf{y}  
%      \]
%    \item Ridge equivalent to regressing $y$ on sample PCs of $X$ but shrinking predictions to zero: higher variance PCs are shrunk less.
%    \item OLS doesn't shrink.
%  \end{itemize}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{PCR versus Ridge}
%  \begin{itemize}
%    \item PCR is a much less smooth version of Ridge
%    \item Conventional wisdom is that PCR will perform worse since it shrinks low variance directions too much and doesn't shrink high variance directions at all.
%  \item However, Dhillon et al.\ (2013) show that the MSE risk of PCR is always within a constant factor of that of Ridge Regression while there are situations in which Ridge can be arbitrarily worse than PCR in terms of MSE. 
%    \item In practice, which is better depends on the DGP
%  \end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
