%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimal Inference After Model Selection (Fithian et al., 2017)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{How Statistics is Done In Reality}

  \begin{block}{Step 1: Selection -- Decide what questions to ask.}
    ``The analyst chooses a statistical model for the data at hand, and formulates testing, estimation, or other problems in terms of unknown aspects of that model.''
  \end{block}

  \begin{block}{Step 2: Inference -- Answer the Questions.}
   ``The analyst investigates the chosen problems using the data and the selected model.''
  \end{block}

  \begin{alertblock}{Problem -- ``Data-snooping''}
    Standard techniques for (frequentist) statistical inference assume that we choose our questions \alert{before} observing the data.  
  \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simple Example: ``File Drawer Problem''}

  \begin{block}{$Y_i \sim \mbox{iid N}(\mu_i, 1)$ for $i=1, \dots, n$}
  \begin{itemize}
    \item I want to know which $\mu_i \neq 0$, but I'm busy and $n$ is big.
    \item My RA looks at each $Y_i$ and finds the ``interesting'' ones, namely $\widehat{\mathcal{I}} =\left\{ i\colon |Y_i|>1 \right\}$.
    \item I test $H_{0,i}\colon \mu_i = 0$ against the two-sided alternative at the 5\% significance level for each $i \in \widehat{\mathcal{I}}$.
  \end{itemize}
\end{block}

  \begin{alertblock}{Two Questions}
    \begin{enumerate}
      \item What is the probability of falsely rejecting $H_{0,i}$?
      \item Among all $H_{0,i}$ that I test, what fraction are false rejections? 
    \end{enumerate}
  \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simple Example: ``File Drawer Problem''}

  \small

  \begin{align*}
    \mathbb{P}_{H_{0,i}}(\{\mbox{Reject } H_{0,i}\}) &= \mathbb{P}_{H_{0,i}}(\{\mbox{Test } H_{0,i} \}\cap \{ \mbox{Reject } H_{0,i} \})\\
    &= \mathbb{P}_{H_{0,i}}(\{\mbox{Reject } H_{0,i}\} |\{\mbox{Test } H_{0,i}\})\mathbb{P}_{H_{0,i}}(\{\mbox{Test } H_{0,i}\})\\
    &= \mathbb{P}_{H_{0,i}}\left(\{|Y_i| > 1.96\}|\{|Y_i|>1\} \right) \mathbb{P}_{H_{0,i}}(\{|Y_i|>1\})\\
    &= \frac{2 \Phi(-1.96)}{2 \Phi(-1)} \times 2 \Phi(-1)\\
    &\approx 0.16 \times 0.32 \approx 0.05
  \end{align*}

  \vspace{-1em}

  \begin{align*}
  \mathbb{P}_{H_{0,i}}(\{\mbox{Reject } H_{0,i}\} |\{\mbox{Test } H_{0,i}\}) &= \mathbb{P}_{H_{0,i}}\left(\{|Y_i| > 1.96\}|\{|Y_i|>1\} \right)\\
  &= \frac{\Phi(-1.96)}{\Phi(-1)} \approx 0.16
  \end{align*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simple Example: ``File Drawer Problem''}

  \begin{block}{Conditional vs.\ Unconditional Type I Error Rates}
  \begin{itemize}
    \item The \alert{conditional} probability of falsely rejecting $H_{0,i}$, given that I have tested it, is about 0.16.
    \item The \alert{unconditional} probability of falsely rejecting $H_{0,i}$ is 0.05 since I only test a false null with probability $0.32$.
  \end{itemize}
\end{block}

\begin{block}{Idea for Post-Selection Inference}
  Control the Type I Error Rate \alert{conditional on selection}: ``The answer must be valid, given that the question was asked.''
\end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Simple Example: ``File Drawer Problem''}


  \begin{block}{Conditional Type I Error Rate}
  Solve $\mathbb{P}_{H_{0,i}}\left( \{|Y_i| > c\} | \{|Y_i|>1\}\right) = 0.05$ for $c$.
\end{block}

  \begin{align*}
    \mathbb{P}_{H_{0,i}}\left( \{|Y_i| > c\} | \{|Y_i|>1\}\right) &= \frac{\Phi(-c)}{\Phi(-1)} = 0.05\\
    c &= -\Phi^{-1}\big(\Phi(-1) \times 0.05\big)\\
    c &\approx 2.41 
  \end{align*}

  \vspace{-2em}

  \begin{alertblock}{Notice:}
  To account for the first-stage selection step, we need a larger critical value: 2.41 vs.\ 1.96. This means the test is less powerful.
\end{alertblock}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Selective Inference vs.\ Sample-Splitting}

  \begin{block}{Classical Inference}
    Control the Type I error under model $M$: $\mathbb{P}_{M,H_0}(\mbox{reject } H_0) \leq \alpha$.
  \end{block}

  \begin{block}{Selective Inference}
    Control the Type I error under model $M$, \alert{given} that $M$ and $H_0$ were selected: $\mathbb{P}_{M,H_0}\big(\mbox{reject } H_0|\{M,H_0 \mbox{ selected}\}\big) \leq \alpha$.
  \end{block}

  \begin{block}{Sample-Splitting}
    Use different datasets to choose $(M, H_0)$ and carry out inference: $\mathbb{P}_{M,H_0}\big(\mbox{reject } H_0|\{M,H_0 \mbox{ selected}\}\big) = \mathbb{P}_{M,H_0}(\mbox{reject }H_0)$.
  \end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Selective Inference in Exponential Family Models}

  \footnotesize
  
  \begin{block}{Questions}
  \begin{enumerate}
    \item Recipe for selective inference in realistic examples?
    \item How to construct the ``best'' selective test in a given example?
    \item How does selective inference compare to sample-splitting?
  \end{enumerate}
\end{block}
 
\begin{block}{Fithian, Sun \& Taylor (2017)}
  \begin{itemize}
    \item Use classical theory for exponential family models (Lehmann \& Scheff\'e).
    \item Computational procedure for UMPU selective test/CI after arbitrary model/hypothesis selection.
    \item Sample-splitting is typically inadmissible (wastes information).
    \item Example: post-selection inference for high-dimensional regression
  \end{itemize}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{A Prototype Example of Selective Inference}
  \framesubtitle{This is my own example, but uses the same idea that underlies Fithian et al.}

  \footnotesize
 
  \begin{itemize}
    \item Choose between two models on a parameter $\delta$. 
      \begin{itemize} 
  \footnotesize
        \item If $\delta \neq 0$, choose M1; if $\delta = 0$, choose M2
        \item E.g.\ $\delta$ is the endogeneity of $X$, M1 is IV and M2 is OLS
  \end{itemize}
\item Observe $Y_\delta \sim N(\delta, \sigma_\delta^2)$ and use this to choose a model.
  \begin{itemize}
  \footnotesize
    \item Selection Event: $A \equiv \left\{ |Y_\delta| >c \right\}$, for some critical value $c$ 
    \item If $A$, then choose M1. Otherwise, choose M2.
  \end{itemize}
\item After choosing a model, carry out inference for $\beta$.
  \begin{itemize}
  \footnotesize
    \item Under a particular model $M$, $Y_\beta \sim N(\beta,\sigma_\beta^2)$
    \item $\beta$ is a \emph{model-specific} parameter: could be meaningless or not even exist under a different model.
  \end{itemize}
\item If $Y_\beta$ and $Y_\delta$ are correlated (under model M), we need to account for conditioning on $A$ when carrying out inference for $\beta$.

  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{All Calculations are Under a Given Model $M$}
  \begin{block}{Key Idea}
Under whichever model $M$ ends up being selected, there is a joint normal distribution for $Y_\beta$ and $Y_\delta$ \emph{without} conditioning on $A$.
\end{block}

  \begin{block}{WLOG unit variances, $\rho$ known}
\[
  \left[
  \begin{array}{c}
    Y_\beta \\ Y_{\delta}
  \end{array}
\right] \sim \mbox{N}\left( \left[
\begin{array}{c}
  \beta \\ \delta
\end{array}
\right], \left[
\begin{array}{cc}
  1 & \rho \\
  \rho & 1\\
\end{array}
\right]\right)
\]

As long as we can consistently estimate the variances of $Y_{\beta}$ and $Y_{\delta}$ along with their covariance, this is not a problem.
  \end{block}



\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Selective Inference in a Bivariate Normal Example}
  
  \vspace{-1em}

  \small
\[
  \alert{\boxed{\left[
  \begin{array}{c}
    Y_\beta \\ Y_{\delta}
  \end{array}
\right] \sim \mbox{N}\left( \left[
\begin{array}{c}
  \beta \\ \delta
\end{array}
\right], \left[
\begin{array}{cc}
  1 & \rho \\
  \rho & 1\\
\end{array}
\right]\right), \quad A \equiv \left\{ |Y_\delta| >c \right\}}}
\]


  \begin{block}{Two Cases}
    \begin{enumerate}
      \item Condition on $A$ occurring  
      \item Condition on $A$ \emph{not} occurring  
    \end{enumerate}
  \end{block}

  \vspace{-1em}

  \begin{block}{Problem}
    If $\delta$ were known, we could directly calculate how conditioning on $A$ affects the distribution of $Y_\beta$, but $\delta$ is unknown!
  \end{block}

  \begin{alertblock}{Solution}
    Condition on a sufficient statistic for $\delta$.
  \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Conditioning on a Sufficient Statistic}

  \small

  \begin{block}{Theorem}
   If $U$ is a sufficient statistic for $\delta$, then the joint distribution of $(Y_\beta, Y_\delta)$ given $U$ does not depend on $\delta$. 
  \end{block}

  \begin{block}{In Our Example}
    Residual $U = Y_\delta - \rho Y_{\beta}$ from a projection of $Y_\delta$ onto $Y_\beta$ is sufficient for $\delta$.
  \end{block}

  \begin{block}{Straightforward Calculation}
\begin{equation*}
  \left.\left[
  \begin{array}{c}
    Y_\beta \\ Y_{\delta}
  \end{array}
\right] \right| (U=u) 
 = \left[
\begin{array}{c}
  \beta + Z \\ u + \rho (\beta + Z)
\end{array}
\right], \quad Z \sim \mbox{N}(0,1)
\end{equation*}
\alert{Notice that this is a singular normal distribution}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Distribution of $Y_\beta|(A,U=u)$}

  \small
\begin{equation*}
  \alert{\boxed{\left.\left[
  \begin{array}{c}
    Y_\beta \\ Y_{\delta}
  \end{array}
\right] \right| (U=u) 
 = \left[
\begin{array}{c}
  \beta + Z \\ u + \rho (\beta + Z)
\end{array}
\right], \quad Z \sim \mbox{N}(0,1)}}
\end{equation*}

\vspace{2em}
  
Start with case in which $A$ occurs so we select $M1$.
Under $H_0\colon \beta = \beta_0$,
\begin{align*}
  \mathbb{P}_{\beta_0}\left( Y_\beta \leq y  | A, U=u \right) &= \frac{\mathbb{P}_{\beta_0}(\{Y_\beta \leq y\} \cap A|U=u)}{\mathbb{P}_{\beta_0}(A|U=u)} \\  
  &=\frac{\mathbb{P}\left(\left\{Z\leq y - \beta_0 \right\}\cap \left\{|u + \rho (\beta_0 + Z)| > c\right\}\right)}{\mathbb{P}\left(|u + \rho (\beta_0 + Z)| > c\right)}
\end{align*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$\mathbb{P}(A|U=u)$ under $H_0\colon \beta = \beta_0$}

\begin{align*}
  P_D(A) &\equiv P_{\beta_0}(A|U=u)\\
  &= \mathbb{P}\left(|u + \rho (\beta_0 + Z)| > c\right) \\
  &= \mathbb{P}\left[ u + \rho(\beta_0 + Z) > c \right]  + \mathbb{P}\left[ u + \rho(\beta_0 + Z) < -c \right] \\
  &= \mathbb{P}\left[ \rho(\beta_0 + Z) > c-u \right]  + \mathbb{P}\left[ u + \rho(\beta_0 + Z) < -c-u \right] \\ 
    &= 1 - \Phi\left( \displaystyle \frac{c - u}{\rho} - \beta_0 \right) + \Phi\left( \displaystyle \frac{-c- u}{\rho} - \beta_0 \right)
\end{align*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$\mathbb{P}(\{Y_{\beta} \leq y\}\cap A|U=u)$ under $H_0\colon \beta = \beta_0$}

  \footnotesize

\begin{align*}
 P_N(A) &\equiv \mathbb{P}(\{Y_{\beta} \leq y\}\cap A|U=u)\\
 &= \mathbb{P}\left(\left\{Z\leq y - \beta_0 \right\}\cap \left\{|u + \rho (\beta_0 + Z)| > c\right\}\right)\\
 %&= \mathbb{P}\left[\left\{ Z \leq (y - \beta_0) \right\} \cap\left\{Z > \displaystyle \frac{c-u}{\rho} - \beta_0\right\} \right]  + \mathbb{P}\left[\left\{ Z\leq (y - \beta_0) \right\} \cap \left\{Z < \displaystyle\frac{-c-u}{\rho} - \beta_0\right\} \right] \\ 
 %&= \mathbf{1}\left\{ y > \displaystyle \frac{c - u}{\rho} \right\}\left[ \Phi\left( y - \beta_0 \right) - \Phi\left( \displaystyle \frac{c - u}{\rho} - \beta_0 \right) \right] + \Phi\left( \min\left\{y - \beta_0, \, \displaystyle \frac{-c - u}{\rho} - \beta_0  \right\} \right)\\
 &= \left\{
 \begin{array}{l}
   \Phi\left( y - \beta_0 \right), \quad y < (-c - u)/\rho \\
   \Phi\left(\displaystyle \frac{-c-u}{\rho} - \beta_0 \right), \quad (-c - u)/\rho \leq y \leq (c - u)/\rho \\
   \Phi(y - \beta_0) - \Phi\left( \displaystyle \frac{c - u}{\rho} - \beta_0 \right) + \Phi\left(\displaystyle \frac{-c-u}{\rho} - \beta_0 \right), \quad y > (c - u)/\rho 
 \end{array}
 \right.
\end{align*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$F_{\beta_0}(y|A,U=u)$}

  \footnotesize
    Define $\ell(u) = (-c -u)/\rho$, $r(u) = (c - u)/\rho$. We have:

    \[F_{\beta_0}(y|A,U=u) = P_N(A) / P_D(A) \]
where
  \begin{align*}
    P_D(A) &\equiv 1 - \Phi\left( r(u) - \beta_0 \right) + \Phi\left( \ell(u) - \beta_0 \right)\\ \\
    P_N(A) &\equiv  \left\{
 \begin{array}{lr}
   \Phi\left( y - \beta_0 \right), & y < \ell(u) \\
   \Phi\left(\ell(u) - \beta_0 \right), &  \ell(u) \leq y \leq r(u) \\
   \Phi(y - \beta_0) - \Phi\left( r(u) - \beta_0 \right) + \Phi\left(\ell(u) - \beta_0 \right), & y > r(u) 
 \end{array}
 \right.
  \end{align*}

  \vspace{1em}

  \alert{Note that $F_{\beta_0}(y|A,U=u)$ has a \emph{flat region} where $\ell(u) \leq y \leq r(u)$}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$Q_{\beta_0}(p|A,U=u)$}

  \scriptsize

Inverting the CDF from the preceding slide:
\begin{equation*}
  Q_{\beta_0}(p|A,U=u) = 
  \left\{
  \begin{array}{lc}
    \beta_0 + \Phi^{-1}\left( p \times P_D(A) \right), & p < p^*\\
    \beta_0 + \Phi^{-1}\left[ p \times P_D(A) + \Phi\left(r(u) - \beta_0 \right) - \Phi\left( \ell(u) - \beta_0 \right) \right], & p \geq p^*
  \end{array}
  \right.
\end{equation*}

where
\begin{align*}
  p^* &\equiv \Phi\left( \ell(u) - \beta_0 \right)/P_D(A)\\
P_D(A) &\equiv 1 - \Phi\left( r(u) - \beta_0 \right) + \Phi\left( \ell(u) - \beta_0 \right)\\
\ell(u) &\equiv (-c-u)/\rho\\
r(u) &\equiv (c-u)/\rho
\end{align*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Distribution of $Y_\beta|(A^c,U=u)$}

  \small
\begin{equation*}
  \alert{\boxed{\left.\left[
  \begin{array}{c}
    Y_\beta \\ Y_{\delta}
  \end{array}
\right] \right| (U=u) 
 = \left[
\begin{array}{c}
  \beta + Z \\ u + \rho (\beta + Z)
\end{array}
\right], \quad Z \sim \mbox{N}(0,1)}}
\end{equation*}

\vspace{2em}
  
If $A$ does not occur, when we select $M2$.
Under $H_0\colon \beta = \beta_0$,
\begin{align*}
  \mathbb{P}_{\beta_0}\left( Y_\beta \leq y  | A^c, U=u \right) &= \frac{\mathbb{P}_{\beta_0}(\{Y_\beta \leq y\} \cap A^c|U=u)}{\mathbb{P}_{\beta_0}(A^c|U=u)} \\  
  &= \frac{\mathbb{P}\left(\left\{Z\leq y - \beta_0 \right\}\cap \left\{|u + \rho (\beta_0 + Z)| < c\right\}\right)}{\mathbb{P}\left(|u + \rho (\beta_0 + Z)| < c\right)}
\end{align*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$F_{\beta_0}(y|A^c,U=u)$}

  \footnotesize
    As above, define $\ell(u) = (-c -u)/\rho$, $r(u) = (c - u)/\rho$. We have:

    \[F_{\beta_0}(y|A^c,U=u) = P_N(A^c) / P_D(A^c) \]
where
\begin{align*}
  P_D(A^c) &\equiv \Phi\left(r(u) - \beta_0 \right) - \Phi\left( \ell(u) - \beta_0\right) \\
 P_N(A^c) &\equiv 
  \left\{
 \begin{array}{rr}
   0 ,& y < \ell(u)\\
   \Phi(y - \beta_0) - \Phi\left( \ell(u) - \beta_0 \right) ,& \ell(u) \leq y \leq r(u)\\
   \Phi\left(r(u) - \beta_0 \right) - \Phi\left( \ell(u) - \beta_0\right) 
 ,& y > r(u) \\
 \end{array}
 \right.
\end{align*}

\vspace{1em}
\alert{Notice that this is a CDF with a bounded support set: $y\in \left[ \ell(u), r(u) \right]$}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$Q_{\beta_0}(p|A^c,U=u)$}

  \footnotesize
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

