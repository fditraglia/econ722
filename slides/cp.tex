\section{Mallow's $C_p$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Motivation: Predict $\mathbf{y}$ from $\mathbf{x}$ via Linear Regression}

  \[\underset{\small(T\times1)}{\textbf{y}} = \underset{(T\times K)}{\mathbf{X} }\underset{(K\times 1)}{\boldsymbol{\beta}} + \boldsymbol{\epsilon}\]
\small
\[\mathbb{E}[\boldsymbol{\epsilon}|\mathbf{X}] = 0 ,\quad
\text{Var}(\boldsymbol{\epsilon}|\mathbf{X}) = \sigma^2 \mathbf{I}\]
\normalsize

\pause

\begin{itemize}
  \item If $\boldsymbol{\beta}$ were known, could never achieve lower MSE than by using all regressors to predict.\pause
  \item But $\boldsymbol{\beta}$ is unknown so we have to estimate it from data $\Rightarrow$ bias-variance tradeoff.\pause
  \item Could make sense to exclude regressors with small coefficients: add small bias but reduce variance.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Operationalizing the Bias-Variance Tradeoff Idea}

  \begin{alertblock}{Mallow's $C_p$}
    Approximate the predictive MSE of each model relative to the infeasible optimum in which $\boldsymbol{\beta}$ is known. 
  \end{alertblock}

  \pause

  \begin{block}{Notation}
    \begin{itemize}
      \item Model index $m$ and regressor matrix $\mathbf{X}_m$
      \item Corresponding OLS estimator $\boldsymbol{\widehat{\beta}}$ padded out with zeros
      \item $\mathbf{X} \widehat{\boldsymbol{\beta}}_m = \mathbf{X}_{(-m)}\mathbf{0} + \mathbf{X}_m\left[ (\mathbf{X}_m'\mathbf{X}_m)^{-1}\mathbf{X}_m' \mathbf{y} \right] = \mathbf{P}_m \mathbf{y}$
    \end{itemize}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{In-sample versus Out-of-sample Prediction Error}

  \begin{block}{Why not compare $\mbox{RSS}(m)$?}
    In-sample prediction error: $\text{RSS}(m) = (\mathbf{y} - \mathbf{X}\widehat{\boldsymbol{\beta}}_m)' (\mathbf{y} - \mathbf{X}\widehat{\boldsymbol{\beta}}_m)$
  \end{block}

  \pause

  \begin{block}{From your Problem Set}
    RSS cannot decrease even if we add irrelevant regressors. 
    Thus in-sample prediction error is an \alert{overly optimistic} estimate of out-of-sample prediction error.
  \end{block}

  \pause

  \begin{block}{Bias-Variance Tradeoff}
   Out-of-sample performance of full model (using all regressors) could be very poor if there is a lot of estimation uncertainty associated with regressors that aren't very predictive. 
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Predictive MSE of $\mathbf{X}\widehat{\boldsymbol{\beta}}_m$ relative to infeasible optimum $\boldsymbol{X}\beta$}

    % Note that we can't use align with beamer pauses so we have to use eqnarray!

  \begin{block}{\small Step 1: Algebra}
    \vspace{-1em}
    \small
    \begin{eqnarray*}
      \mathbf{X}\widehat{\boldsymbol{\beta}}_m - \mathbf{X}\boldsymbol{\beta} &=& \mathbf{P}_m \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \pause =  \mathbf{P}_m(\mathbf{y}-\mathbf{X}\boldsymbol{\beta}) - (\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta} \\ \pause
  &=& \mathbf{P}_m \boldsymbol{\epsilon} - (\mathbf{I}- \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta}
    \end{eqnarray*}
  \end{block}

  \pause
  \vspace{-1em}

  \begin{block}{\small Step 2: $\mathbf{P}_m$ and $(\mathbf{I} - \mathbf{P}_m)$ are symmetric, idempotent, and orthogonal} 
    \small

    \vspace{-1em}
\begin{eqnarray*}
  \left|\left|\mathbf{X}\widehat{\boldsymbol{\beta}}_m - \mathbf{X}\boldsymbol{\beta}\right|\right|^2 &=&\left\{ \mathbf{P}_m \boldsymbol{\epsilon} - (\mathbf{I}- \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta}\right\}'\left\{ \mathbf{P}_m \boldsymbol{\epsilon} + (\mathbf{I}- \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta} \right\}\\ \pause
  &=&\boldsymbol{\epsilon}'\mathbf{P}_m'\mathbf{P}_m \boldsymbol{\epsilon} - \boldsymbol{\beta}'\mathbf{X}'(\mathbf{I}-\mathbf{P}_m)'\mathbf{P}_m\boldsymbol{\epsilon} - \boldsymbol{\epsilon}'\mathbf{P}_m'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta}\\
  &&\quad \quad + \; \boldsymbol{\beta}'\mathbf{X}' (\mathbf{I} - \mathbf{P}_m)(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta}\\ \pause
  &=& \boldsymbol{\epsilon}'\mathbf{P}_m \boldsymbol{\epsilon} + \boldsymbol{\beta}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta}
\end{eqnarray*}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Predictive MSE of $\mathbf{X}\widehat{\boldsymbol{\beta}}_m$ relative to infeasible optimum $\boldsymbol{X}\boldsymbol{\beta}$}
  \begin{block}{Step 3: Expectation of Step 2 conditional on $\mathbf{X}$}
	\begin{eqnarray*}
    \mbox{MSE}(m|\mathbf{X}) &=& \mathbb{E}\left[(\mathbf{X}\widehat{\boldsymbol{\beta}}_m - \mathbf{X}\boldsymbol{\beta})'(\mathbf{X}\widehat{\boldsymbol{\beta}}_m - \mathbf{X}\boldsymbol{\beta})|\mathbf{X} \right]\\
    &=& \mathbb{E}\left[\boldsymbol{\epsilon}'\mathbf{P}_m \boldsymbol{\epsilon}|\mathbf{X}\right] + \mathbb{E}\left[\boldsymbol{\beta}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta} |\mathbf{X}\right]\\ \pause
    &=&\mathbb{E}\left[\mbox{tr}\left\{\boldsymbol{\epsilon}'\mathbf{P}_m \boldsymbol{\epsilon}\right\}|\mathbf{X}\right] + \boldsymbol{\beta}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta} \\ \pause
    &=&\mbox{tr}\left\{\mathbb{E}[\boldsymbol{\epsilon} \boldsymbol{\epsilon}'|\mathbf{X}]\mathbf{P}_m\right\} + \boldsymbol{\beta}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta} \\ \pause
    &=&\mbox{tr}\left\{\sigma^2 \mathbf{P}_m\right\} + \boldsymbol{\beta}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta} \\ \pause
    &=& \sigma^2 k_m + \boldsymbol{\beta}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta}
	\end{eqnarray*}

  \vspace{1em}
  \small
  \alert{where $k_m$ denotes the number of regressors in $\mathbf{X}_m$ and $\text{tr}(\mathbf{P}_m) = \text{tr}\left\{ \mathbf{X}_m\left( \mathbf{X}_m'\mathbf{X}_m \right)^{-1}\mathbf{X}_m' \right\} = \text{tr}\left\{ \mathbf{X}'_m \mathbf{X}_m \left( \mathbf{X}_m' \mathbf{X}_m \right)^{-1} \right\} = \text{tr}(\mathbf{I}_m)$}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Now we know the MSE of a given model\dots}
  
  \[\text{MSE}(m|\mathbf{X}) = \sigma^2 k_m + \boldsymbol{\beta}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta}\]

\begin{block}{Bias-Variance Tradeoff}
  \pause
  \begin{itemize}
    \item Smaller Model $\Rightarrow \sigma^2 k_m$ smaller: less estimation uncertainty. \pause
    \item Bigger Model $\Rightarrow \mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X} = \left|\left|(\mathbf{I} - \mathbf{P}_m)\mathbf{X} \right|\right|^2$ is in general smaller: less (squared) bias.
  \end{itemize}
\end{block}

\vspace{-1em}

\pause
\begin{alertblock}{Mallow's $C_p$}
  \begin{itemize}
    \item Problem: MSE formula is infeasible since it involves $\boldsymbol{\beta}$ and $\sigma^2$.\pause
    \item Solution: Mallow's $C_p$ constructs an unbiased estimator.
    \item First step: what about plugging in $\widehat{\boldsymbol{\beta}}$ to estimate second term?
  \end{itemize}
\end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{What if we plug in $\widehat{\boldsymbol{\beta}}$ to estimate the second term?}
  \framesubtitle{For the missing algebra in Step 4, see the lecture notes.}
  \begin{block}{Notation}
    Let $\widehat{\boldsymbol{\beta}}$ denote the full model estimator and $\mathbf{P}$ be the corresponding projection matrix: $\mathbf{X}\widehat{\beta} = \mathbf{P}\mathbf{y}$.
  \end{block}

  \pause

  \begin{alertblock}{Crucial Fact}
    $\text{span}(\mathbf{X}_m)$ is a subspace of $\text{span}(\mathbf{X})$, so $\mathbf{P}_m \mathbf{P} = \mathbf{P} \mathbf{P}_m = \mathbf{P}_m$.
  \end{alertblock}

  \pause

  \begin{block}{Step 4: Algebra using the preceding fact }
    \vspace{-1em}
   \[
     \mathbb{E}\left[ \widehat{\boldsymbol{\beta}}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m) \mathbf{X}\widehat{\boldsymbol{\beta}}|\mathbf{X}\right] = \dots = \boldsymbol{\beta}' \mathbf{X}' (\mathbf{I} - \mathbf{P}_m)\mathbf{X} \boldsymbol{\beta} + \mathbb{E}\left[ \boldsymbol{\epsilon}'(\mathbf{P} - \mathbf{P}_m)\boldsymbol{\epsilon} | \mathbf{X}\right]
   \]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Substituting $\widehat{\boldsymbol{\beta}}$ doesn't work\dots}

\begin{block}{Step 5: Use ``Trace Trick'' on second term from Step 4}
  \vspace{-1.5em} 
    \begin{eqnarray*}
      \mathbb{E}[\boldsymbol{\epsilon}'(\mathbf{P} - \mathbf{P}_m)\boldsymbol{\epsilon}|\mathbf{X}]&=&\mathbb{E}[\mbox{tr}\left\{\boldsymbol{\epsilon}'(\mathbf{P} - \mathbf{P}_m)\boldsymbol{\epsilon}\right\}|\mathbf{X}]\\ \pause
      &=&  \mbox{tr}\left\{\mathbb{E}[\boldsymbol{\boldsymbol{\epsilon}\epsilon}'|\mathbf{X}](\mathbf{P} - \mathbf{P}_m)\right\}\\ \pause
      &=&\mbox{tr}\left\{\sigma^2(\mathbf{P} - \mathbf{P}_m)\right\}\\ \pause
      &=& \sigma^2 \left(\mbox{trace}\left\{\mathbf{P}\right\} - \mbox{trace}\left\{\mathbf{P}_m\right\}\right)\\  \pause
			&=& \sigma^2 (K - k_m) 
      \end{eqnarray*}
      where $K$ is the total number of regressors in $\mathbf{X}$
\end{block}

\pause

\begin{block}{Bias of Plug-in Estimator}
  \vspace{-1.5em}
  \[
    \mathbb{E}\left[ \widehat{\boldsymbol{\beta}}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m) \mathbf{X}\widehat{\boldsymbol{\beta}}|\mathbf{X}\right] = \underbrace{\boldsymbol{\beta}' \mathbf{X}' (\mathbf{I} - \mathbf{P}_m)\mathbf{X} \boldsymbol{\beta}}_{\text{Truth}} +
    \alert{\underbrace{\sigma^2(K - k_m)}_{\text{Bias}}}
   \]
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Putting Everything Together: Mallow's $C_p$}

  \begin{block}{Want An Unbiased Estimator of This:}
    \vspace{-1em}
  \[\text{MSE}(m|\mathbf{X}) = \sigma^2 k_m + \boldsymbol{\beta}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\boldsymbol{\beta}\]
\end{block}

\vspace{-1em}
\begin{block}{Previous Slide:}
  \vspace{-1em}
  \[
    \mathbb{E}\left[ \widehat{\boldsymbol{\beta}}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m) \mathbf{X}\widehat{\boldsymbol{\beta}}|\mathbf{X}\right] = \boldsymbol{\beta}' \mathbf{X}' (\mathbf{I} - \mathbf{P}_m)\mathbf{X} \boldsymbol{\beta} + \sigma^2(K - k_m)
   \]
 \end{block}

 \pause

 \begin{alertblock}{End Result:}
   \vspace{-2.5em}
   \begin{eqnarray*}
     \text{MC}(m) &=& \widehat{\sigma}^2 k_m  + \left[ \widehat{\boldsymbol{\beta}}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\widehat{\boldsymbol{\beta}} +  \widehat{\sigma}^2 (2k_m - K) \right]\\
     &=& \widehat{\boldsymbol{\beta}}'\mathbf{X}'(\mathbf{I} - \mathbf{P}_m)\mathbf{X}\widehat{\boldsymbol{\beta}} + \widehat{\sigma}^2(2 k_m - K) 
   \end{eqnarray*}
   where $\widehat{\sigma}^2 = \mathbf{y}'(\mathbf{I} - \mathbf{P})\mathbf{y}/(T-K)$ is an unbiased estimator of MSE. 
 \end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Why is this different from the textbook formula?}

  \vspace{-1em}

  \begin{block}{Just algebra, but tedious\dots}

    \vspace{-2em}

   \small 
\begin{eqnarray*}
  \vspace{-1em}
  \text{MC}(m) - 2\widehat{\sigma}^2 k_m &=& \widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} - K\widehat{\sigma}^2\\
			&\vdots& \\
			&=& \mathbf{y}'(\mathbf{I} - P_M)\mathbf{y} - T\widehat{\sigma}^2\\
      &=& \text{RSS}(m) - T\widehat{\sigma}^2
      \end{eqnarray*}
  \end{block}

  \vspace{-2em}

  \pause

  \begin{block}{Therefore:}
    \vspace{-1.5em}
    \small
   \[
     \text{MC}(m) = \text{RSS}(m) + \widehat{\sigma}^2(2 k_m - T)
   \]
  \end{block}


  \vspace{-1em}

  \pause

  \begin{block}{Divide Through by $\widehat{\sigma}^2$:}
    \vspace{-1em}
    \small
  \[
    C_p(m) = \frac{\text{RSS}(m)}{\widehat{\sigma}^2} + 2k_m - T
  \]
  Tells us how to adjust RSS for number of regressors\dots
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
