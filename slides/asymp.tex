\section{Overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Overview}

  \begin{block}{Asymptotic Properties}
   What happens as the sample size increases?
  \end{block}

  \begin{block}{Consistency}
    Choose ``best'' model with probability approaching 1 in the limit.
  \end{block}

  \begin{block}{Efficiency}
    Post-model selection estimator with low risk.
  \end{block}

  \begin{block}{Some References}
      Sin and White (1992, 1996), P\"{o}tscher (1991), Leeb \& P\"{o}tscher (2005), Yang (2005) and Yang (2007).
  \end{block}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Penalizing the Likelihood}
  
  \begin{block}{Examples we've seen:}
	\begin{eqnarray*}
		 TIC &=& 2\ell_T(\widehat{\theta}) -\mbox{trace}\left\{\widehat{J}^{-1} \widehat{K} \right\}\\
		AIC &=& 2\ell_T(\widehat{\theta}) - 2\; \mbox{length}(\theta)\\
		BIC &=& 2\ell_T(\widehat{\theta}) - \log(T)\; \mbox{length}(\theta)
	\end{eqnarray*}

  \pause

  \begin{block}{Generic penalty $c_{T,k}$}
    \[IC(M_k) = 2 \sum_{t=1}^T \log f_{k,t}(Y_t| \widehat{\theta_k}) - c_{T,k}\]
  \end{block}

  \pause

  \begin{alertblock}{How does choice of $c_{T,k}$ affect behavior of the criterion?}
  \end{alertblock}


  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Weak Consistency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Weak Consistency: Suppose $\text{M}_{k_0}$ Uniquely Minimizes KL}
  
  \begin{block}{Assumption}

    \vspace{-1em}
	$$\underset{T\rightarrow \infty}{\lim\inf}\left(\underset{k \neq k_0}{\min} \frac{1}{T}\sum_{t = 1}^T \left\{ KL(g; f_{k,t}) - KL(g;f_{k_0,t}) \right\} \right) > 0$$
  \end{block}

  \pause

  \begin{block}{Consequences}
    \begin{itemize}
      \item Any criterion with $c_{T,k}> 0$ and $c_{T,k} = o_p(T)$ is weakly consistent: \alert{selects $\text{M}_{k_0}$ wpa 1 in the limit}. \pause
      \item Weak consistency still holds if $c_{T,k}$ is zero for one of the models, so long as it is strictly positive for all the others.
    \end{itemize}
  \end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Both AIC and BIC are Weakly Consistent}
  
Both satisfy $T^{-1}c_{T,k} \overset{p}{\rightarrow} 0$.
	\begin{eqnarray*}
		\mbox{BIC Penalty:}&& c_{T,k} = \log(T) \times \mbox{length}(\theta_k)\\
		\mbox{AIC Penalty:} && c_{T,k} = 2\times \mbox{length}(\theta_k)
	\end{eqnarray*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Consistency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Consistency: No Unique KL-minimizer}

  \begin{block}{Example}
    If the truth is an AR(5) model then AR(6), AR(7), AR(8), etc.\  models \alert{all have zero KL-divergence}. 
  \end{block}

  \pause

  \begin{block}{Principle of Parsimony}
    Among the KL-minimizers, choose the \alert{simplest model}, i.e.\ the one with the fewest parameters.
  \end{block}

  \pause

  \begin{block}{Notation}
  $\mathcal{J} = $ be the set of all models that attain minimum KL-divergence\\
  $\mathcal{J}_0= $ subset with the minimum number of parameters. 
  \end{block}
 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Sufficient Conditions for Consistency -- (A)}
%
%  \begin{block}{Consistency: Select Model from $\mathcal{J}_0$ wpa 1}
%    $$\lim_{T\rightarrow \infty}\mathbb{P}\left\{ \underset{\ell \in \mathcal{J}\backslash \mathcal{J}_0}{\min} \left[ IC(M_{j_0}) - IC(M_\ell)\right] > 0 \right\} = 1$$
%  \end{block}
% 
%  \begin{block}{Sufficient Conditions (A)}
%			\begin{enumerate}[(i)]
%				\item For all $k \neq \ell \in \mathcal{J}$ 
%					$$\underset{T\rightarrow \infty}{\lim\sup} \frac{1}{\sqrt{T}} \sum_{t=1}^T\left\{ KL(g; f_{k,t}) - KL(g;f_{\ell,t}) \right\}<\infty$$
%				\item For all $j_0 \in \mathcal{J}_0$ and $\ell \in (\mathcal J \backslash \mathcal{J}_0)$
%					$$P\left\{\left(c_{T,\ell} - c_{T,j_0} \right)/\sqrt{T} \rightarrow \infty \right\}= 1$$
%			\end{enumerate}
%\end{block}
%  
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Sufficient Conditions for Consistency}

  \begin{block}{Consistency: Select Model from $\mathcal{J}_0$ wpa 1}
    $$\lim_{T\rightarrow \infty}\mathbb{P}\left\{ \underset{\ell \in \mathcal{J}\backslash \mathcal{J}_0}{\min} \left[ IC(M_{j_0}) - IC(M_\ell)\right] > 0 \right\} = 1$$
  \end{block}

  \pause
 
  \begin{block}{Sufficient Conditions}
			\begin{enumerate}[(i)]
				\item For all $k \neq \ell \in \mathcal{J}$ 
					$$\sum_{t=1}^T \left[\log f_{k,t}(Y_t|\theta^*_k) - \log f_{\ell,t}(Y_t|\theta^*_\ell) \right] = O_p(1)$$
				where $\theta^*_k$ and $\theta^*_\ell$ are the KL minimizing parameter values.
				\item For all $j_0 \in \mathcal{J}_0$ and $\ell \in (\mathcal J \backslash \mathcal{J}_0)$
					$$P\left(c_{T,\ell} - c_{T,j_0} \rightarrow \infty \right) = 1$$
			\end{enumerate}
\end{block}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{BIC is Consistent; AIC and TIC Are Not}

  \begin{itemize}
    \item AIC and TIC \emph{cannot} satisfy (ii) since $(c_{T,\ell} - c_{T,j_0})$ \emph{does not depend on sample size}. \pause
    \item  It turns out that AIC and TIC are \emph{not} consistent. \pause
    \item BIC is consistent:
	$$c_{T,\ell} - c_{T,j_0} = \log(T) \left\{\mbox{length}(\theta_\ell) - \mbox{length}(\theta_{j_0}) \right\}$$
  \item Term in braces is \emph{positive} since $\ell \in\mathcal{J}\backslash \mathcal{J}_0$, i.e.\ $\ell$ is not as parsimonious as $j_0$
  \item $\log(T) \rightarrow \infty$, so BIC always selects a model in $\mathcal{J}_0$ in the limit.
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Efficiency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Efficiency: Risk Properties of Post-selection Estimator}


  \begin{block}{Setup}
   \begin{itemize}
      \item Models $M_0$ and $M_1$; corresponding estimators $\widehat{\theta}_{0,T}$ and $\widehat{\theta}_{1,T}$
      \item Model Selection: If $\widehat{M} = 0$ choose $M_0$; if $\widehat{M} = 1$ choose $M_1$.
    \end{itemize}
  \end{block}

  \begin{block}{Post-selection Estimator}
    \vspace{-1.5em}
    \[\widehat{\theta}_{\widehat{M},T} \equiv \mathbf{1}_{\{ \widehat{M} = 0 \}} \widehat{\theta}_{0,T} + \mathbf{1}_{\{ \widehat{M} = 1 \}}\widehat{\theta}_{1,T}\]
  \end{block}

  \vspace{-1em}

  \begin{block}{Two Sources of Randomness}
    Variability in $\widehat{\theta}_{\widehat{M},T}$ arises both from $\left(\widehat{\theta}_{0,T}, \widehat{\theta}_{1,T} \right)$ and from $\widehat{M}$.
  \end{block}

  \begin{alertblock}{Question}
    How does the risk of $\widehat{\theta}_{\widehat{M},T}$ compare to that of other estimators?
  \end{alertblock}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Efficiency: Risk Properties of Post-selection Estimator}

  \begin{block}{Pointwise-risk Adaptivity}
    $\widehat{\theta}_{\widehat{M},T}$ is \textbf{pointwise-risk adaptive} if for any fixed $\theta \in \Theta$,
    \[
      \frac{R(\theta, \widehat{\theta}_{\widehat{M},T})}{\min\left\{ R(\theta, \widehat{\theta}_{0,T}),\, R(\theta, \widehat{\theta}_{1,T}) \right\}} \rightarrow 1, \quad \mbox{as } T\rightarrow \infty
    \]
  \end{block}

  \begin{block}{Minimax-rate Adaptivity}
    $\widehat{\theta}_{\widehat{M},T}$ is \textbf{minimax-rate adaptive} if
    \[
      \sup_{T} \left[\frac{\displaystyle\sup_{\theta\in \Theta} R(\theta, \widehat{\theta}_{\widehat{M},T})}{\displaystyle\inf_{\widetilde{\theta}_{T}}\sup_{\theta \in \Theta} R(\theta, \widetilde{\theta}_{T})}\right] < \infty
    \]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Strengths of AIC and BIC Cannot be Shared}

  \begin{alertblock}{Theorem}
      No model post-model selection estimator can be both pointwise-risk adaptive and minimax-rate adaptive.
  \end{alertblock}

  \begin{block}{AIC vs.\ BIC}
  \begin{itemize}
    \item BIC is pointwise-risk adaptive but AIC is not. (This is effectively identical to consistency.)
    \item AIC is minimax-rate adaptive, but BIC is not.
    \item Further Reading: Yang (2005), Yang (2007)
  \end{itemize}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AIC versus BIC in a Simple Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Consistency and Efficiency in a Simple Example}

  \begin{block}{Information Criteria}
    Consider criteria of the form $\text{IC}_m = 2\ell(\theta) - d_T \times \text{length}(\theta)$.
  \end{block}
 
  \pause
  
  \begin{block}{True DGP}
  $Y_{1}, \dots, Y_T \sim \mbox{iid N}(\mu, 1)$
  \end{block}

  \pause

  \begin{block}{Candidate Models}
    $\text{M}_0$ assumes $\mu = 0$, $\text{M}_1$ does not restrict $\mu$. Only one parameter:

    \vspace{-1em}
  \begin{align*}
    \text{IC}_0 &= 2 \max_\mu \left\{ \ell(\mu) \colon \text{M}_0 \right\} \\ 
    \text{IC}_1 &= 2 \max_\mu \left\{ \ell(\mu) \colon \text{M}_1 \right\} - d_T
  \end{align*}
  \end{block}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Log-Likelihood Function}

  \begin{block}{Simple Algebra}
    \vspace{-1.5em}
\[ 
    \ell_T(\mu)=  \mbox{Constant} - \frac{1}{2} \sum_{t=1}^{T} (Y_t - \mu)^2
\]
\end{block}

\begin{block}{Tedious Algebra}
    \vspace{-1.5em}
\[
\sum_{t=1}^T (Y_t -\mu)^2 = T(\bar{Y} - \mu)^2 + T \widehat{\sigma}^2
\]
\end{block}

\begin{alertblock}{Combining These}
    \vspace{-1.5em}
    \[
      \ell_T(\mu) = \mbox{Constant} - \frac{T}{2}\left( \bar{Y} - \mu \right)^2
    \]

\end{alertblock}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Log-Likelihood Function}
%
%  Since $\; \alert{\sum_{t=1}^T (Y_t -\mu)^2 = T(\bar{Y} - \mu)^2 + T \widehat{\sigma}^2}$,
%
%\begin{eqnarray*}
%  \ell_T(\mu)&=& \sum_{t=1}^T \log \left( \frac{1}{\sqrt{2\pi}} \exp \left\{-\frac{1}{2}(Y_t - \mu)^2 \right\}\right)\\ \pause
%  &=& -\frac{T}{2} \log\left( 2\pi \right) - \frac{1}{2} \sum_{t=1}^{T} (Y_t - \mu)^2\\ \pause
%  &=& -\frac{T}{2} \log\left( 2\pi \right) - \frac{T}{2} \widehat{\sigma}^2 - \frac{T}{2}(\bar{Y} - \mu)^2\\ \pause
%  &=& \text{Constant} - \frac{T}{2}(\bar{Y} - \mu)^2
%\end{eqnarray*}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Side Calculation: \normalsize $\sum_{t=1}^T (Y_t -\mu)^2 = T(\bar{Y} - \mu)^2 + T \widehat{\sigma}^2$}
%
%\scriptsize
%\begin{eqnarray*}
%  T\widehat{\sigma}^2 &=& \sum_{t=1}^T \left(Y_t - \bar{Y}\right)^2 = \sum_{t=1}^T \left(Y_t - \mu + \mu - \bar{Y}\right)^2 = \sum_{t=1}^T \left[(Y_t -\mu) - (\bar{Y} - \mu)\right]^2\\
%		&=&\sum_{t=1}^T (Y_t -\mu)^2 - \sum_{t=1}^T 2(Y_t -\mu)(\bar{Y} - \mu) + \sum_{t=1}^T (\bar{Y} - \mu)^2\\
%				&=& \left[  \sum_{t=1}^T \left(Y_t - \mu\right)^2\right]   - 2(\bar{Y} - \mu) \left( \sum_{t=1}^T Y_t- \sum_{t=1}^T \mu \right)+T(\bar{Y} - \mu)^2\\
%				&=& \left[  \sum_{t=1}^T \left(Y_t - \mu\right)^2\right]   - 2(\bar{Y} - \mu)(T\bar{Y}-T\mu)+T(\bar{Y} - \mu)^2\\
%				&=&\left[  \sum_{t=1}^T \left(Y_t - \mu\right)^2\right]   - 2T(\bar{Y} - \mu)^2+T(\bar{Y} - \mu)^2\\
%				&=&\left[  \sum_{t=1}^T \left(Y_t - \mu\right)^2\right]   - T(\bar{Y} - \mu)^2
%\end{eqnarray*}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Selected Model $\widehat{\text{M}}$}

  \begin{block}{Information Criteria}
  M$_0$ sets $\mu=0$ while M$_1$ uses the MLE $\bar{Y}$, so we have
  \vspace{1em}
  \begin{align*}
    \text{IC}_0 &= 2 \max_\mu \left\{ \ell(\mu) \colon \text{M}_0 \right\} = 2 \times \text{Constant} - T\bar{Y}^2\\ 
    \text{IC}_1 &= 2 \max_\mu \left\{ \ell(\mu) \colon \text{M}_1 \right\} - d_T = 2 \times \text{Constant} - d_T\\ 
  \end{align*}
\end{block}

\pause

\vspace{-2em}

  \begin{block}{Difference of Criteria}
    \vspace{-1em}
  \[
    \text{IC}_1 - \text{IC}_0 = T \bar{Y}^2 - d_T
  \]
\end{block}

\pause

\vspace{-1em}

  \begin{block}{Selected Model}
    \vspace{-1em}
   \[
     \widehat{\text{M}} = \left\{
       \begin{array}{cc}
         \text{M}_1, & |\sqrt{T} \bar{Y}| \geq \sqrt{d_T}\\
         \text{M}_0, & |\sqrt{T} \bar{Y}| < \sqrt{d_T}
       \end{array}
       \right.
   \]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Case I: $\mu \neq 0$}
%  \framesubtitle{Apply theory from earlier in lecture\dots}
%
%  \begin{block}{KL-Divergence of $\text{M}_1$}
%    $\text{M}_1$ is the true DGP with minimized KL-divergence equal to zero.
%  \end{block}
%
%  \pause
%
%  \begin{block}{KL-Divergence of $\text{M}_0$}
%    \begin{itemize}
%      \item Truth: $g(y) = (2\pi)^{-1/2}\exp\left\{ -(y-\mu)^2/2 \right\}$  \pause
%      \item $\text{M}_0$: $f(y) = (2\pi)^{-1/2}\exp\left\{ -y^2/2\right\}$ \pause
%      \item Hence: $\log g(y) - \log f(y) = -\frac{1}{2}(y-\mu)^2 + \frac{1}{2}y^2 
%          = \mu \left(y - \frac{\mu}{2}\right)$ \pause
%    \end{itemize}
%
%    \vspace{-1em}
%          \begin{align*}
%          \text{KL}(g;\text{M}_0) &= \int_{\mathbb{R}}\mu(y - \mu/2) (2\pi)^{-1/2}\exp\left\{ (y-\mu)^2/2 \right\}\; \text{d}y \\
%          &= \mu(\mu - \mu/2) = \mu^2 /2
%        \end{align*}
%  \end{block}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Verifying Weak Consistency: $\mu \neq 0$}

  \begin{block}{KL Divergence for $M_0$ and $M_1$}
$KL(g;M_0) = \mu^2/2, \quad KL(g;M_1) = 0$
  \end{block}

  \begin{block}{Condition on KL-Divergence}
  \small
  \vspace{-2em}
  \[
    \underset{T\rightarrow \infty}{\lim\inf} \frac{1}{T}\sum_{t = 1}^T \left\{ KL(g; \text{M}_0) - KL(g;\text{M}_1) \right\} = \underset{T\rightarrow \infty}{\lim\inf}\ \frac{1}{T}\sum_{t = 1}^T  \left(\frac{\mu^2}{2} - 0\right) > 0
  \]
\end{block}

\pause

\begin{block}{Condition on Penalty}
  \begin{itemize}
    \item Need $c_{T,k} = o_p(T)$, i.e.\ $c_{T,k}/T \overset{p}{\rightarrow} 0$.  
    \item Both AIC and BIC satisfy this
    \item If $\mu \neq 0$, both AIC and BIC select $\text{M}_1$ wpa 1 as $T\rightarrow \infty$.
  \end{itemize}
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Verifying Consistency: $\mu = 0$}

  \begin{block}{What's different?}
  \begin{itemize}
    \item Both $M_1$ and $M_0$ are true and minimize KL divergence at zero. 
    \item \alert{Consistency} says choose most parsimonious true model: $\text{M}_0$
  \end{itemize}
  \end{block}

  \pause

  \begin{block}{Verifying Conditions for Consistency}
    \begin{itemize}
      \item $N(0,1)$ model nested inside $N(\mu,1)$ model
      \item Truth is $N(0,1)$ so LR-stat is asymptotically $\chi^2(1) = O_p(1)$.
      \item For penalty term, need $\mathbb{P}(c_{T,k} - c_{T,0})\rightarrow \infty$
      \item BIC satisfies this but AIC doesn't.
    \end{itemize}
    
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Finite-Sample Selection Probabilities: AIC}
  \begin{block}{AIC Sets $d_T = 2$}


	$$\widehat{M}_{AIC} = \left\{\begin{array}
		{cc} M_1, &|\sqrt{T}\bar{Y}| \geq \sqrt{2} \\
		M_0, & |\sqrt{T} \bar{Y}| < \sqrt{2}
	\end{array} \right.$$
  \end{block}

  \pause

  \vspace{-2em}

    \footnotesize
	\begin{eqnarray*}
		P\left(\widehat{M}_{AIC} = M_1\right) &=& P\left(\left|\sqrt{T}\bar{Y} \right| \geq \sqrt{2}  \right)\\ \pause
		&=& P\left(\left|\sqrt{T}\mu + Z\right| \geq \sqrt{2}  \right)\\\pause
		&=& P\left(\sqrt{T}\mu + Z \leq -\sqrt{2}\right) + \left[1 - P\left(\sqrt{T} \mu +Z \leq \sqrt{2}\right) \right]\\\pause
			&=& \Phi\left(-\sqrt{2} - \sqrt{T}\mu\right) + \left[1 -  \Phi\left(\sqrt{2} - \sqrt{T} \mu \right)\right]
	\end{eqnarray*}

  \normalsize
where $Z \sim N(0,1)$ since $\bar{Y} \sim N(\mu, 1/T)$ because $Var(Y_t)=1$.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Finite-Sample Selection Probabilities: BIC}

  \begin{block}{BIC sets $d_T = \log(T)$}
	$$\widehat{M}_{BIC} = \left\{\begin{array}
		{cc} M_1, & |\sqrt{T}\bar{Y} | \geq \sqrt{\log(T)} \\
		M_0, & |\sqrt{T} \bar{Y}| < \sqrt{\log(T)}
	\end{array} \right.$$
  \end{block}

  \pause
Same steps as for the AIC except with $\sqrt{\log(T)}$ in the place of $\sqrt{2}$:
\footnotesize
	\begin{eqnarray*}
		P\left(\widehat{M}_{BIC} = M_1\right) &=& P\left(\left|\sqrt{T}\bar{Y} \right| \geq \sqrt{\log(T)}  \right)\\
			&=& \Phi\left(-\sqrt{\log(T)} - \sqrt{T}\mu\right) + \left[1 -  \Phi\left(\sqrt{\log(T)} - \sqrt{T} \mu \right)\right]
	\end{eqnarray*}

  \pause

  \begin{block}{Interactive Demo: AIC vs BIC}

\url{https://fditraglia.shinyapps.io/CH\_Figure\_4\_1/}
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Probability of Over-fitting}

  \small

  \begin{itemize}
    \item If $\mu = 0$ both models are true but $M_0$ is more parsimonious.\pause 
    \item Probability of over-fitting ($Z$ denotes standard normal): 
\begin{eqnarray*}
	P\left(\widehat{M} = M_1\right) &=& P\left(|\sqrt{T}\bar{Y}|\geq \sqrt{d_T}\right) = P(|Z|\geq \sqrt{d_T})\\
	 &=& P(Z^2 \geq d_T) = P(\chi^2_1 \geq d_T)
\end{eqnarray*}\pause
\item AIC: $d_T = 2$ and $P(\chi^2_1 \geq 2)\approx 0.157$.  \pause
\item BIC: $d_T = \log(T)$ and $P(\chi^2_1 \geq \log T) \rightarrow 0$ as $T\rightarrow \infty$.
  \end{itemize}

  \alert{AIC has $\approx$ 16\% prob.\ of over-fitting; BIC does not over-fit in the limit.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Risk of the Post-Selection Estimator}
  \begin{block}{The Post-Selection Estimator}
	$$\widehat{\mu}=\left\{\begin{array}
		{cc} \bar{Y}, & |\sqrt{T}\bar{Y} | \geq \sqrt{d_T} \\
		0, & |\sqrt{T}\bar{Y} | < \sqrt{d_T}
		\end{array}\right.$$
  \end{block}

  \pause

  \vspace{-2em}

  \begin{block}{Recall from above}
Recall from above that $\sqrt{T} \bar{Y} = \sqrt{T}\mu +Z$ where $Z\sim N(0,1)$
  \end{block}

  \pause

  \begin{block}{Risk Function}
MSE risk times $T$ to get risk relative to minimax rate: $1/T$.

\[
  R(\mu, \widehat{\mu}) = T \cdot \mathbb{E}\left[\left( \widehat{\mu} - \mu\right)^2\right] = \mathbb{E}\left[\left(\sqrt{T} \widehat{\mu} - \sqrt{T} \mu\right)^2\right] 
\]
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Simplifying the Risk Function}
%  \framesubtitle{$\sqrt{T} \bar{Y} = \sqrt{T}\mu +Z$ where $Z\sim N(0,1)$}
%
%  \footnotesize
%
%  Let $X = \mathbf{1}\left\{ A \right\}$ where $A = \left\{|\sqrt{T}\mu + Z|\geq \sqrt{d_T}  \right\}$ \pause
%  \begin{eqnarray*}
%    R(\mu, \widehat{\mu}) &=& \mathbb{E}\left[\left(\sqrt{T} \widehat{\mu} - \sqrt{T} \mu\right)^2\right] \\ \pause
%  &=& \mathbb{E}\left\{ \left[ \left( \sqrt{T}\mu + Z \right)X - \sqrt{T}\mu \right]^2 \right\}\\ \pause
%  &=& \mathbb{P}(A)\; \mathbb{E}\left\{ \left.\left[ \left(\sqrt{T}\mu + Z\right) - \sqrt{T}\mu\right]^2\right| X = 1 \right\} + \left[ 1 - \mathbb{P}(A) \right]\left( \sqrt{T}\mu \right)^2\\ \pause
%  &=& \mathbb{P}(A)\; \mathbb{E}
%  \left[ Z^2 |X = 1 \right] + \left[ 1 - \mathbb{P}(A) \right]T\mu^2
%\end{eqnarray*}
%
%\alert{So we need to calculate $\mathbb{P}(A)\; \mathbb{E}[Z^2|X=1]$ and $\mathbb{P}(A)$.}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Calculating $\mathbb{P}(A)$}
%  Define $a = (-\sqrt{d_T} - \sqrt{T}\mu)$ and $b = (\sqrt{d_T} - \sqrt{T}\mu)$ 
%
%  \pause
%
%  \begin{eqnarray*}
%    \mathbb{P}(A) &=& \mathbb{P}\left( |\sqrt{T}\mu + Z| \geq \sqrt{d_T} \right) \\ \pause
%    &=& \mathbb{P}\left( \sqrt{T}\mu + Z \geq \sqrt{d_T} \right) + \mathbb{P}\left( \sqrt{T}\mu + Z \leq -\sqrt{d_T} \right) \\ \pause
%    &=& \mathbb{P}(Z\geq b) + \mathbb{P}(Z \leq a)\\ \pause
%    &=& 1 - \Phi(b) + \Phi(a) \pause
%  \end{eqnarray*}
%
%
%  And hence:
%  \[
%    1 - \mathbb{P}(A) = \Phi(b) - \Phi(a)
%  \]
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Calculating $\mathbb{P}(A)\;\mathbb{E}[Z^2|X=1]$ -- Step 1}
%  \begin{block}{Conditional Density of $Z|X=1$}
%    \vspace{-1em}
%    \[f(z|x=1) = \frac{\mathbf{1}(A)\varphi(z)}{\mathbb{P}(A)} \quad \text{where } \varphi \text{ is the } N(0,1) \text{ density} \]
%  \end{block}
%
%  \pause
%
%  \begin{block}{Therefore:}
%    \begin{eqnarray*}
%      \mathbb{P}(A)\; \mathbb{E}[Z^2|X=1] &=& \mathbb{P}(A) \int_{\mathbb{R}} z^2 \left[ \frac{\mathbf{1}(A)\varphi(z)}{\mathbb{P}(A)} \right]\; \text{d}z\\ \pause 
%       &=&  \int_{-\infty}^a z^2 \varphi(z)\; \text{d}z + \int_{b}^\infty z^2 \varphi(z)\; \text{d}z\\ 
%    \end{eqnarray*}
%  \end{block}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Calculating $\mathbb{P}(A)\; \mathbb{E}[Z^2|X=1]$ -- Step 2}
%  \begin{block}{Unconditional Expectation: $\mathbb{E}[Z^2]$}
%    \[
%      1 = \mathbb{E}[Z^2]  = 
%      \int_{-\infty}^a z^2 \varphi(z)\; \text{d}z +
%      \int_{a}^b z^2 \varphi(z)\; \text{d}z +
%      \int_{b}^\infty z^2 \varphi(z)\; \text{d}z
%    \]
%  \end{block}
%
%  \pause
%
%  \begin{block}{Therefore:}
%    \begin{eqnarray*}
%      \mathbb{P}(A)\; \mathbb{E}[Z^2|X=1] &=&  \int_{-\infty}^a z^2 \varphi(z)\; \text{d}z + \int_{b}^\infty z^2 \varphi(z)\; \text{d}z\\\pause 
%       &=& 1 - \int_a^b z^2 \varphi(z)\; \text{d}z
%    \end{eqnarray*}
%  \end{block}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%  \frametitle{Calculating $\mathbb{P}(A)\;\mathbb{E}[Z^2|X=1]$ -- Step 3}
%
%  \vspace{1em}
%
%  \begin{block}{Integration By Parts}
%  \small 
%Take $u = -z$ and $dv = -z \exp\{-z^2/2\}$ since
%
%	$$\frac{d}{dz} \left(\exp\left\{-z^2/2\right\}\right) = -z\exp\left\{-z^2/2\right\}$$
%
%  \vspace{0.5em}
%
%  \pause
%
%Thus, $v = \exp\{-z^2/2\}$, $du = -1$ and 
%\begin{eqnarray*}
%  \int_a^b z^2 \phi(z) \; dz &=& \left( 2\pi \right)^{-1/2} \int_a^b z^2 \exp\left\{-z^2/2 \; \right\} \;\text{d}z\\ \pause
%  &=& \left( 2\pi \right)^{-1/2}\left[-z \exp\left\{ \left.-z^2/2\right\}\right|_a^b+ \int_a^b \exp\left\{-\frac{z^2}{2}\right\}  \; dz \right]\\ \pause
%		&=& a\phi(a) - b\phi(b) + \Phi(b) - \Phi(a)
%\end{eqnarray*}
%  \end{block}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Simplifed MSE Risk Function}
	\begin{eqnarray*}
    R(\mu,\widehat{\mu}) &=& 1 - \left[a\phi(a) - b\phi(b) + \Phi(b) - \Phi(a) \right] + T\mu^2 \left[\Phi(b) - \Phi(a) \right]\\
		&=&1 + \left[b\phi(b) - a\phi(a)\right]  + (T\mu^2 - 1) \left[\Phi(b) - \Phi(a) \right] 
	\end{eqnarray*}
where
	\begin{eqnarray*}
		a &=& -\sqrt{d_T} - \sqrt{T}\mu\\
		b &=& \sqrt{d_T} - \sqrt{T}\mu
	\end{eqnarray*}

\url{https://fditraglia.shinyapps.io/CH\_Figure\_4\_2/}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Understanding the Risk Plot}

\begin{block}{AIC}
  \begin{itemize}
    \item For any $\mu \neq 0$, risk $\rightarrow 1$ as $T\rightarrow \infty$, the risk of the MLE
    \item For $\mu = 0$, risk $\nrightarrow 0$, risk of ``zero'' estimator 
    \item Max risk is bounded
  \end{itemize}
\end{block}

\begin{block}{BIC}
  \begin{itemize}
    \item For any $\mu \neq 0$, risk $\rightarrow 1$ as $T\rightarrow \infty$, the risk of the MLE
    \item For $\mu = 0$, risk $\rightarrow 0$, risk of ``zero'' estimator 
    \item Max risk is unbounded
  \end{itemize}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

