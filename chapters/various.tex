%!TEX root = ../main.tex
\chapter{More on ``Classical'' Model Selection}


\section{Mallow's $C_p$}
Suppose that we want to predict $y$ from $\mathbf{x}$ using a linear regression model:
	$$\underset{(T\times1)}{\textbf{y}} = \underset{(T\times K)}{X} \underset{(K\times 1)}{\beta} + \boldsymbol{\epsilon}$$
Where $E[\boldsymbol{\epsilon}|X] = 0$ and $Var(\boldsymbol{\epsilon}|X) = \sigma^2 \mathbf{I}$. 
We know that the conditional mean is the minimum mean-squared error predictor.
This means that if $\beta$ were \emph{known}, we could never improve upon simply using all the regressors for prediction.
But since $\beta$ must be \emph{estimated} from the data, a bias-variance tradeoff arises.
In particular, we might be better off \emph{excluding} a regressor with small coefficient, since it adds very little predictive power but introduces additional estimation uncertainty.
Mallow's $C_p$ is a model selection criterion that trys to capture this idea by approximating the \emph{predictive mean squared error} of each model, relative to the infeasible optimum where $\beta$ is known.

We'll now consider using \emph{subsets} of $X$ rather than the full data matrix. 
Let $X_M$ denote a design matrix that possibly excludes some columns of $X$. 
The index $M$ refers to a particular \emph{model}. 
Accordingly, let $\widehat{\beta}_M$ be the least-squares estimator based on the design matrix $X_M$. 
We'll adopt the convention that $\widehat{\beta}_M$ is padded out with zeros for the elements of $\beta$ that are \emph{not estimated} under model $M$. 
This way we can write
	$$X\widehat{\beta}_M = X_{(-M)}\textbf{0} + X_M (X_M'X_M)^{-1} X_M'\textbf{y} = P_M \textbf{y}$$
Now, suppose we want to compare the predictive power of the competing estimators $\widehat{\beta}_M$ using mean-squared error. A na\"{i}ve idea would be to use in-sample prediction error to compare models:
	$$RSS(M) = (\textbf{y} - X\widehat{\beta}_M)'(\textbf{y} - X\widehat{\beta}_M)$$
As is well-known, however, the residual sum of squares can \emph{never} decrease even as we add irrelevant predictors to our model. In constrast, out-of-sample predictive ability can easily decrease when we add more predictors: there's a bias-variance tradeoff that arises from estimation uncertainty. Somehow or other we need to develop a criterion to take this into account.

We'll start off by calculating the predictive mean-squared error of $X\widehat{\beta}_M$ relative to the infeasible optimum, namely $X\beta$. Let $P_M = X_M(X_M'X_M)^{-1}X_M$. Then we have
\begin{eqnarray*}
	\left|\left|X\widehat{\beta}_M - X\beta\right|\right|^2 &=& (P_M \mathbf{y} - X\beta)'(P_M \mathbf{y} - X\beta)\\
		&=&\left\{P_M(Y-X\beta) - (\mathbf{I} - P_M)X\beta \right\}' \left\{P_M(Y-X\beta) - (\mathbf{I} - P_M)X\beta  \right\}\\
		&=&\left\{ P_M \boldsymbol{\epsilon} - (\mathbf{I}- P_M)X\beta\right\}'\left\{ P_M \boldsymbol{\epsilon} + (\mathbf{I}- P_M)X\beta \right\}\\
		&=&\boldsymbol{\epsilon}'P_M'P_M \boldsymbol{\epsilon} - \beta'X'(\mathbf{I}-P_M)'P_M\boldsymbol{\epsilon} \\
			&&\quad \quad - \;\boldsymbol{\epsilon}'P_M'(\mathbf{I} - P_M)X\beta + \beta'X' (\mathbf{I} - P_M)(\mathbf{I} - P_M)X\beta\\
		&=& \boldsymbol{\epsilon}'P_M \boldsymbol{\epsilon} + \beta'X'(\mathbf{I} - P_M)X\beta
\end{eqnarray*}
since $P_M$ and $(\mathbf{I} - P_M)$ are both symmetric and idempotent and their product in any order is zero. Thus, evaluating the predictive mean-squared error conditional on $X$, we have
	\begin{eqnarray*}
		\mbox{MSE}(M|X) &=& E\left[(X\widehat{\beta}_M - X\beta)'(X\widehat{\beta}_M - X\beta)|X \right]\\
		 &=& E\left[\boldsymbol{\epsilon}'P_M \boldsymbol{\epsilon}|X\right] + E\left[\beta'X'(\mathbf{I} - P_M)X\beta |X\right]\\
			&=&E\left[\mbox{trace}\left\{\boldsymbol{\epsilon}'P_M \boldsymbol{\epsilon}\right\}|X\right] + \beta'X'(\mathbf{I} - P_M)X\beta \\
		&=&\mbox{trace}\left\{E[\boldsymbol{\epsilon} \boldsymbol{\epsilon}'|X]P_M\right\} + \beta'X'(\mathbf{I} - P_M)X\beta \\
	&=&\mbox{trace}\left\{\sigma^2 P_M\right\} + \beta'X'(\mathbf{I} - P_M)X\beta \\
	&=& \sigma^2 k_M + \beta'X'(\mathbf{I} - P_M)X\beta
	\end{eqnarray*}
where $k_M$ denotes the number of regressors included in $X_M$ and we have used the fact that the trace of a projection matrix equals its dimension. 

So far, so good: we have derived an expression of the predictive mean-squared error of each model $M$. The problem is that it's infeasible: it depends on the unknown quantities $\sigma^2$ and $\beta$. To get around this, Mallow's $C_p$ constructs an \emph{unbiased} estimator of MSE. We proceed as follows. First, let $\widehat{\beta}$ be the estimator based on the full set of regressors, i.e.\ $\widehat{\beta} = (X'X)^{-1}X'\mathbf{y}$ and let $P_{X}$ be the corresponding projection matrix so that we have 
		$$X \widehat{\beta} = X(X'X)^{-1}X'\mathbf{y} = P_{X}\mathbf{y}$$
Using the fact that $P_MP_X = P_XP_M = P_M$, 
	\begin{eqnarray*}
		E\left[\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} |X\right] &=& E\left[\mathbf{y}'P_X'(\mathbf{I} - P_M)P_{X}\mathbf{y} |X\right]\\
			&=& E\left[\mathbf{y}'(P_X'P_X - P_X'P_MP_X)\mathbf{y} |X\right]\\
			&=& E\left[\mathbf{y}'(P_X - P_M)\mathbf{y} |X\right]
	\end{eqnarray*}
which we can expand as
      \begin{eqnarray*}
     E\left[\mathbf{y}'(P_X - P_M)\mathbf{y} |X\right]&=& E\left[(X\beta + \boldsymbol{\epsilon})'(P_X - P_M)(X\beta + \boldsymbol{\epsilon})\mathbf{y} |X\right]\\
  		&=& E\left[\beta'X'(P_X - P_M)X\beta|X\right] + E[\boldsymbol{\epsilon}'(P_X - P_M)X\beta|X] \\
				&& \quad  + \; E[\beta'X'(P_X - P_M)\boldsymbol{\epsilon}|X] + E[\boldsymbol{\epsilon}'(P_X - P_M)\boldsymbol{\epsilon}|X]\\
			&=& \beta'X'(P_X - P_M)X\beta +  E[\boldsymbol{\epsilon}'(P_X - P_M)\boldsymbol{\epsilon}|X]
    \end{eqnarray*}
Now, we can re-write the first term as    
    \begin{eqnarray*}
    \beta'X'(P_X - P_M)X\beta  &=& \beta'X'P_X X\beta - \beta'X'P_MX\beta\\
      &=& \beta'X'X(X'X)^{-1}X' X\beta - \beta'X'P_MX\beta\\
      &=& \beta'X'X\beta - \beta'X'P_MX\beta\\
      &=& \beta'X'(\textbf{I} - P_M)X\beta
    \end{eqnarray*}
and evaluating the second term, we find that
    \begin{eqnarray*}
			E[\boldsymbol{\epsilon}'(P_X - P_M)\boldsymbol{\epsilon}|X]&=&E[\mbox{trace}\left\{\boldsymbol{\epsilon}'(P_X - P_M)\boldsymbol{\epsilon}\right\}|X]\\
      &=&  \mbox{trace}\left\{E[\boldsymbol{\boldsymbol{\epsilon}\epsilon}'|X](P_X - P_M)\right\}\\
			&=&\mbox{trace}\left\{\sigma^2(P_X - P_M)\right\}\\ 
			&=& \sigma^2 \left(\mbox{trace}\left\{P_X\right\} - \mbox{trace}\left\{P_M\right\}\right)\\ 
			&=& \sigma^2 (K - k_M) 
      \end{eqnarray*}
Hence, putting all the pieces together,
\begin{equation*}
E\left[\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} |X\right] = \beta'X'(\mathbf{I}- P_M)X\beta +  \sigma^2 (K - k_M) 
\end{equation*}
In other words, substituting the estimator $\widehat{\beta}$ from the full model in order to estimate $\beta'X(\mathbf{I}- P_M)X\beta$ \emph{doesn't work}. 
The estimator $\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta}$ is \emph{biased upwards}.
However, we have an explicit expression for the bias, namely $\sigma^2 (K - k_M)$.
This means that if we can find an unbiased estimator of $\sigma^2$, we can \emph{correct} the bias in our estimator of $\beta'X(\mathbf{I}- P_M)X\beta$.
Fortunately there is an obvious unbiased estimator of $\sigma^2$: we simply use the residuals from the full model:
	$$\widehat{\sigma}^2 = \frac{\mathbf{y}'(\mathbf{I} - P_X)\mathbf{y}}{(T-K)}$$
Thus, 
	$$E[\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} - \widehat{\sigma}^2(K- k_M) |X ] = \beta'X(\mathbf{I}- P_M)X\beta$$ 
Now we've managed to construct an unbiased estimator of the \emph{second} term of the MSE.
The first term is easy since we already have an unbiased estimator of $\widehat{\sigma}^2$ and $k_M$ is a known constant: the number of regressors in model $M$.
Therefore, collecting terms
	\begin{eqnarray*}
		\mbox{MC}_M &=& \widehat{\sigma}^2 k_M + \left[\widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} - \widehat{\sigma}^2(K- k_M) \right]\\
			&=& \widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} + 2\widehat{\sigma}^2 (k_M - K)
	\end{eqnarray*}
is an unbiased estimator of $\mbox{MSE}(M|X)$. 
It turns out, however, that we can re-write this expression in a simpler form.
As shown in the appendix to this section,
$$MC_M - 2 \widehat{\sigma}^2 k_m = RSS(M) - T\widehat{\sigma}^2$$
where $RSS(M) = \mathbf{y}'(\mathbf{I} - P_M)\mathbf{y})$ is the residual sum of squares for model $M$.


Substituting this into the expression for $\mbox{MC}_M$ we see that 
	$$\mbox{MC}_M = RSS(M) + \widehat{\sigma}^2(2 k_M - T)$$
which is much easier to interpret than the formula we had before.
Finally, dividing through by $\widehat{\sigma}^2$ gives Mallow's $C_p$
	$$C_p(M) = \frac{RSS(M)}{\widehat{\sigma}^2} + 2k_M - T$$
This expression tells us how we need to \emph{adjust} the residual sum of squares to account for the fact that in-sample fit is a misleading guide to out-of-sample predictive performance.

\subsection{Appendix for Mallow's $C_p$: Tedious Algebra} 
\begin{eqnarray*}
	MC_M - 2\widehat{\sigma}^2k_M &=& \widehat{\beta}'X'(\mathbf{I} - P_M)X\widehat{\beta} - K\widehat{\sigma}^2\\
	&=& \mathbf{y}'(P_X - P_M)\mathbf{y} -  K\widehat{\sigma}^2\\
   &=& \mathbf{y}'(P_X - P_M)\mathbf{y} - \left(\frac{K}{T-K}\right)\mathbf{y}'\left(\mathbf{I} - P_X\right)\mathbf{y}\\
		&=& \left(\frac{T-K}{T-K}\right)(\mathbf{y}'P_X\mathbf{y} - \mathbf{y}'P_M\mathbf{y}) - \left(\frac{K}{T-K}\right)(\mathbf{y}'\mathbf{y}' - \mathbf{y}'P_X \mathbf{y})\\
			&=&\left(\frac{T}{T-K}\right)\mathbf{y}'P_X \mathbf{y} - \mathbf{y}'P_M\mathbf{y} - \left(\frac{K}{T-K} \right)\mathbf{y}'\mathbf{y}  \\
			&=& \left(\frac{T}{T-K}\right)\mathbf{y}'P_X \mathbf{y} - \mathbf{y}'P_M\mathbf{y} + \left(\frac{T - K - T}{T-K} \right)\mathbf{y}'\mathbf{y}  \\
			&=&  \left(\frac{T}{T-K} \right)\mathbf{y}'P_X \mathbf{y} - \mathbf{y}' P_M \mathbf{y} + \left(1 - \frac{T}{T-K}\right)\mathbf{y}'\mathbf{y} \\
			&=&  \left(\frac{T}{T-K} \right)\mathbf{y}'P_X \mathbf{y} - \mathbf{y}' P_M \mathbf{y} + \mathbf{y}'\mathbf{y} - \left(\frac{T}{T-K}\right)\mathbf{y}'\mathbf{y}  \\
			&=& \mathbf{y}'\mathbf{y} - \mathbf{y}'P_M\mathbf{y} - \left(\frac{T}{T-K} \right)(\mathbf{y}'\mathbf{y} - \mathbf{y}'P_m\mathbf{y})\\
			&=& \mathbf{y}'(\mathbf{I} - P_M)\mathbf{y} - \left(\frac{T}{T-K}\right)\mathbf{y}'\left(\mathbf{I} - P_X\right)\mathbf{y}\\
			&=& \mathbf{y}'(\mathbf{I} - P_M)\mathbf{y} - T\widehat{\sigma}^2\\
			&=& RSS(M) - T\widehat{\sigma}^2
      \end{eqnarray*}

\section{Bayesian Information Criterion}
As in our derivation of TIC and AIC, we'll consider a setting with an iid sample of scalar random variables $Y_1, \hdots, Y_T$. 
The results still hold in the more general case, but this simplifies the notation. 
Note that the Bayesian Information Criterion (BIC) is sometimes called the SIC, for ``Schwarz's Information Criterion.''

\subsection{Overview of the BIC}
Despite its name, the BIC is \emph{not} a Bayesian procedure.
It is a large-sample Frequentist \emph{approximation} to Bayesian model selection:
	\begin{enumerate}
		\item Begin with a uniform prior on the set of candidate models.
      Then, choosing the model with the highest posterior probability is equivalent to maximizing the Marginal Likelihood.
		\item The BIC is a large sample approximation to the Marginal Likelihood:
		$$\int \pi(\beta_i)f_i(\mathbf{y}|\beta_i)d\beta_i$$
		where $i$ indexes models $M_i$ in a set $\mathcal{M}$.
		\item As usual when Bayesian procedures are subjected to Frequentist asymptotics, the priors on parameters vanish in the limit.
		\item We proceed by a \emph{Laplace Approximation} to the Marginal Likelihood
	\end{enumerate}

\subsection{Laplace Approximation}
For the moment simplify the notation by suppressing dependence on $M_i$. We want to approximate: 
	$$\int \pi(\beta)f(\mathbf{y}|\beta)d\beta$$
This is actually a common problem in applications of Bayesian inference:
	\begin{itemize} 
		\item Notice that $\pi(\beta)f(\mathbf{y}|\beta)$ is the \emph{kernel} of some probability density, i.e.\ the density without its normalizing constant. 
		\item \emph{How do we know this?} By Bayes' Rule 
	$$\pi(\beta|\mathbf{y}) = \frac{\pi(\beta)f(\mathbf{y}|\beta)}{\int \pi(\beta)f(\mathbf{y}|\beta) d\beta }$$
is a proper probability density and the denominator is \emph{constant} with respect to $\beta$. (The parameter has been ``integrated out.'')
	\item In Bayesian inference, we specify $\pi(\beta)$ and $f(\mathbf{y}|\beta)$, so $\pi(\beta)f(\mathbf{y}|\beta)$ is known. But to calculate the posterior we need to \emph{integrate} to find the normalizing constant.
	\item Only in special cases (e.g.\ conjugate families) can we find the exact normalizing constant. Typically some kind of approximation is needed:  
		\begin{itemize}
			\item Importance Sampling
			\item Markov-Chain Monte Carlo (MCMC)
			\item \emph{Laplace Approximation}
		\end{itemize}
	\end{itemize}
The Laplace Approximation is an \emph{analytical approximation} based on Taylor Expansion arguments. In Bayesian applications, the expansion is carried out around the posterior mode, i.e.\ the mode of $\pi(\beta)f(\mathbf{y}|\beta)$, but we will expand around the Maximum likelihood estimator. 

\begin{pro}[Laplace Approximation]
	\label{pro:laplace}
$$\int \pi(\beta)f(\mathbf{y}|\beta)d\beta \approx \frac{\exp\left\{ \ell(\hat{\beta}) \right\} \pi(\hat{\beta})(2\pi)^{p/2}}{n^{p/2}\left| J(\hat{\beta}) \right|^{1/2}}$$
	Where $\hat{\beta}$ is the \emph{maximum likelihood estimator}, $p$ the dimension of $\beta$ and
		$$J(\hat{\beta}) = -\frac{1}{n} \frac{\partial^2 \log f(\mathbf{y}|\hat{\beta})}{\partial \beta \partial \beta'}$$
\end{pro}

\begin{proof}
A rigorous proof of this result is complicated. The following is a sketch. First write $\ell(\beta)$ for $\log{f(\mathbf{y}|\beta)}$ so that 
$$\pi(\beta)f(\mathbf{y}|\beta) = \pi(\beta) \exp{\left\{ \log{f(\mathbf{y}| \beta)} \right\}}=\pi(\beta) \exp{\left\{ \log{\ell(\beta)} \right\}}$$
By a second-order Taylor Expansion around the MLE $\hat{\beta}$
	\begin{equation}
	\label{taylorell}
		\ell(\beta) = \ell(\hat{\beta}) +\frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell
	\end{equation}
since the derivative of $\ell(\beta)$ is zero at $\hat{\beta}$ by the definition of MLE. A first-order expansion is sufficient for $\pi(\beta)$ because the derivative does not vanish at $\hat{\beta}$
	\begin{equation}
		\label{taylorpi}
		\pi(\beta) = \pi(\hat{\beta}) +  \frac{\partial \pi(\hat{\beta})}{\partial \beta'} \left(\beta - \hat{\beta}  \right)+ R_\pi
	\end{equation}
Substituting Equations \ref{taylorell} and \ref{taylorpi},
	\begin{eqnarray*}
		\int \pi(\beta)f(\mathbf{y}|\beta)d\beta &=& \int \exp\left\{ \ell(\hat{\beta}) +\frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell \right\}\\
		&&\;\;\;\;\;\;\times \left[ \pi(\hat{\beta}) + \left(\beta - \hat{\beta}  \right)' \frac{\partial \pi(\hat{\beta})}{\partial \beta} + R_\pi \right]  d\beta\\\\
		&=& \exp\left\{ \ell(\hat{\beta}) \right\} (I_1 + I_2 + I_3)
	\end{eqnarray*}
where
	\begin{eqnarray*}
		I_1 &=& \pi(\hat{\beta}) \int  \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell\right\}} d\beta\\
		I_2 &=& \frac{\partial \pi(\hat{\beta})}{\partial \beta'}   \int  \left(\beta - \hat{\beta}  \right) \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell\right\}} d\beta\\
\\
		I_3 &=& \int R_\pi \; \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) + R_\ell\right\}} d\beta
	\end{eqnarray*}
Under certain regularity conditions (not the standard ones!) we can treat $R_\ell$ and $R_\pi$ as approximately equal to zero for large $n$ uniformly in $\beta$, so that
	\begin{eqnarray*}
		I_1 &\approx& \pi(\hat{\beta}) \int  \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} d\beta\\
		I_2 &\approx& \frac{\partial \pi(\hat{\beta})}{\partial \beta'}   \int  \left(\beta - \hat{\beta}  \right) \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} d\beta\\
		I_3 &\approx& 0
	\end{eqnarray*}	
Because $\hat{\beta}$ is the MLE, 
	$$\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}$$ 
must be negative definite, so 
	$$-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}$$
is positive definite. It follows that 
	$$ \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} =  \exp{\left\{ -\frac{1}{2} \left( \beta - \hat{\beta}  \right)'\left[ \left(-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right)^{-1} \right]^{-1}\left( \beta - \hat{\beta}  \right) \right\}}$$
can be viewed as the kernel of a Normal distribution with mean $\hat{\beta}$ and variance matrix 
	$$\left(-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right)^{-1}$$
Thus,
	$$\int  \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} d\beta = \left(2\pi\right)^{p/2}\left| \left(-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right)^{-1} \right|^{1/2}$$
and
	$$\int \left(\beta - \hat{\beta}  \right)  \exp{\left\{ \frac{1}{2} \left( \beta - \hat{\beta}  \right)' \frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'} \left( \beta - \hat{\beta}  \right) \right\}} d\beta = 0$$
Therefore,
	\begin{eqnarray*}
		\int \pi(\beta)f(\mathbf{y}|\beta)d\beta &\approx& \exp\left\{ \ell(\hat{\beta}) \right\}\pi(\hat{\beta}) \left(2\pi\right)^{p/2}\left| \left(-\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right)^{-1} \right|^{1/2}\\
		&=&  \exp\left\{ \ell(\hat{\beta}) \right\}\pi(\hat{\beta}) \left(2\pi\right)^{p/2}\left|n \left(-\frac{1}{n}\frac{\partial^2 \ell(\hat{\beta})}{\partial \beta \partial \beta'}\right) \right|^{-1/2}\\\\
		&=&\frac{ \exp\left\{ \ell(\hat{\beta}) \right\}\pi(\hat{\beta}) \left(2\pi\right)^{p/2}}{n^{p/2}\left| J(\hat{\beta}) \right|^{1/2}}
	\end{eqnarray*}
\end{proof}


\subsection{Finally the BIC}
Now we re-introduce the dependence on the model $M_i$. Taking logs of the Laplace Approximation and multiplying by two (again, this is traditional but has no effect on model comparisons)
	\begin{eqnarray*}
		2 \log f(y|M_i) &=& 2 \log \left\{ \int f_i(y|\beta_i)\pi(\beta_i)d\beta_i \right\}\\
    &\approx& 2\ell(\hat{\beta}_i) -p\log(n) + p \log(2\pi) + \log[\pi(\hat{\beta}_i)] -\log \left| J(\hat{\beta_i}) \right|
	\end{eqnarray*}
The first two terms are $O_p(n)$ and $O_p(\log{n})$, while the last three are $O_p(1)$, hence negligible as $n\rightarrow \infty$. This gives us Schwarz's BIC
	$$BIC(M_i) = 2\log{f_i(\mathbf{y}|\hat{\beta}_i)} - p\log{n}$$
We choose the model $M_i$ for which $BIC(M_i)$ is largest. Notice that the prior on the parameter, $\pi(\beta)$, drops out in the limit, and recall that we began by putting a uniform prior on the \emph{models} under consideration. 

\section{Some Time Series Examples}
Thus far we've looked at a number of model selection criteria. 
Some of them, namely AIC, BIC and TIC, are completely portable: they can be applied to \emph{any} model that is estimated by maximum likelihood. 
Each of these can be immediately applied to time series data: if you have a routine to carry out ML estimation, be it conditional ML or the Kalman filter, it already produces all the quantities you need. 
In contrast, some of the other examples we considered, namely Mallow's $C_p$ and AIC$_c$, were derived for the special case of linear regression. 
How can we adapt these examples to time series data? Fortunately, if we're willing to use conditional ML estimation, some of the most widely used time series models \emph{are in fact} regression models. In this section we'll take a closer look at model selection for autoregression and vector autoregression models.


\subsection{Autoregressive Models}
For simplicity assume there is no constant term. Then the AR($p$) model is
	$$y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \epsilon_t$$
where $\epsilon_t \sim \mbox{iid} \; N(0,\sigma^2)$ and we observe a sample $y_1, \hdots, y_N$. We'll use conditional maximum likelihood, so we lose the first $p$ observations. Thus the \emph{effective sample size} is $T = N-p$. The conditional ML estimator of $\boldsymbol{\phi} = \left(\phi_1, \hdots, \phi_p \right)'$ is simply the least-squares estimator
	$$\widehat{\boldsymbol{\phi}} = (X'X)^{-1}X'\textbf{y}$$
where $\mathbf{y} = (y_{p+1}, y_{p+2}, \hdots, y_N)'$ and the design matrix is
	$$X = \left[ \begin{array}
		{cccc}
		y_p & y_{p-1} & \cdots & y_1\\
		y_{p+1} & y_{p} & \cdots  & y_2\\
		\vdots &  \vdots & &\vdots\\
		y_{N-1} & y_{N-2} &\cdots & y_{N-p-1}
	\end{array}\right]$$
The maximum likelihood estimator of $\sigma^2$ is
	$$\widehat{\sigma}^2_p = \frac{\mbox{RSS}_p}{T}$$
where RSS denotes the residual sum of squares, namely $\lvert\lvert\mathbf{y} - X\widehat{\boldsymbol{\phi}} \rvert\rvert^2$. Since this is a regression model, it's trivial to adapt both Mallow's $C_p$ and the AIC$_C$ to this case.\footnote{If you'd like to see all of the details written out, consult McQuarrie \& Tsai (1998), Chapter 3.} For Mallow's $C_p$ we have
	$$C_p = \frac{\mbox{RSS}_p}{\widehat{\sigma}^2_{wide}} - T + 2p$$
where $\widehat{\sigma}^2_{wide}$ is the estimate of $\sigma^2$ from the model with \emph{maximum order} among those under consideration. For AIC$_c$ we have
	$$\mbox{AIC}_c = \log\left(\widehat{\sigma}^2_p \right) + \frac{T+p}{T-p-2} $$
For both $C_p$ and AIC$_c$ we choose the lag length that \emph{minimizes} the criteiron. 

Using an argument essentially identical to the one presented in the notes for Lecture 2, the maximized log-likelihood for the AR$(p)$ model is
	$$-\frac{T}{2}\left[\log(2\pi) + \log\left(\widehat{\sigma}^2_p \right) + 1\right]$$
To construct the AIC and BIC, we multiply this quantity by 2 and subtract the appropriate penalty term, ignoring terms that are constant across models. The number of parameters for an AR($p$) model is $p+1$, since we estimate $\sigma^2$ in addition to the $p$ autoregressive parameters. We'll rescale both AIC and BIC and flip their signs to make them comparable to the $C_p$ and AIC$_c$ expressions from above. Putting everything together for the sake of comparison, we have
\begin{eqnarray*}
	\mbox{AIC} &=& \log\left(\widehat{\sigma}^2_p \right) + \frac{2(p+1)}{T}\\
	\mbox{AIC}_c &=&  \log\left(\widehat{\sigma}^2_p \right) + \frac{T+p}{T-p-2}\\
	C_p &=&  \frac{\mbox{RSS}_p}{\widehat{\sigma}^2_{wide}} + 2p- T\\
	\mbox{BIC} &=&\log\left(\widehat{\sigma}^2_p \right) + \frac{ \log(T)(p+1)}{T}
\end{eqnarray*}
In each case, we choose the model that \emph{minimizes} the criterion. 


\paragraph{Ng \& Perron (2005)} There are some subtle but important points that we glossed over in the preceding discussion and that are, indeed, rarely mentioned in textbooks or articles on model selection. First there is the question of whether we should use the maximum likelihood estimator $\widehat{\sigma}^2$ or the unbiased estimator that divides by $T-p$ rather than $T$. In time series applications $T$ may be small enough that it makes a difference. More troubling, however, is the problem of deciding what should count as the sample size, since different lag lengths use a different number of observations in the conditional maximum likelihood setting. Indeed, as they are usually written, expressions for AIC and BIC drop terms that are constant across models in \emph{cross-section regression}, where changing the number of regressors doesn't affect sample size. The situation is of course entirely different for AR models but practicioners \emph{still use the same formulas} in this case. There are numerous different ways to handle these complications. Ng \& Perron (2005) review the possibilities and illustrate how each performs in a number of simulation studies.


\subsection{Vector Autoregression Models}
Again, assume the intercept is zero. Then the VAR$(p)$ model is given by
	\begin{eqnarray*}
		\underset{(q\times 1)}{\textbf{y}_t} &=& \underset{(q\times q)}{\Phi_1} \textbf{y}_{t-1} + \hdots + \Phi_{p}\textbf{y}_{t-p} + \boldsymbol{\epsilon_t}\\
		\boldsymbol{\epsilon}_t &\overset{iid}{\sim}& N_q(\mathbf{0}, \Sigma)
	\end{eqnarray*}
where we observe $\mathbf{y}_1, \hdots, \mathbf{y}_N$. Again, if we're content to use conditional maximum likelihood, dropping the first $p$ observations to estimate a VAR$(p)$ model, this is simply a multivariate regression problem and we have an \emph{effective sample size of} $T = N-p$. Written as a multivariate regression model, we have 
	$$\underset{(T\times q)}{Y} = \underset{(T\times pq)}{X}\underset{(pq\times q)}{\Phi}  + \underset{(T\times q)}{U}$$
where 
	$$
	\underset{(T\times q)}{Y} = \left[\begin{array}
		{c} \textbf{y}_{p+1}' \\ \textbf{y}_{p+2}' \\
		\vdots \\ \textbf{y}_{N}' 
	\end{array} \right],\quad
	\underset{(pq\times q)}{\Phi} = \left[\begin{array}
		{c} \Phi_1' \\ \Phi_2' \\ \vdots \\ \Phi_p'
	\end{array} \right],\quad
	\underset{(T\times q)}{U} = \left[\begin{array}
		{c} \boldsymbol{\epsilon}_{p+1}' \\ 
		\boldsymbol{\epsilon}_{p+2}' \\ 
		\vdots \\ 
		\boldsymbol{\epsilon}_{N}' \\ 
	\end{array} \right]
	$$
and the design matrix is
$$\underset{(T\times pq)}{X} = \left[\begin{array}
		{cccc}
		\textbf{y}_p' & \textbf{y}_{p-1}' & \cdots & \textbf{y}_{1}'\\
		\textbf{y}_{p+1}' & \textbf{y}_{p}' & \cdots & \textbf{y}_{2}'\\
		\vdots & \vdots & &\vdots \\
		\textbf{y}_{N-1}' & \textbf{y}_{N-2}' & \cdots & \textbf{y}_{N-p-1}'\\
	\end{array} \right]$$
Thus, the conditional maximum likelihood estimator for $\Phi$ is
	$$\widehat{\Phi} = (X'X)^{-1} X'Y$$
and the maximum likelihood estimator for $\Sigma$ is
	$$\widehat{\Sigma}_p = \frac{\left(Y - X\widehat{\Phi}\right)'\left(Y - X\widehat{\Phi}\right)}{T}$$
The VAR$(p)$ model has a very large number of parameters. First, we have the coefficients of $\Phi_1, \hdots, \Phi_p$. Each of these is an unrestricted $q\times q$ matrix so $\Phi$ contains a total of $pq^2$ parameters. We also need to estimate the variance matrix $\Sigma$ of the errors $\epsilon$. Although $\Sigma$ contains $q^2$ elements, it is a symmetric matrix so there are only $q(q+1)/2$ free parameters. Thus, a VAR$(p)$ model requires us to estimate a total of $pq^2 + (q+1)q/2$ parameters. To calculate the AIC and BIC we also need the maximized log-likelihood, which is given by
	$$-\frac{T}{2} \left[q\log(2\pi) + \log \left| \widehat{\Sigma}_p\right| + q\right]$$
Re-scaling as we did for the AR model, we have
\begin{eqnarray*}
	\mbox{AIC} &=& \log \left| \widehat{\Sigma}_p\right| + \frac{2pq^2 + q(q+1)}{T}\\ \\
	\mbox{BIC} &=& \log \left| \widehat{\Sigma}_p\right| +  \frac{\log(T)(pq^2 + q(q+1)/2)}{T}
\end{eqnarray*}
The multivariate generalization of AIC$_c$ is
$$\mbox{AIC}_c = \log \left| \widehat{\Sigma}_p\right|  + \frac{(T + qp)q}{T - qp - q -1}$$
as explained in Chapter 5 of McQuarrie and Tsai (1998). For each of the preceding three expressions, we choose the model that \emph{minimizes} the given criterion.

\subsection{Corrected AIC for State Space Models}
As the lag length $p$ grows, the number of parameters in a VAR$(p)$ model explodes, and can easily come close to the effective sample size. 
In situations like this, AIC is known to perform poorly. 
The bias correction $2\times \mbox{length}(\theta)$ is based on a large-sample argument and fails to provide a good approximation when the number of parameters is too close to the sample size, leading the AIC to choose models that are in general ``too large'' to acheive our target of minimizing the KL divergence.
\footnote{Cavanaugh \& Shumway (1997) suggest $\mbox{length}(\theta) \approx T/2$ as a rough approximation of what counts as ``too many parameters relative to sample size'' for the AIC to work well.} 
The idea behind the AIC$_c$ of Hurvich and Tsai (1989) was to provide a better approximation to the AIC bias correction for AR models under a certain set of assumptions. 
In a similar vein, Cavanaugh \& Shumway (1997) propose a refined AIC, the AIC$_b$, for general state space models. 
Rather than deriving an analytical correction term, they suggest using the bootstrap to approximate the bias of the maximized log-likelihood as an estimator of the expected log likelihood, using the state-space bootstrap procedure proposed by Stoffer and Wall (1991).  

