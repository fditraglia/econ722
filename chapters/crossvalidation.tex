%!TEX root = ../main.tex
\chapter{Cross-Validation}
In our first lecture, we learned that choosing a model by minimizing the KL divergence is equivalent to choosing a model by maximizing the expected log likelihood. 
We also learned that the sample analogue
 $$E_{\widehat{G}}\left[\log f(\textbf{y}|\widehat{\theta}) \right]= \frac{\ell(\widehat{\theta})}{T}= \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta})$$
provides a biased estimator of this quantity. Intuitively, the problem is that it uses the data twice: first to estimate $\widehat{\theta}$ and then to approximate the integral
	$$\int g(y) \log f(y|\widehat{\theta}) \; dy = E_G\left[\log f(Y_{new}|\widehat{\theta}) \right]$$
using the empirical CDF constructed from the sample observations.
This problem is not limited to estimating the KL-divergence: it is generic to \emph{any measure of goodness of fit}.
Since the problem is that we've used the data twice, an obvious idea is to find some way to use two \emph{independent} datasets: one for parameter estimation and another for model selection.
This is the idea behind cross-validation.
We split the data into two parts, use one for estimation and the \emph{other} for model evaluation.
To avoid ``wasting data'' we repeat this process sucessively for \emph{different} splits, so each observation has a chance to be used for for estimation and evaluation but \emph{never for both at the same time}.
Although simple and flexible, notice that this idea of ``splitting up our dataset'' essentially presupposes that we are working with iid data.
In fact it is possible to adapt the idea behind cross-validation to handle time series data, as discussed in Section \ref{sec:CVdependent} below.
For all other parts of this discussion, however, we will assume iid data.

\section{K-fold Cross-Validation}
The most general form of the cross-validation algorithm is as follows:
	\begin{enumerate}
		\item Randomly partition the dataset into $K$ ``folds'' of roughly equal size.
		\item For each $k = 1, \hdots, K$ estimate your model using all observations \emph{except} those contained in the $k$th fold
    \item Each observation belongs to a \emph{single} fold.
      Let $\widehat{y}^{-k(t)}$ denote the predicted value of $y_t$ from the model estimated \emph{without} the fold containing $y_t$. 
		\item Let $L$ be a loss function.
      Then the K-fold cross-validation estimate of the out-of-sample predictive loss is given by
			$$CV(K) = \frac{1}{T}\sum_{t=1}^T L\left(y_t, \widehat{y}^{-k(t)}\right)$$
		\item Repeat the above steps for each model under consideration and choose the model that minimizes $CV(K)$.
	\end{enumerate}

To use cross-validation in practice we need to make two choices.
First, what loss function should we use and second what value should we choose for $K$?
The first choice is problem specific: in a regression problem we may choose squared error loss; in a classification problem we may choose zero-one loss.
As we'll see below, we can even use the log-likelihood as a ``loss function'' in a slight abuse of notation.
The idea, however, remains the same: evaluate some measure of fit at an observation not used to estimate the model.
So how to choose $K$?
One possibility is to set $K = T$ leading to what is called \textbf{leave-one-out cross-validation} or LOO-CV for short.
In this case there are \emph{as many folds as observations}: we predict $y_t$ using a model fitted with all observations \emph{except} $t$.
As we will see below, this choice combined with the log-likelihood as a measure of fit yields some very interesting results.
In general, however, there is no clear answer to what value of $K$ is best.
Nevertheless, several points are worth considering.

The first is computational complexity. 
Leave-one-out CV requires us to re-fit each model $T$ times. 
In contrast 5-fold cross-validation only requires us to re-fit 5 times. 
For linear models and quadratic loss there is a computational shortcut that makes LOO-CV essentially costless, as you will show on the problem set.
A similar results holds for and model that can be expressed as a linear smoother.
Many interesting models, however, cannot be expressed as linear smoothers so this consideration can be important. 
A second consideration in the choice of $K$ is the tradeoff between bias and variance in estimating the out of sample predictive loss, a point emphasized by Hastie, Tibshirani \& Friedman (2008).
When $K = T$, we have as many folds as observations. 
This is simply leave-one-out CV and it turns out to give an approximately unbiased estimator of the expected out-of-sample prediction error. 
Using a larger value of $K$, they argue, introduces a bias but tends to produce a lower variance estimator of the prediction estimator because the partial-sample estimators are less similar to each other when they have fewer observations in common.
While this advice is reasonable in certain situations, such as classification and density estimation, it is far from universally applicable as Arlot \& Celisse (2010) point out in their comprehensive review article.
For example, setting $K=T$ actually \emph{minimizes} the variance of the prediction error estimator in certain settings, such as linear regression.
A third consideration is asymptotic properties. 
We have yet to discuss the ideas of consistency and efficiency in model selection but, as we will see below, we can say something very interesting about LOO-CV in large samples. 

\subsection{Cross-Validation for Dependent Data}
\label{sec:CVdependent}
If our data are dependent, the intuition behind cross-validation breaks down.
It seems strange, for example, to think about randomly partitioning a time series when the whole point is that order matters.
Moreover, if the data are correlated then sequentially leaving out folds in estimation does \emph{not} necessarily break the dependence between $y_t$  and $\widehat{y}^{-k(t)}$. 

To adapt LOO-CV to the case of dependent data, Burman, Chow \& Nolan (1994) proposed an idea called  ``$h$-block cross-validation.''
Roughly speaking, the idea is to assume that dependence dies sufficiently quickly over time that we can treat observations that are ``far enough apart'' as \emph{approximately} independent.
Specifically, we choose an integer value $h$ and assume that $y_t$ and $y_s$ can be treated as independent as long as $|s - t|>h$.
As in the iid version of leave-one-out cross-validation, we still evaluate a loss function by predicting \emph{one} witheld observation at a time using a model estimated without it.
The difference is that we also omit the $h$ neighboring observations \emph{on each side} when fitting the model.
For example, if we choose to evaluate squared-error loss, the criterion is
	$$CV_h(1) = \frac{1}{T-p}\sum_{t = p+1}^T \left(y_t - \hat{y}_{(t)}^h\right)^2$$
where 
$$\hat{y}^h_{(t)} = \hat{\phi}^h_{1(t)} y_{t-1} + \hdots + \hat{\phi}^h_{1(t)}y_{t-p}$$
and $\hat{\phi}^h_{j(t)}$ denotes the $j$th parameter estimate from the conditional maximum likeluhood (i.e.\ least-squares) estimator with observations $y_{t-h}, \hdots,  y_{t+h}$ removed.
We still have the question of what $h$ to choose. 
Here there is a trade-off between making the assumption of independence more plausible and leaving enough observations to get precise model estimates.
Intriguingly, the simulation evidence presented in McQuarrie and Tsai (1998) suggests that setting $h=0$, which yields plain-vanilla leave-one-out CV, works well even in settings with dependence.
The idea of $h$-block cross-validation can also be adapted to versions of cross-validation other than leave-one-out.
For details, see Racine (2000).

\subsection{The Equivalence Between LOO-CV and TIC}
Suppose we set $K=1$ and use the log-likelihood as our measure of model fit.
Let $Y_1, \hdots, Y_T$ be a collection of iid observations and let $\widehat{\theta}_{(t)}$ denote the ML estimator for $\theta$ using all observations \emph{except} $Y_t$. 
The leave-one-out cross-validation estimator of the expected log likelihood is
	$$CV(1) = \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta}_{(t)})$$
The idea is that, since the data are iid, $\widehat{\theta}_{(t)}$ is \emph{independent} of $Y_t$. Accordingly, the cross-validation estimate of the expected log-likelihood should \emph{not} be subject to the over-optimism problem that plagues the maximized log-likelihood. To use cross-validation for model selection, we simply calculate $CV(1)$ for the various models under consideration, and choose the one with the \emph{highest} value.

As it turns out, leave-one-out cross-validation is intimately connected with the TIC. In fact the two are \emph{asymptotically equivalent} as we'll now show. To begin note that, by a first-order Taylor Expansion of the leave-one-out estimator around the full-sample MLE we have
	\begin{eqnarray*}
		CV(1) &=& \frac{1}{T} \sum_{t=1}^T \log f(y_t|\widehat{\theta}_{(t)})\\
			&=&\frac{1}{T} \sum_{t=1}^T \left[\log f(y_t|\widehat{\theta}) + \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) \right] + o_p(1)\\
			&=& \frac{\ell(\widehat{\theta})}{T} + \frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) + o_p(1)
	\end{eqnarray*}
so we simply need to show that
	$$\frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) = -\frac{1}{T}\mbox{trace}\left(\widehat{J}^{-1} \widehat{K} \right) + o_p(1)$$
In the following section we will see why this assertion holds.

\subsection{Influence Functions and LOO-CV}

To understand the preceding assertion, we'll need to take a slight detour and talk about \emph{influence functions}, an idea from the robust estimation literature.\footnote{For a detailed overview, see ``Robust Statistics'' by Huber \& Ronchetti (2009).} 
Let $\mathbb{T}=\mathbb{T}(G)$ be a functional and $G$ be some probability distribution. 
Then the influence function of $\mathbb{T}$ at a point $y$ is defined as 
	$$\mbox{infl}(G,y) = \lim_{\epsilon \rightarrow 0} \frac{\mathbb{T}\left(\left(1-\epsilon\right)G + \epsilon \delta_y\right) - \mathbb{T}(G)}{\epsilon}$$
where $\delta_y$ is a \emph{point mass} at $y$, that is
		$$\delta_y(a)\left\{\begin{array}{c} 0, \; a<y \\ 1, \; a\geq y\end{array} \right.$$
All kinds of quantities that we know and love can be viewed as functionals of a distribution, for example the mean and variance.\footnote{``Information Criteria and Statistical Modeling'' by Konishi and Kitagawa (2008) provides a good overview.} 
Here we're going to be concerned with a particular functional that should look familiar from our lecture on AIC and friends:
	$$\theta_0 = \mathbb{T}(G) = \underset{\theta \in \Theta}{\arg \min} \;E_G\left[\log\left\{\frac{g(Y)}{f(Y|\theta)} \right\} \right]$$
What this says is that we can view $\theta_0$ as the result of applying an \emph{operator} $\mathbb{T}$ to the distribution $G$. 
In this case $\theta_0$ is simply the pseudo-true value: the probability limit of the maximum likelihood estimator of $\theta$ based on $f(y|\theta)$. 
Clearly the pseudo-true value depends on the DGP, namely $G$. 
Different distributions $G$ would yield different pseudo-true values for the \emph{same} likelihood $f$. 
If we evaulate $\mathbb{T}$ at the \emph{empirical} distribution $\widehat{G}$ we get the maximum likelihood estimator $\widehat{\theta}$ rather than the pseudo-true value $\theta_0$.


The influence function is in fact a \emph{functional derivative}. 
It allows us to evaulate, for example, how the pseudo-true value $\theta_0$  would change if we \emph{slightly} changed the distribution $G$ that generated the data by ``polluting'' it with a tiny mass point located at $y$. 
We could also consider how the maximum likelihood estimator, $\widehat{\theta}$, would change if we slightly changed the dataset, represented by empirical distribution function. 
% Under appropriate regularity conditions, it can be shown that
% 	$$T(\widehat{G}_T) = T(G) + \frac{1}{T}\sum_{t=1}^T \mbox{infl}(G,Y_t) + o_p(T^{-1/2})$$
% so that
% 	$$\sqrt{T}\left(\mathbb{T}(\widehat{G}_T) - \mathbb{T}(G) \right) \overset{d}{\rightarrow}N(0,\Omega)$$
Now, since the empirical distribution is given by
$$\widehat{G}(a) = \frac{1}{T}\sum_{t=1}^T \textbf{1}\left\{y_t \leq a\right\} = \frac{1}{T}\sum_{t=1}^T \delta_{y_t}(a)$$
we can re-write it as 
$$\widehat{G} = (1 - 1/T) \widehat{G}_{(t)} + \delta_{y_t}/T$$
where $\widehat{G}_{(t)}$ is the empirical distribution with $y_t$ excluded from the dataset. Applying $\mathbb{T}$ to both sides, subtracting $\mathbb{T}(\widehat{G}_{(t)})$ and multiplying the right-hand-side by $T/T$, we have
	$$\mathbb{T}(\widehat{G}) - \mathbb{T}(\widehat{G}_{(t)}) = \frac{1}{T}\left[\frac{\mathbb{T}\left((1 - 1/T) \widehat{G}_{(t)} + \delta_{y_t}/T \right) - \mathbb{T}(\widehat{G}_{(t)})}{1/T} \right]$$
If we take $\epsilon = 1/T$, the term in square brackets is \emph{exactly} the expression whose limit is defined as the influence function. 
Hence, for $T$ large we have the approximation
$$\mathbb{T}(\widehat{G}) - \mathbb{T}(\widehat{G}_{(t)}) = \frac{1}{T} \mbox{infl}\left(\widehat{G}_{(t)}, y_t \right) + o_p(1)$$
Now, since $\mathbb{T}(\widehat{G}) = \widehat{\theta}$ and $\mathbb{T}(\widehat{G}_{(t)}) =\widehat{\theta}_{(t)}$, we have the following expression for the leave-one-out estimator
\begin{eqnarray*}
	\widehat{\theta}_{(t)} &=& \widehat{\theta} - \frac{1}{T} \mbox{infl}\left(\widehat{G}_{(t)}, y_t\right) + o_p(1)\\
	&=& \widehat{\theta} - \frac{1}{T} \mbox{infl}\left(\widehat{G}, y_t\right) + o_p(1)
\end{eqnarray*}
The second expression indicates that dropping one observation is asymptotically negligible in its effect, through the empirical CDF, on the influence function.
As it turns out, the influence function for maximum likelihood estimation is
	$$\mbox{infl}(G,y) = J^{-1} \left(\frac{\partial \log f(y|\theta_0)}{\partial \theta}\right)$$
where $\theta_0 = \mathbb{T}(G)$ is the pseudo-true value.\footnote{For details, see the next section.} Hence, evaluating this expression at $\widehat{G}$ and $y_t$ and substituting into our expression for $\widehat{\theta}_{(t)}$
	$$\widehat{\theta}_{(t)} = \widehat{\theta} - \frac{1}{T}\widehat{J}^{-1} \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) + o_p(1)$$
since $\mathbb{T}(\widehat{G}) = \widehat{\theta}$. This gives us an asymptotic expansion for $\left(\widehat{\theta}_{(t)} - \widehat{\theta}\right)$, namely 
$$\left(\widehat{\theta}_{(t)} - \widehat{\theta}\right) =  - \frac{1}{T}\widehat{J}^{-1} \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) + o_p(1)$$
which is exactly what we need. Finally, substituting this back into the expression we initially set out to prove,
\begin{eqnarray*}
	\frac{1}{T}\sum_{t=1}^T \frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta'}\left(\widehat{\theta}_{(t)} - \widehat{\theta} \right) &=& -\frac{1}{T}\sum_{t=1}^T \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right)' \frac{\widehat{J}^{-1}}{T}\left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) + o_p(1)\\
		&=& -\frac{1}{T}\mbox{trace}\left\{\widehat{J}^{-1}\left[\frac{1}{T}\sum_{t=1}^T \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) \left(\frac{\partial \log f(y_t|\widehat{\theta})}{\partial \theta}\right) '\right]\right\}\\
		&&\quad \quad + \; o_p(1)\\
		&=&-\frac{1}{T}\mbox{trace}\left\{\widehat{J}^{-1} \widehat{K} \right\}
\end{eqnarray*}


\subsection{The Influence Function for MLE}
In the preceding argument I claimed that the influence function for MLE is 
	$$\mbox{infl}(G,y) = J^{-1} \left(\frac{\partial \log f(y|\theta_0)}{\partial \theta}\right)$$
  Here's a justification for this assertion, following Chapter 5 of Konishi \& Kitagawa (2008).
First, note that the functional $\mathbb{T}$ for MLE is defined as
$$\int\left.\frac{\partial\log f\left(z|\theta\right)}{\partial\theta}\right|_{\theta=\mathbb{T}\left(G\right)}dG\left(z\right)=0$$
where $\mathbb{T}\left(G\right)=\theta_{0}$. 
Now, to calculate the influence function, we need to evaulate $\mathbb{T}$ not at $G$ but at $\left(1-\epsilon\right)G+\epsilon\delta_{y}$. 
Substituting, we have
$$\int\left.\frac{\partial\log f\left(z|\theta\right)}{\partial\theta}\right|_{\theta=\mathbb{T}\left(\left(1-\epsilon\right)G+\epsilon\delta_{y}\right)}d\left(\left(1-\epsilon\right)G\left(z\right)+\epsilon\delta_{y}\left(z\right)\right)=0$$
Note that the pseudo-true value has changed to $\mathbb{T}(\left(1-\epsilon\right)G+\epsilon\delta_{y}) \neq \theta_0$ since we're evaluating the functional at a different distribution than $G$. In fact, the preceding expression gives $\theta$ as an \emph{implicit function} of $\epsilon$. The next step is to differentiate both sides of the preceding equation with respect to $\epsilon$ and evaulate the result at $\epsilon = 0$. As written, this looks a little intimidating so let's simplify the notation a bit and unpack this somewhat strange-looking integral. First, let $s(z| \theta) = \partial \log f(z|\theta)/\partial \theta$ and write $\theta(\epsilon,y) = \mathbb{T}(\left(1-\epsilon\right)G+\epsilon\delta_{y})$ and $H(z) = \left(1-\epsilon\right)G\left(z\right)+\epsilon\delta_{y}\left(z\right)$. Using this notation, the integral becomes
	$$\int s(z|\theta(\epsilon,y))\; dH(z)$$
Now, the measure $H(z)$ is simply a \emph{mixture distribution}: $Z \sim H(z)$ is a random variable that equals $y$ with probability $\epsilon$ and $X$ with probability $1-\epsilon$ where $X \sim G(x)$. Indeed, the preceding integral is simply the \emph{expected value} of a \emph{function} of $Z$. Hence,
\begin{eqnarray*}
	\int s(z|\theta(\epsilon,y))\; dH(z) &=& (1-\epsilon)\int s(z|\theta(\epsilon,y))\; dG(z) + \epsilon s(y|\theta(\epsilon,y))\\
		&=& (1-\epsilon) A(\epsilon) + \epsilon B(\epsilon)
\end{eqnarray*}
First we'll differentiate each piece:
	\begin{eqnarray*}
		\frac{\partial}{\partial \epsilon} \left[(1-\epsilon) A(\epsilon)\right] &=& -A(\epsilon) + (1-\epsilon) \frac{\partial}{\partial \epsilon} A(\epsilon)\\
		\frac{\partial}{\partial \epsilon}\left[ \epsilon B(\epsilon)\right] &=& B(\epsilon) +\epsilon\; \frac{\partial}{\partial \epsilon} B(\epsilon)
	\end{eqnarray*}
Combining and and evaluating at $\epsilon = 0$, 
	\begin{eqnarray*}
		\frac{\partial}{\partial \epsilon}\left[\int s(z|\theta(\epsilon,y))\; dH(z) \right]_{\epsilon = 0} &=& B(0) - A(0)  + \frac{\partial}{\partial \epsilon} A(\epsilon)
	\end{eqnarray*}
Converting back into the notation of the original problem
	\begin{eqnarray*}
		B(0) &=& s(y|\theta(0,y))= \left.\frac{\partial \log f(y|\theta)}{\partial \theta}\right|_{\theta = \mathbb{T}(G)}\\
		A(0) &=& \int s(z|\theta(0,y)) \; dG(z) = \int \left.\frac{\partial \log f(z|\theta)}{\partial \theta}\right|_{\theta = \mathbb{T}(G)} \;dG(z) = 0
	\end{eqnarray*}
by the definition of $\theta_0 = \mathbb{T}(G)$ as the solution to the population moment condition for MLE under the data generating process $G$. Similarly,
	\begin{eqnarray*}
		\frac{\partial}{\partial \epsilon} A(\epsilon) &=& \int \frac{\partial s(z|\theta(0,y))}{\partial \theta} \frac{\partial \theta(0,y)}{\partial \epsilon}\; dG(z)\\
		&=& \int \left.\frac{\partial^2 \log f(z|\theta)}{\partial \theta \partial \theta'}\right|_{\theta = \mathbb{T}(G)} \frac{\partial}{\partial \epsilon} \left[  \mathbb{T}(\left(1-\epsilon\right)G+\epsilon\delta_{y})\right]_{\epsilon = 0}\; dG(z)
	\end{eqnarray*}
Thus, putting everything together,
	$$\left.\frac{\partial^2 \log f(y|\theta)}{\partial \theta}\right|_{\theta = \mathbb{T}(G)} + \int \left.\frac{\partial^2 \log f(z|\theta)}{\partial \theta \partial \theta'}\right|_{\theta = \mathbb{T}(G)} \frac{\partial}{\partial \epsilon} \left[  \mathbb{T}(\left(1-\epsilon\right)G+\epsilon\delta_{y})\right]_{\epsilon = 0}\; dG(z) = 0$$
Rearranging, and noting that the second factor in the second term is constant with respect to the variable of integration gives
\begin{eqnarray*}
	\frac{\partial}{\partial\epsilon}\left[\mathbb{T}\left(\left(1-\epsilon\right)G+\epsilon\delta_{y}\right)\right]_{\epsilon = 0}
 & =&\left\{ -\int\left.\frac{\partial\log f\left(z|\theta\right)}{\partial\theta\partial\theta^{\prime}}\right|_{\theta=\mathbb{T}\left(G\right)}dG\left(z\right)\right\} ^{-1}\left.\frac{\partial\log f\left(y|\theta\right)}{\partial\theta}\right|_{\theta=\mathbb{T}\left(G\right)}\\
 & =&J^{-1}\frac{\partial\log f\left(y|\theta_{0}\right)}{\partial\theta}
\end{eqnarray*}
And now we're finished since:
$$\frac{\partial}{\partial\epsilon}\left[\mathbb{T}\left(\left(1-\epsilon\right)G+\epsilon\delta_{y}\right)\right]_{\epsilon = 0} = \lim_{\epsilon \rightarrow 0} \frac{\mathbb{T}\left(\left(1-\epsilon\right)G+\epsilon\delta_{y}\right)}{\epsilon} = \mbox{infl}(G,y)$$

